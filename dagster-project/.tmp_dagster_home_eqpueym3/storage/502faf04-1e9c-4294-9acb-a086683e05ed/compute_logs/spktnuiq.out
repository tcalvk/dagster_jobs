[{'title': '(USA) Data Engineer III', 'company_name': 'Walmart', 'location': 'Goshen, AR', 'via': 'ZipRecruiter', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=LiIzACoJnlZrITEqAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMsQrCMBAAUFy7uzjdqKKJCC46FRSJoyKO5VqOJJLmQu6GfoAfLl3e-JrfojHr96vdwBUV4ZZ9zEQVnHOwhwf3IIR1CMAZ7sw-0eoSVIucrRVJxouixsEMPFrO1PNkv9zLTCcBK5WESt3xdJhMyX67_GAasSrE-ZNAeQft8w9HkHibhwAAAA&shmds=v1_AdeF8Kg-H_A16DxjOdY2E2yH7Nw8pl7XKLYcppN2_XQXLqtCjg&shem=damc,dimg2&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=LiIzACoJnlZrITEqAAAAAA%3D%3D', 'extensions': ['6 days ago', '90Kâ€“180K a year', 'Full-time and Part-time', 'Paid time off', 'Health insurance', 'Dental insurance'], 'detected_extensions': {'posted_at': '6 days ago', 'salary': '90Kâ€“180K a year', 'schedule_type': 'Full-time and Part-time', 'paid_time_off': True, 'health_insurance': True, 'dental_coverage': True}, 'description': "Position Summary...\n\nWhat you'll do...\n\nResponsibilities:\nâ€¢ Data Transformation and Integration: Extracts data from identified databases and transform data to a structure that is relevant to the problem by selecting appropriate techniques.\nâ€¢ Data Source Identification: Helps identify the most suitable source for data that is fit for the requirements.\nâ€¢ Data Modeling: Analyses complex data elements, systems, data flows, dependencies, and relationships to contribute to conceptual, physical, and logical data models.\nâ€¢ Build and maintain robust and scalable data pipelines with internal and external partners.\nâ€¢ Perform deep analysis leveraging customer datasets to build customer insights.\nâ€¢ Raise the bar on sustainable engineering by improving best practices, producing best in class of code, documentation, testing and monitoring.\nâ€¢ Participates in medium- to large-scale, complex, cross-functional projects by reviewing project requirements; translating requirements into technical solutions; gathering requested information (for example, design documents, product requirement); writing and developing code; conducting unit testing; communicating status and issues to team members and stakeholders; collaborating with cross functional teams; troubleshooting open issues and bug-fixes; enhancing design to prevent re-occurrences of defects; ensuring on-time delivery.\nâ€¢ Creates training documentation and trains end-users on data modeling. Oversees the tasks of less experienced programmers and stipulates system troubleshooting supports.\n\nWhat You'll Bring:\nâ€¢ Bachelors degree in computer science, Data Engineering, Mathematics, or a related field and 3-5 years of proven experience as a Data Engineer or similar role in data management\nâ€¢ Or, masters degree in computer science, Data Engineering, Mathematics, or a related field and 2-3 years of proven experience as a Data Engineer or similar role in data management.\nâ€¢ Proficient in real time ; batch pipelines in big data technologies (i.e.Spark/Kafka/Cassandra/Elasticsearch, No Sql Dbs etc.)\nâ€¢ Proficiency in languages such as Java , Scala , Python\nâ€¢ Experience with the workflow management platform like Airflow/hudi.\nâ€¢ Experience with Cloud computing platforms like GCP and Azure.\nâ€¢ Proficient with Designing and building APIs\nâ€¢ Hands on Experience with PL/SQL to query data ; generate insights using complex queries, stored procedures, and user defined functions.\nâ€¢ Security, API Gateway.\nâ€¢ Familiarity with Engineering practices in Analytics space using tools like Power Bi, Tableau.\nâ€¢ Experience with data modeling ; data migration protocols.\nâ€¢ Excellent problem-solving and analytical skills.\nâ€¢ Release ; Deployment , CI/CD\nâ€¢ Code/ Design reviews\nâ€¢ Strong project management and organizational skills.\nâ€¢ Ability to work in a fast-paced environment and manage multiple priorities.\nâ€¢ Lead the discovery phase of medium to large projects to come up with high-level design.\n\nAbout Sams Club\nSam Walton opened the first Sams Club in 1983 to meet a growing need among customers who wanted to buy merchandise in bulk. Since then, Sams Club has grown rapidly, opening more than 600 clubs in the U.S. and 100 clubs internationally. By offering affordable, wholesale merchandise to members, Sams Club helps make saving simple for families and small business owners. Sams Club employs about 110,000 associates in the U.S. The average club is 134,000 square feet and offers bulk groceries and general merchandise. Most clubs also have specialty services, such as a pharmacy, an optical department, a photo center, or a tire and battery center.\n\nFlexible, hybrid work:\nWe use a hybrid way of working that is primarily in office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives.\n\nBenefits:\nBenefits: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more.\n\nEqual Opportunity Employer:\nWalmart, Inc. is an Equal Opportunity Employer By Choice. We believe we are best equipped to help our associates, customers and the\n\ncommunities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions while being welcoming of all people.\n\nThe above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.\n\nAt Walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet. Health benefits include medical, vision and dental coverage. Financial benefits include 401(k), stock purchase and company-paid life insurance. Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting. Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more.\n\n\u200e\n\n\u200e\n\n\u200e\nYou will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes. The amount you receive depends on your job classification and length of employment. It will meet or exceed the requirements of paid sick leave laws, where applicable.\n\n\u200e\n\nFor information about PTO, see https://one.walmart.com/notices.\n\n\u200e\n\n\u200e\nLive Better U is a Walmart-paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities. Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates. Tuition, books, and fees are completely paid for by Walmart.\n\n\u200e\nEligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to a specific plan or program terms.\n\n\u200e\n\nFor information about benefits and eligibility, see One.Walmart.\n\n\u200e\nThe annual salary range for this position is $90,000.00-$180,000.00\n\n\u200e\nAdditional compensation includes annual or quarterly performance bonuses.\n\n\u200e\nAdditional compensation for certain positions may also include:\n\n\u200e\n\n\u200e\n- Stock\n\n\u200e\n\n\u200e\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nOption 1: Bachelorâ€™s degree in Computer Science and 2 years' experience in software engineering or related field. Option 2: 4 yearsâ€™ experience in\nsoftware engineering or related field. Option 3: Master's degree in Computer Science.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nData engineering, database engineering, business intelligence, or business analytics, Masterâ€™s degree in Computer Science or related field and 2 years' experience in software engineering or related field, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly. The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmartâ€™s accessibility standards and guidelines for supporting an inclusive culture.\n\nPrimary Location...\n\n2101 Se Simple Savings Dr, Bentonville, AR 72712-4304, United States of America\n\nWalmart and its subsidiaries are committed to maintaining a drug-free workplace and has a no tolerance policy regarding the use of illegal drugs and alcohol on the job. This policy applies to all employees and aims to create a safe and productive work environment.", 'job_highlights': [{'title': 'Qualifications', 'items': ['Bachelors degree in computer science, Data Engineering, Mathematics, or a related field and 3-5 years of proven experience as a Data Engineer or similar role in data management', 'Or, masters degree in computer science, Data Engineering, Mathematics, or a related field and 2-3 years of proven experience as a Data Engineer or similar role in data management', 'Proficient in real time ; batch pipelines in big data technologies (i.e.Spark/Kafka/Cassandra/Elasticsearch, No Sql Dbs etc.)', 'Proficiency in languages such as Java , Scala , Python', 'Experience with the workflow management platform like Airflow/hudi', 'Experience with Cloud computing platforms like GCP and Azure', 'Proficient with Designing and building APIs', 'Hands on Experience with PL/SQL to query data ; generate insights using complex queries, stored procedures, and user defined functions', 'Security, API Gateway', 'Familiarity with Engineering practices in Analytics space using tools like Power Bi, Tableau', 'Experience with data modeling ; data migration protocols', 'Excellent problem-solving and analytical skills', 'Release ; Deployment , CI/CD', 'Code/ Design reviews', 'Strong project management and organizational skills', 'Ability to work in a fast-paced environment and manage multiple priorities', 'Lead the discovery phase of medium to large projects to come up with high-level design', "Option 1: Bachelorâ€™s degree in Computer Science and 2 years' experience in software engineering or related field", 'Option 2: 4 yearsâ€™ experience in', 'software engineering or related field', "Option 3: Master's degree in Computer Science", "Data engineering, database engineering, business intelligence, or business analytics, Masterâ€™s degree in Computer Science or related field and 2 years' experience in software engineering or related field, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly", 'The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmartâ€™s accessibility standards and guidelines for supporting an inclusive culture']}, {'title': 'Benefits', 'items': ['Benefits: Beyond our great compensation package, you can receive incentive awards for your performance', 'Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more', 'At Walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet', 'Health benefits include medical, vision and dental coverage', 'Financial benefits include 401(k), stock purchase and company-paid life insurance', 'Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting', 'Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more', 'You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes', 'The amount you receive depends on your job classification and length of employment', 'It will meet or exceed the requirements of paid sick leave laws, where applicable', "Live Better U is a Walmart-paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities", "Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates", 'Tuition, books, and fees are completely paid for by Walmart', 'Benefits are subject to change and may be subject to a specific plan or program terms', 'The annual salary range for this position is $90,000.00-$180,000.00', 'Additional compensation includes annual or quarterly performance bonuses']}, {'title': 'Responsibilities', 'items': ['Data Transformation and Integration: Extracts data from identified databases and transform data to a structure that is relevant to the problem by selecting appropriate techniques', 'Data Source Identification: Helps identify the most suitable source for data that is fit for the requirements', 'Data Modeling: Analyses complex data elements, systems, data flows, dependencies, and relationships to contribute to conceptual, physical, and logical data models', 'Build and maintain robust and scalable data pipelines with internal and external partners', 'Perform deep analysis leveraging customer datasets to build customer insights', 'Raise the bar on sustainable engineering by improving best practices, producing best in class of code, documentation, testing and monitoring', 'Participates in medium- to large-scale, complex, cross-functional projects by reviewing project requirements; translating requirements into technical solutions; gathering requested information (for example, design documents, product requirement); writing and developing code; conducting unit testing; communicating status and issues to team members and stakeholders; collaborating with cross functional teams; troubleshooting open issues and bug-fixes; enhancing design to prevent re-occurrences of defects; ensuring on-time delivery', 'Creates training documentation and trains end-users on data modeling', 'Oversees the tasks of less experienced programmers and stipulates system troubleshooting supports']}], 'apply_options': [{'title': 'ZipRecruiter', 'link': 'https://www.ziprecruiter.com/c/Walmart/Job/(USA)-Data-Engineer-III/-in-Goshen,AR?jid=d54b4356576ac98a&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiIoVVNBKSBEYXRhIEVuZ2luZWVyIElJSSIsImNvbXBhbnlfbmFtZSI6IldhbG1hcnQiLCJhZGRyZXNzX2NpdHkiOiJHb3NoZW4sIEFSIiwiaHRpZG9jaWQiOiJMaUl6QUNvSm5sWnJJVEVxQUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': 'Staff Data Engineer â€“ Intelligent Manufacturing', 'company_name': 'General Motors', 'location': 'Warren, MI (+1 other)', 'via': 'General Motors Careers', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=I4WRx5Lrj0VlYacQAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEMQrCQBAAQGzzBLHYWjQngo22SoyQysJSNsfmcnLuhr0NpPQPvsGP-RJxiik-s-J8New6OKIhnDhEJlL4vt5Qs1FKMRAbNMhjh95GjRxgDRdpIROq70EYKpGQaH7ozYa8dy7nVIZsaNGXXp5OmFqZ3EPa_O-ee1QaEhrdt7vNVA4clouKmBQTNGKiGSLDDVWJV9DUP1SbANmoAAAA&shmds=v1_AdeF8Kj1vqabSQpgy3-RejZTM0MCD1hM4jTT-fIMpaqgxiYiOA&shem=damc,dimg2&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=I4WRx5Lrj0VlYacQAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965b6392e0ba80ab52bc3/images/d3336264a45327e82eaca9a0f19e89f9c87814e793c4fa2b89bb1c5fbcdcf978.png', 'extensions': ['12 days ago', 'Full-time', 'Paid time off', 'Dental insurance', 'Health insurance'], 'detected_extensions': {'posted_at': '12 days ago', 'schedule_type': 'Full-time', 'paid_time_off': True, 'dental_coverage': True, 'health_insurance': True}, 'description': "This role is categorized as hybrid. This means the successful candidate is expected to report to Warren, MI or Austin, TX three times per week, at minimum [or other frequency dictated by the business if more than 3 days].\n\nThe Role\n\nThe Intelligent Manufacturing teams are responsible for ideating, incubating, and delivering new plant data solutions for General Motors Manufacturing and our partners. We integrate with business and IT teams to develop real-time solutions that leverage plant floor data to improve decisions, plant asset maintenance, safety, and operational performance, as well as Vehicle Build Data.\n\nAs a senior level Data Engineer, you will design and build industrialized data assets and pipelines to support Business Intelligence and Advanced Analytics objectives. In this senior-level technical leadership role, youâ€™ll bring a passion for quality, efficiency, and reliability, along with proven experience leading complex data engineering initiatives from concept to production. You will work in a highly collaborative environment across databases, streaming technology, CI/CD, cloud platforms, and modern data engineering toolsâ€”to create large, complex data sets that meet both functional and non-functional business requirements.\n\nBeyond strong data engineering skills, you should have a solid foundation in modern software engineering principlesâ€”including code quality, design patterns, testing, and CI/CDâ€”to deliver robust, maintainable, and production-ready systems. The ideal candidate combines a data-driven mindset with a strong understanding of business priorities, demonstrating creativity, sound decision-making, and the ability to influence and collaborate across teams.\n\nWhat You'll Do\nâ€¢ Provide technical leadership for complex data engineering initiatives from concept through production, ensuring solutions are high-quality, efficient, and reliable.\nâ€¢ Assemble large, complex data sets that meet both functional and non-functional business requirements.\nâ€¢ Identify, design, and implement process improvements, including automation, data delivery optimization, and redesign for greater scalability.\nâ€¢ Architect, build, and optimize highly scalable data pipelines that incorporate complex transformations and efficient, maintainable code.\nâ€¢ Design and develop new source system integrations from a variety of formats including files, database extracts, and APIs.\nâ€¢ Lead and deliver data-driven solutions across multiple languages, tools, and technologies, contributing to architecture discussions, solution design, and strategic technology adoption.\nâ€¢ Develop solutions for delivering data that consistently meets SLA requirements and supports operational excellence.\nâ€¢ Partner closely with operations teams to troubleshoot and resolve production issues, ensuring platform stability.\nâ€¢ Drive engineering excellence by applying Agile methodologies, design thinking, continuous deployment, CI/CD best practices, and performance tuning strategies.\nâ€¢ Build tooling and automation to make deployments, production monitoring, and operational support more repeatable and efficient.\nâ€¢ Collaborate with business and technology partners, providing strategic guidance, leadership, and coaching to influence outcomes and align with enterprise goals.\nâ€¢ Actively mentor peers and junior engineers, fostering a culture of learning, innovation, and continuous improvement, while educating colleagues on emerging industry trends and technologies.\nâ€¢ Represent the team in executive-level forums to communicate status, risks, opportunities, and the strategic value of data engineering initiatives.\n\nYour Skills & Abilities (Required Qualifications)\nâ€¢ Bachelorâ€™s degree in Computer Science, Software Engineering, or related field\nâ€¢ 10+ years of experience in data engineering, including Python or Scala, SQL, and relational/non-relational storage (ETL frameworks, big data processing, NoSQL)\nâ€¢ 5+ years of experience in distributed, petabyte-scale data processing with Spark and container orchestration (Kubernetes)\nâ€¢ Hands-on experience with real-time data streaming in Kubernetes and Kafka\nâ€¢ Expertise in performance tuning (partitioning, clustering, caching, serialization techniques)\nâ€¢ Proficiency with SQL, key-value datastores, and document stores\nâ€¢ Strong CI/CD expertise and best practices\nâ€¢ Background in data architecture and modeling for optimized consumption patterns\nâ€¢ Proven experience developing data models and schemas for efficient storage, retrieval, and analytics, with query performance optimization\nâ€¢ Cloud experience with at least one major platform (Azure preferred; AWS or GCP acceptable)\nâ€¢ Strong teamwork and leadership skills to collaborate and influence across product, program, and engineering teams\nâ€¢ Commitment to ensuring data security, privacy, and regulatory compliance\n\nWhat Can Give You a Competitive Advantage (Preferred Qualifications)\nâ€¢ Masterâ€™s degree in Computer Science, Software Engineering, or related field\nâ€¢ Background in data governance, metadata management, and/or data quality/observability\nâ€¢ Knowledge of schema design and data contracts\nâ€¢ Experience processing and storing media file formats (video, audio, image)\nâ€¢ Experience with tools such as Databricks or Snowflake\n\nThis job is not eligible for relocation benefits. Any relocation costs would be the responsibility of the selected candidate.\n\nCompany Vehicle: Upon successful completion of a motor vehicle report review, you will be eligible to participate in a company vehicle evaluation program, through which you will be assigned a General Motors vehicle to drive and evaluate. Note: program participants are required to purchase/lease a qualifying GM vehicle every four years unless one of a limited number of exceptions applies.\n\nCompensation:\nâ€¢ The expected base compensation for this role is: $165,000 - $270,900. Actual base compensation within the identified range will vary based on factors relevant to the position.\nâ€¢ Bonus Potential: An incentive pay program offers payouts based on company performance, job level, and individual performance.\nâ€¢ Benefits : GM offers a variety of health and wellbeing benefit programs. Benefit options include medical, dental, vision, Health Savings Account, Flexible Spending Accounts, retirement savings plan, sickness and accident benefits, life insurance, paid vacation & holidays, tuition assistance programs, employee assistance program, GM vehicle discounts and more.\n\nGM DOES NOT PROVIDE IMMIGRATION-RELATED SPONSORSHIP FOR THIS ROLE. DO NOT APPLY FOR THIS ROLE IF YOU WILL NEED GM IMMIGRATION SPONSORSHIP NOW OR IN THE FUTURE. THIS INCLUDES DIRECT COMPANY SPONSORSHIP, ENTRY OF GM AS THE IMMIGRATION EMPLOYER OF RECORD ON A GOVERNMENT FORM, AND ANY WORK AUTHORIZATION REQUIRING A WRITTEN SUBMISSION OR OTHER IMMIGRATION SUPPORT FROM THE COMPANY (e.g., H-1B, OPT, STEM OPT, CPT, TN, J-1, etc.)\n\n#LI-CC1", 'job_highlights': [{'title': 'Qualifications', 'items': ['Beyond strong data engineering skills, you should have a solid foundation in modern software engineering principlesâ€”including code quality, design patterns, testing, and CI/CDâ€”to deliver robust, maintainable, and production-ready systems', 'The ideal candidate combines a data-driven mindset with a strong understanding of business priorities, demonstrating creativity, sound decision-making, and the ability to influence and collaborate across teams', 'Bachelorâ€™s degree in Computer Science, Software Engineering, or related field', '10+ years of experience in data engineering, including Python or Scala, SQL, and relational/non-relational storage (ETL frameworks, big data processing, NoSQL)', '5+ years of experience in distributed, petabyte-scale data processing with Spark and container orchestration (Kubernetes)', 'Hands-on experience with real-time data streaming in Kubernetes and Kafka', 'Expertise in performance tuning (partitioning, clustering, caching, serialization techniques)', 'Proficiency with SQL, key-value datastores, and document stores', 'Strong CI/CD expertise and best practices', 'Background in data architecture and modeling for optimized consumption patterns', 'Proven experience developing data models and schemas for efficient storage, retrieval, and analytics, with query performance optimization', 'Strong teamwork and leadership skills to collaborate and influence across product, program, and engineering teams', 'Commitment to ensuring data security, privacy, and regulatory compliance', 'THIS INCLUDES DIRECT COMPANY SPONSORSHIP, ENTRY OF GM AS THE IMMIGRATION EMPLOYER OF RECORD ON A GOVERNMENT FORM, AND ANY WORK AUTHORIZATION REQUIRING A WRITTEN SUBMISSION OR OTHER IMMIGRATION SUPPORT FROM THE COMPANY (e.g., H-1B, OPT, STEM OPT, CPT, TN, J-1, etc.)']}, {'title': 'Benefits', 'items': ['The expected base compensation for this role is: $165,000 - $270,900', 'Actual base compensation within the identified range will vary based on factors relevant to the position', 'Bonus Potential: An incentive pay program offers payouts based on company performance, job level, and individual performance', 'Benefits : GM offers a variety of health and wellbeing benefit programs', 'Benefit options include medical, dental, vision, Health Savings Account, Flexible Spending Accounts, retirement savings plan, sickness and accident benefits, life insurance, paid vacation & holidays, tuition assistance programs, employee assistance program, GM vehicle discounts and more', 'GM DOES NOT PROVIDE IMMIGRATION-RELATED SPONSORSHIP FOR THIS ROLE']}, {'title': 'Responsibilities', 'items': ['This means the successful candidate is expected to report to Warren, MI or Austin, TX three times per week, at minimum [or other frequency dictated by the business if more than 3 days]', 'The Intelligent Manufacturing teams are responsible for ideating, incubating, and delivering new plant data solutions for General Motors Manufacturing and our partners', 'As a senior level Data Engineer, you will design and build industrialized data assets and pipelines to support Business Intelligence and Advanced Analytics objectives', 'In this senior-level technical leadership role, youâ€™ll bring a passion for quality, efficiency, and reliability, along with proven experience leading complex data engineering initiatives from concept to production', 'You will work in a highly collaborative environment across databases, streaming technology, CI/CD, cloud platforms, and modern data engineering toolsâ€”to create large, complex data sets that meet both functional and non-functional business requirements', 'Provide technical leadership for complex data engineering initiatives from concept through production, ensuring solutions are high-quality, efficient, and reliable', 'Assemble large, complex data sets that meet both functional and non-functional business requirements', 'Identify, design, and implement process improvements, including automation, data delivery optimization, and redesign for greater scalability', 'Architect, build, and optimize highly scalable data pipelines that incorporate complex transformations and efficient, maintainable code', 'Design and develop new source system integrations from a variety of formats including files, database extracts, and APIs', 'Lead and deliver data-driven solutions across multiple languages, tools, and technologies, contributing to architecture discussions, solution design, and strategic technology adoption', 'Develop solutions for delivering data that consistently meets SLA requirements and supports operational excellence', 'Partner closely with operations teams to troubleshoot and resolve production issues, ensuring platform stability', 'Drive engineering excellence by applying Agile methodologies, design thinking, continuous deployment, CI/CD best practices, and performance tuning strategies', 'Build tooling and automation to make deployments, production monitoring, and operational support more repeatable and efficient', 'Collaborate with business and technology partners, providing strategic guidance, leadership, and coaching to influence outcomes and align with enterprise goals', 'Actively mentor peers and junior engineers, fostering a culture of learning, innovation, and continuous improvement, while educating colleagues on emerging industry trends and technologies', 'Represent the team in executive-level forums to communicate status, risks, opportunities, and the strategic value of data engineering initiatives', 'Company Vehicle: Upon successful completion of a motor vehicle report review, you will be eligible to participate in a company vehicle evaluation program, through which you will be assigned a General Motors vehicle to drive and evaluate']}], 'apply_options': [{'title': 'General Motors Careers', 'link': 'https://search-careers.gm.com/en/jobs/jr-202514580/staff-data-engineer-intelligent-manufacturing/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'LinkedIn', 'link': 'https://www.linkedin.com/jobs/view/senior-data-engineer-%E2%80%93-intelligent-manufacturing-at-general-motors-4287205867?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'BeBee', 'link': 'https://us.bebee.com/job/e6de34e6f9a48a94c915e309f5807a57?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Snagajob', 'link': 'https://www.snagajob.com/jobs/1151534298?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Talentify', 'link': 'https://www.talentify.io/job/senior-data-engineer-intelligent-manufacturing-warren-michigan-us-general-motors-jr-202513442?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'J-O-B-Z', 'link': 'https://j-o-b-z.com/seo/job/133214095/mi/warren/staff-data-engineer--intelligent-manufacturing?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Job Abstracts', 'link': 'https://jobabstracts.com/Job/Single/?id1=133214095&hash=103088214040003132238012105118198223006198030165004237075206011173106171217175166004189096121181248221001212003087057210048192109244119240041002135151194090006218175039174088231209023147218024070214234008241069037167009128114137174097007045024179186061127026033048123227245109140074093159220012025099132210121160006086021167166197182231020190050243137069019204108026227243024248023011019010135203042217212249058107002086083117016221036241218134029095015246110140045081237075247182&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Expertini', 'link': 'https://us.expertini.com/jobs/job/senior-data-engineer-intelligent-manufacturing-warren-general-motors-7a7d6dc4c428/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTdGFmZiBEYXRhIEVuZ2luZWVyIOKAkyBJbnRlbGxpZ2VudCBNYW51ZmFjdHVyaW5nIiwiY29tcGFueV9uYW1lIjoiR2VuZXJhbCBNb3RvcnMiLCJhZGRyZXNzX2NpdHkiOiJXYXJyZW4sIE1JIiwiaHRpZG9jaWQiOiJJNFdSeDVMcmowVmxZYWNRQUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': 'Senior Data Engineer - Remote', 'company_name': 'UnitedHealth Group', 'location': 'Austin, TX', 'via': 'ZipRecruiter', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=CoNFir6TA4L4eZ--AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXGvQrCMBAAYFx9BKebRRsRXXQSlIqjP-BWrvVIIuldyF2hL-L7qsvHN_1MppsbcZQCRzSEE_vIRAWWcKVejH65SAtKWLoAwlCL-ESzfTDLunNONVVeDS12VSe9E6ZWRveWVv80GrBQTmjUrLerscrs5_DgaPQ6EyYLUBcZMkSGw6AWeQH35xeihcEtmAAAAA&shmds=v1_AdeF8Ki6VGpQOMuFIcINihLWaL-XaCNxCbgbeY6FfO5esgFxWQ&shem=damc,dimg2&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=CoNFir6TA4L4eZ--AAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965b6392e0ba80ab52bc3/images/d3336264a45327e8616e6d1019853effbcd09a0b451b7e7062682c023e7ca6b3.jpeg', 'extensions': ['89.9Kâ€“161K a year', 'Full-time', 'Health insurance'], 'detected_extensions': {'salary': '89.9Kâ€“161K a year', 'schedule_type': 'Full-time', 'health_insurance': True}, 'description': 'Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health optimization on a global scale. Join us to start Caring. Connecting. Growing together.\n\nOptum Insight partners with payers, providers, governments and life sciences companies to simplify and enhance clinical, administrative and financial processes through software-enabled services and analytics, while advancing value-based care. Our differentiated products, technology insights, clinical expertise and analytics support the entire health system - ultimately delivering better experiences for consumers.\n\nOptum Insight Technology and Engineering is a critical function in Optum Insight driving the innovation and value we provide our customers and partners. This team is focused on products, solutions, platform / enabling capability development, product development lifecycle, engineering excellence and connectivity to Optum Technology.\n\nAs Sr Data Engineer, you will play a role in implementing the data requirements, ETL design, development, implementation, and operations/maintenance. This position will also support data architecture, assure data quality/integrity for analytics/business intelligence reports and workflow based application. Youâ€™ll have the opportunity to work on complex ETL/data development tasks and support a talented team of data engineers and application developers.\n\nYouâ€™ll enjoy the flexibility to work remotely * from anywhere within the U.S. as you take on some tough challenges.\n\nPrimary Responsibilities:\nâ€¢ Design, develop, and maintain scalable data/code pipelines using Azure Databricks, Apache Spark, and Scala\nâ€¢ Collaborate with architects, data engineers and business stakeholders to understand data requirements and deliver high-quality data solutions\nâ€¢ Optimize and tune Spark applications for performance and scalability\nâ€¢ Ensure data quality, integrity, and security throughout the data lifecycle\nâ€¢ Implement data processing workflows, ETL processes, and data integration solutions\nâ€¢ Participate in code reviews and provide meaningful feedback to improve code quality\nâ€¢ Understand product architecture, features being built and come up with product improvement ideas and POCs\nâ€¢ Ensure data quality, integrity, and security throughout the data lifecycle\nâ€¢ Troubleshoot and resolve issues related to data processing and pipeline failures\nâ€¢ Participate in product support activities as needed by the team\nâ€¢ Stay updated with the latest industry trends and best practices in big data technologies\nâ€¢ Serves as a resource to others\n\nYouâ€™ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.\n\nRequired Qualifications:\nâ€¢ Bachelorâ€™s degree in Computer Science or Information Technology\nâ€¢ 5 years of working experience with Databricks, Spark, Scala, Cloud databases / data warehouse, Azure DeltaLake, Azure DataFactory, CI/CD\nâ€¢ Implementing end to end features and user stories from analysis/design, building of feature, validation and deployment and post deployment support\nâ€¢ Solid understanding of Data modelling, Databases and Data Warehousing concepts\nâ€¢ Understanding agile of methodology\n\nPreferred Qualifications:\nâ€¢ Experience with Dot NET/C#\nâ€¢ Experience with query optimization techniques\nâ€¢ Solid troubleshooting skills\nâ€¢ Solid written and verbal communication skills working with technical and non-technical audience\nâ€¢ All employees working remotely will be required to adhere to UnitedHealth Groupâ€™s Telecommuter Policy.\n\nPay is based on several factors including but not limited to local labor markets, education, work experience, certifications, etc. In addition to your salary, we offer benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with us, youâ€™ll find a far-reaching choice of benefits and incentives. The salary for this role will range from $89,900 to $160,600 annually based on full-time employment. We comply with all minimum wage laws as applicable.\n\nApplication Deadline: This will be posted for a minimum of 2 business days or until a sufficient candidate pool has been collected. Job posting may come down early due to volume of applicants.\n\nAt UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyoneâ€“of every race, gender, sexuality, age, location and incomeâ€“deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes - an enterprise priority reflected in our mission.\n\nUnitedHealth Group is an Equal Employment Opportunity employer under applicable law and qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status, or any other characteristic protected by local, state, or federal laws, rules, or regulations.\n\nUnitedHealth Group is a drug - free workplace. Candidates are required to pass a drug test before beginning employment.', 'job_highlights': [{'title': 'Qualifications', 'items': ['from anywhere within the U.S. as you take on some tough challenges', 'Bachelorâ€™s degree in Computer Science or Information Technology', '5 years of working experience with Databricks, Spark, Scala, Cloud databases / data warehouse, Azure DeltaLake, Azure DataFactory, CI/CD', 'Implementing end to end features and user stories from analysis/design, building of feature, validation and deployment and post deployment support', 'Solid understanding of Data modelling, Databases and Data Warehousing concepts', 'Understanding agile of methodology', 'Candidates are required to pass a drug test before beginning employment']}, {'title': 'Benefits', 'items': ['Youâ€™ll enjoy the flexibility to work remotely', 'Pay is based on several factors including but not limited to local labor markets, education, work experience, certifications, etc', 'In addition to your salary, we offer benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements)', 'No matter where or when you begin a career with us, youâ€™ll find a far-reaching choice of benefits and incentives', 'The salary for this role will range from $89,900 to $160,600 annually based on full-time employment']}, {'title': 'Responsibilities', 'items': ['As Sr Data Engineer, you will play a role in implementing the data requirements, ETL design, development, implementation, and operations/maintenance', 'This position will also support data architecture, assure data quality/integrity for analytics/business intelligence reports and workflow based application', 'Youâ€™ll have the opportunity to work on complex ETL/data development tasks and support a talented team of data engineers and application developers', 'Design, develop, and maintain scalable data/code pipelines using Azure Databricks, Apache Spark, and Scala', 'Collaborate with architects, data engineers and business stakeholders to understand data requirements and deliver high-quality data solutions', 'Optimize and tune Spark applications for performance and scalability', 'Ensure data quality, integrity, and security throughout the data lifecycle', 'Implement data processing workflows, ETL processes, and data integration solutions', 'Participate in code reviews and provide meaningful feedback to improve code quality', 'Understand product architecture, features being built and come up with product improvement ideas and POCs', 'Ensure data quality, integrity, and security throughout the data lifecycle', 'Troubleshoot and resolve issues related to data processing and pipeline failures', 'Participate in product support activities as needed by the team', 'Stay updated with the latest industry trends and best practices in big data technologies', 'Serves as a resource to others', 'Youâ€™ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in']}], 'apply_options': [{'title': 'ZipRecruiter', 'link': 'https://www.ziprecruiter.com/c/UnitedHealth-Group/Job/Senior-Data-Engineer-Remote/-in-Eden-Prairie,MN?jid=8e2025b8fbbc1b2f&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'USNLX Virtual Jobs - National Labor Exchange', 'link': 'https://virtualjobs.usnlx.com/austin-tx/senior-data-engineer-remote/3AC572B1380C4FA9B2184F6281EDAF5A/job/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Careers At UnitedHealth Group', 'link': 'https://uhg.cert.xcc.smashfly.io/job/22255433/senior-data-engineer-remote-austin-tx/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Hispanic Alliance For Career Enhancement', 'link': 'https://jobs.haceonline.org/job/senior-data-engineer-remote/81047768/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'ðŸ’» Remote Health Care Jobs', 'link': 'https://jobs.healthjob.org/organizations/optum/jobs/senior-data-engineer-remote-Jac1b3da66fb940ee9549b9de363b97b1_OHcJwcZn2SGlzLAIPaqfx?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Expertini', 'link': 'https://us.expertini.com/jobs/job/senior-data-engineer-remote-austin-unitedhealth-group-uhcg-22255433/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBFbmdpbmVlciAtIFJlbW90ZSIsImNvbXBhbnlfbmFtZSI6IlVuaXRlZEhlYWx0aCBHcm91cCIsImFkZHJlc3NfY2l0eSI6IkF1c3RpbiwgVFgiLCJodGlkb2NpZCI6IkNvTkZpcjZUQTRMNGVaLS1BQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Data Engineer', 'company_name': 'Amentum', 'location': 'Fort Liberty, NC', 'via': 'Amentum Careers', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=zAVgfT8E_0evBsuQAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEOwoCMRAAUGz3CNpMYSWaiGDjVuIPRLzCkixDEklmQmaEtfPo4ite9511y7NTBxcKiRAbbODOHgRdGyMwwY05ZJz3UbXKwVqRbIKo0zSakYtlQs-TfbGXf4NE17Bmpzjs9tvJVAqrxbEg6btAIrhyU3gkj00_a3iefgphxEWDAAAA&shmds=v1_AdeF8KgniOuU_cOYdymfVVVwjDvx51gNOeSQ-ZSQIVZgIei2xw&shem=damc,dimg2&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=zAVgfT8E_0evBsuQAAAAAA%3D%3D', 'extensions': ['Full-time'], 'detected_extensions': {'schedule_type': 'Full-time'}, 'description': 'Amentum is seeking a Data Engineer to support the 389th MI BN Process Exploit and Disseminate (PED) Information collected from identified sensors in Ft. Bragg, NC. The Data Engineer will provide support to ensure the integration and visualization of large data sources to support PED operations and all-source analysis.\n\nEssential Responsibilities:\nâ€¢ Provide database engineer support to develop and translate computer algorithms into prototype code and maintain, organize, and identify trends in large data\nâ€¢ Support complex IT architecture projects with full competency in data gathering, artifact development, presentation, and completion of enterprise architect (EA) models. The contractor shall apply hardware engineering and software design theories and principles to researching, developing, and reviewing EA products. The contractor shall recommend innovative technologies to enhance the presentation and usage of EA products. The contractor shall provide integrated systems planning. The contractor shall coordinate project team activities and assist with monitoring project schedules and costs.\nâ€¢ Formulate and lead guided, multifaceted analytic studies against large volumes of data to interpret and analyze data using exploratory mathematic and statistical techniques based on the scientific method to support PED operations.\nâ€¢ Use technology to mine complex, voluminous, and different types of data from various sources and platforms to collect, analyze, and compile data to meet customer needs.\n\nMinimum Education/Experience:\nâ€¢ 5+ years of experience in one or more of the following: Business Analysis, Army Special Operations, Intelligence and/or Information Management/Know ledge Management\nâ€¢ 5+ years of experience supporting the United States Military preferably SOF elements.\nâ€¢ BA/BS from an accredited institution, or former Officer, NCO or Warrant Officer with Military Experience or Intelligence/Knowledge Management background\n\nRequired Specialized Qualifications:\nâ€¢ 5+ years of experience with Single Page Application Development and client-side coding, including Jscript, React, Angular, Aurelia, Vue, Ajax, JSON, or REST, such as OData, HTML, or CSS.\nâ€¢ 5+ years of experience with two or more of the following: C#, Python, PHP, or Java Knowledge of database architecture and data transformations.\n\nAmentum is proud to be an Equal Opportunity Employer. Our hiring practices provide equal opportunity for employment without regard to race, religion, color, sex, gender, national origin, age, United States military veteranâ€™s status, ancestry, sexual orientation, gender identity, marital status, family structure, medical condition including genetic characteristics or information, veteran status, or mental or physical disability so long as the essential functions of the job can be performed with or without reasonable accommodation, or any other protected category under federal, state, or local law. Learn more about your rights under Federal EEO laws and supplemental language at EEO including Disability/Protected Veterans and Labor Laws Posters.', 'job_highlights': [{'title': 'Qualifications', 'items': ['5+ years of experience in one or more of the following: Business Analysis, Army Special Operations, Intelligence and/or Information Management/Know ledge Management', '5+ years of experience supporting the United States Military preferably SOF elements', 'BA/BS from an accredited institution, or former Officer, NCO or Warrant Officer with Military Experience or Intelligence/Knowledge Management background', '5+ years of experience with Single Page Application Development and client-side coding, including Jscript, React, Angular, Aurelia, Vue, Ajax, JSON, or REST, such as OData, HTML, or CSS', '5+ years of experience with two or more of the following: C#, Python, PHP, or Java Knowledge of database architecture and data transformations']}, {'title': 'Responsibilities', 'items': ['Amentum is seeking a Data Engineer to support the 389th MI BN Process Exploit and Disseminate (PED) Information collected from identified sensors in Ft', 'The Data Engineer will provide support to ensure the integration and visualization of large data sources to support PED operations and all-source analysis', 'Provide database engineer support to develop and translate computer algorithms into prototype code and maintain, organize, and identify trends in large data', 'Support complex IT architecture projects with full competency in data gathering, artifact development, presentation, and completion of enterprise architect (EA) models', 'The contractor shall apply hardware engineering and software design theories and principles to researching, developing, and reviewing EA products', 'The contractor shall recommend innovative technologies to enhance the presentation and usage of EA products', 'The contractor shall provide integrated systems planning', 'The contractor shall coordinate project team activities and assist with monitoring project schedules and costs', 'Formulate and lead guided, multifaceted analytic studies against large volumes of data to interpret and analyze data using exploratory mathematic and statistical techniques based on the scientific method to support PED operations', 'Use technology to mine complex, voluminous, and different types of data from various sources and platforms to collect, analyze, and compile data to meet customer needs']}], 'apply_options': [{'title': 'Amentum Careers', 'link': 'https://www.amentumcareers.com/jobs/data-engineer-fort-bragg-north-carolina-united-states-1f08625d-7476-4e59-894d-4b0340cdd5c0?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Indeed', 'link': 'https://www.indeed.com/viewjob?jk=db8fec20eba9dde6&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'ZipRecruiter', 'link': 'https://www.ziprecruiter.com/c/Prescient-Edge/Job/Data-Engineer/-in-Fort-Bragg,NC?jid=1b9e10357de40ac1&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Ladders', 'link': 'https://www.theladders.com/job/data-engineer-amentum-fort-bragg-nc_80532285?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Getwork', 'link': 'https://www.getwork.com/details/5098972359?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobilize', 'link': 'https://www.jobilize.com/job/us-nc-fort-bragg-data-engineer-prescient-edge-hiring-now-job-immediately?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Learn4Good', 'link': 'https://www.learn4good.com/jobs/fort-bragg/north-carolina/security/4549629499/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoiQW1lbnR1bSIsImFkZHJlc3NfY2l0eSI6IkZvcnQgTGliZXJ0eSwgTkMiLCJodGlkb2NpZCI6InpBVmdmVDhFXzBldkJzdVFBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Data Engineer (Intern) United States', 'company_name': 'Cisco', 'location': 'San Jose, CA', 'via': 'Teal', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=zLquEF4d3a7nnR_FAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMsQrCMBCAYVy7uzjdqKKJCC7tJFVE1-JckngkkXpXcjf0NXxj6_JPP1_1XVTNxamDK8VMiAXWd1IstIEnZcUXdOoUBfbwYA-CroQETHBjjgOumqQ6Sm2tyGCizG8OJvDHMqHnyb7Zyz-9JFdwHGarP54OkxkpbpdtlsCQCTpHsy-4g_b8A9uNefCUAAAA&shmds=v1_AdeF8Kh1nFf_8P-nJkYbdAIq-GAlil60-4yRCHlVKPthH-uM2g&shem=damc,dimg2&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=zLquEF4d3a7nnR_FAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965b6392e0ba80ab52bc3/images/d3336264a45327e8767eaa4ef8bbdf4ab1a03ee403f40620657b16038d2562a3.png', 'extensions': ['Internship', 'Paid time off', 'Dental insurance', 'Health insurance'], 'detected_extensions': {'schedule_type': 'Internship', 'paid_time_off': True, 'dental_coverage': True, 'health_insurance': True}, 'description': "About the position\n\nResponsibilities\nâ€¢ Provide leadership and support for the development of integrated data foundations for business intelligence and machine learning.\nâ€¢ Collaborate with business partners and IT teams to ensure high-quality, on-time deliverables.\nâ€¢ Provide hands-on technical support for development, research, and quality assurance testing.\nâ€¢ Apply various technologies to ingest, transform, index, aggregate, and visualize data.\nâ€¢ Analyze structural requirements for data storage solutions.\n\nRequirements\nâ€¢ Upcoming graduate of a technical degree or certification program.\nâ€¢ Recently completed or actively enrolled in a Bachelor's or Master's degree program in Engineering, Computer Science, Data Science, Statistics, or a related field.\nâ€¢ Proficient in programming languages: Python, Java, or Scala.\nâ€¢ Experience with databases: MySQL, PostgreSQL, MongoDB.\nâ€¢ Experience with big-data processing frameworks: Hadoop, Spark.\nâ€¢ Experience with cloud services such as AWS, Microsoft Azure, or Google Cloud Platform.\nâ€¢ Able to legally live and work in the country for which you are applying, without visa support or sponsorship.\n\nNice-to-haves\nâ€¢ Experience with RESTful API design and development for data integration.\nâ€¢ Experience with business intelligence tools such as Tableau.\nâ€¢ Understanding of security standards related to data encryption and secure data transfer.\nâ€¢ Knowledge of full-stack development for building comprehensive data solutions.\n\nBenefits\nâ€¢ Medical, dental, and vision insurance.\nâ€¢ 401(k) plan with Cisco matching contribution.\nâ€¢ Short and long-term disability coverage.\nâ€¢ Basic life insurance.\nâ€¢ Numerous wellbeing offerings.\nâ€¢ Paid holidays and vacation time off.\nâ€¢ Sick time off policy.\nâ€¢ Paid time to volunteer and give back to the community.", 'job_highlights': [{'title': 'Qualifications', 'items': ['Upcoming graduate of a technical degree or certification program', "Recently completed or actively enrolled in a Bachelor's or Master's degree program in Engineering, Computer Science, Data Science, Statistics, or a related field", 'Proficient in programming languages: Python, Java, or Scala', 'Experience with databases: MySQL, PostgreSQL, MongoDB', 'Experience with big-data processing frameworks: Hadoop, Spark', 'Experience with cloud services such as AWS, Microsoft Azure, or Google Cloud Platform', 'Able to legally live and work in the country for which you are applying, without visa support or sponsorship', 'Experience with RESTful API design and development for data integration', 'Experience with business intelligence tools such as Tableau', 'Understanding of security standards related to data encryption and secure data transfer', 'Knowledge of full-stack development for building comprehensive data solutions']}, {'title': 'Benefits', 'items': ['Medical, dental, and vision insurance', '401(k) plan with Cisco matching contribution', 'Short and long-term disability coverage', 'Basic life insurance', 'Numerous wellbeing offerings', 'Paid holidays and vacation time off', 'Sick time off policy', 'Paid time to volunteer and give back to the community']}, {'title': 'Responsibilities', 'items': ['Provide leadership and support for the development of integrated data foundations for business intelligence and machine learning', 'Collaborate with business partners and IT teams to ensure high-quality, on-time deliverables', 'Provide hands-on technical support for development, research, and quality assurance testing', 'Apply various technologies to ingest, transform, index, aggregate, and visualize data', 'Analyze structural requirements for data storage solutions']}], 'apply_options': [{'title': 'Teal', 'link': 'https://www.tealhq.com/job/data-engineer-intern-united-states_097ec972-9185-46e5-ac70-9961415a5e92?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChJbnRlcm4pIFVuaXRlZCBTdGF0ZXMiLCJjb21wYW55X25hbWUiOiJDaXNjbyIsImFkZHJlc3NfY2l0eSI6IlNhbiBKb3NlLCBDQSIsImh0aWRvY2lkIjoiekxxdUVGNGQzYTdublJfRkFBQUFBQT09IiwiaGwiOiJlbiJ9'}, {'title': 'Senior Data Analytics Engineer', 'company_name': 'CACI', 'location': 'Linthicum Heights, MD', 'via': 'CACI Careers', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=fyoM6OvnsoI5x3P9AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXLSwrCMBAAUNz2CIIwa9FGhG50VVrxg648QEnDkKSkMyUzQj2HF1Y3b_eKz6KonkiRM7RWLdRk01ujEziRj4SYYQs37kHQZheACc7MPuHyGFQnORgjkkovan-rdDwaJux5NgP38qeTYDNOySp2-2o3lxP59aqpmytEgnskDdG9Rrhg9EFlA4_2C5ku9jeWAAAA&shmds=v1_AdeF8KgpV3voUJia1Ss8bBeVw9avp988T-qnCEmJsDGdunH5zg&shem=damc,dimg2&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=fyoM6OvnsoI5x3P9AAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965b6392e0ba80ab52bc3/images/d3336264a45327e8f65327766345e42538d6441097f3e37425afc7099f20ada9.png', 'extensions': ['Full-time', 'Health insurance', 'Paid time off'], 'detected_extensions': {'schedule_type': 'Full-time', 'health_insurance': True, 'paid_time_off': True}, 'description': "Senior Data Analytics Engineer\n\nJob Category: Information Technology\n\nTime Type: Full time\n\nMinimum Clearance Required to Start: TS/SCI with Polygraph\n\nEmployee Type: Regular\n\nPercentage of Travel Required: None\n\nType of Travel: None\nâ€¢ * *\n\nThe Opportunity:\n\nCACI is seeking a Senior Data Analytics Software Engineer to be at the forefront of transforming system security evaluation! You will play a crucial role in developing cutting-edge capabilities that automate and streamline security processes, implement continuous monitoring and assessment, and enhance network data gathering across project lifecycles. Join our innovative team and help shape the future of data management and system security!\n\nResponsibilities:\nâ€¢ Develop, maintain, and execute Pig and/or PySpark analytics\nâ€¢ Review and approve data ingest tickets and merge requests\nâ€¢ Ensure reliable and accurate data delivery to end users\nâ€¢ Manage day-to-day operations and troubleshoot data accuracy issues\nâ€¢ Contribute to the shift from manual to automated security evaluation processes\n\nQualifications:\n\nRequired:\nâ€¢ Active TS/SCI clearance with Polygraph\nâ€¢ 7+ years of experience as a Software Engineer on similar scope and complexity projects\nâ€¢ Bachelor's degree in Computer Science or related field (or 4 additional years of relevant SWE experience)\nâ€¢ Strong background in analytics development\nâ€¢ Proficiency in Pig and PySpark\n\nDesired:\nâ€¢ Experience with patch management and IAVA tracking\nâ€¢ Programming skills in Python, Java, or Scala\nâ€¢ Familiarity with NiFi and Ansible\nâ€¢ Experience working in Agile environments\n\nThis position is contingent on funding and may not be filled immediately. However, this position is representative of positions within CACI that are consistently available. Individuals who apply may also be considered for other positions at CACI.\n\n________________________________________________________________________________________\n\nWhat You Can Expect:\n\nA culture of integrity.\n\nAt CACI, we place character and innovation at the center of everything we do. As a valued team member, youâ€™ll be part of a high-performing group dedicated to our customerâ€™s missions and driven by a higher purpose â€“ to ensure the safety of our nation.\n\nAn environment of trust.\n\nCACI values the unique contributions that every employee brings to our company and our customers - every day. Youâ€™ll have the autonomy to take the time you need through a unique flexible time off benefit and have access to robust learning resources to make your ambitions a reality.\n\nA focus on continuous growth.\n\nTogether, we will advance our nation's most critical missions, build on our lengthy track record of business success, and find opportunities to break new ground â€” in your career and in our legacy.\n\nYour potential is limitless. So is ours.\n\nLearn more about CACI here.\n\n________________________________________________________________________________________\n\nPay Range: There are a host of factors that can influence final salary including, but not limited to, geographic location, Federal Government contract labor categories and contract wage rates, relevant prior work experience, specific skills and competencies, education, and certifications. Our employees value the flexibility at CACI that allows them to balance quality work and their personal lives. We offer competitive compensation, benefits and learning and development opportunities. Our broad and competitive mix of benefits options is designed to support and protect employees and their families. At CACI, you will receive comprehensive benefits such as; healthcare, wellness, financial, retirement, family support, continuing education, and time off benefits. Learn more here.\n\nThe proposed salary range for this position is:\n$103,800 - $218,100\n\nCACI is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, age, national origin, disability, status as a protected veteran, or any other protected characteristic.", 'job_highlights': [{'title': 'Qualifications', 'items': ['Minimum Clearance Required to Start: TS/SCI with Polygraph', 'Active TS/SCI clearance with Polygraph', '7+ years of experience as a Software Engineer on similar scope and complexity projects', "Bachelor's degree in Computer Science or related field (or 4 additional years of relevant SWE experience)", 'Strong background in analytics development', 'Proficiency in Pig and PySpark']}, {'title': 'Benefits', 'items': ['Percentage of Travel Required: None', 'CACI values the unique contributions that every employee brings to our company and our customers - every day', 'Youâ€™ll have the autonomy to take the time you need through a unique flexible time off benefit and have access to robust learning resources to make your ambitions a reality', 'A focus on continuous growth', 'Pay Range: There are a host of factors that can influence final salary including, but not limited to, geographic location, Federal Government contract labor categories and contract wage rates, relevant prior work experience, specific skills and competencies, education, and certifications', 'Our employees value the flexibility at CACI that allows them to balance quality work and their personal lives', 'We offer competitive compensation, benefits and learning and development opportunities', 'Our broad and competitive mix of benefits options is designed to support and protect employees and their families', 'At CACI, you will receive comprehensive benefits such as; healthcare, wellness, financial, retirement, family support, continuing education, and time off benefits', '$103,800 - $218,100']}, {'title': 'Responsibilities', 'items': ['You will play a crucial role in developing cutting-edge capabilities that automate and streamline security processes, implement continuous monitoring and assessment, and enhance network data gathering across project lifecycles', 'Join our innovative team and help shape the future of data management and system security!', 'Develop, maintain, and execute Pig and/or PySpark analytics', 'Review and approve data ingest tickets and merge requests', 'Ensure reliable and accurate data delivery to end users', 'Manage day-to-day operations and troubleshoot data accuracy issues', 'Contribute to the shift from manual to automated security evaluation processes']}], 'apply_options': [{'title': 'CACI Careers', 'link': 'https://careers.caci.com/global/en/job/CACIGLOBAL317903EXTERNALENGLOBAL/Senior-Data-Analytics-Engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'ClearedJobs.Net', 'link': 'https://clearedjobs.net/job/senior-data-analytics-engineer-linthicum-maryland-1817274?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'ZipRecruiter', 'link': 'https://www.ziprecruiter.com/c/CACI-International,-Inc./Job/Senior-Data-Analytics-Engineer/-in-Linthicum,MD?jid=c20d5f1eaf604731&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Built In', 'link': 'https://builtin.com/job/senior-data-analytics-engineer/7234029?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'LinkedIn', 'link': 'https://www.linkedin.com/jobs/view/senior-data-analytics-engineer-at-caci-international-inc-4306246733?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobs | CACI International', 'link': 'https://www.caci.jobs/DF4D6D2CDFE84ED184BC7D5B7236124325/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Glassdoor', 'link': 'https://www.glassdoor.com/job-listing/senior-data-analytics-engineer-caci-international-JV_IC1165775_KO0,30_KE31,49.htm?jl=1009887059119&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Teal', 'link': 'https://www.tealhq.com/job/senior-data-analytics-engineer_d763c47c-f08f-4e07-980f-8f2a1c93adf9?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBBbmFseXRpY3MgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJDQUNJIiwiYWRkcmVzc19jaXR5IjoiTGludGhpY3VtIEhlaWdodHMsIE1EIiwiaHRpZG9jaWQiOiJmeW9NNk92bnNvSTV4M1A5QUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': 'AWS Data Engineer at Irvine Technology Corporation United, PA', 'company_name': 'Irvine Technology Corporation', 'location': 'Pennsylvania', 'via': 'Huari Delivery', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=yGlx2AN8UNHUkAcZAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_32NQQrCMBBFcdsjuJq1aCOCG10VFdFVwYrLMo1DEokzIQmlvZ4ns17AzefB4_GLz6xoqscNjpgRTmwcE0XADJfYTwwNacvixYxwkBgkYnbCcGeX6bmEuoIVXKWDRBi1hUmdRYyn-d7mHNJOqZR8aVKeOl1qeSth6mRQL-nSb9pkMVLwmKndbNdDGdgsVv_fHUNNzGn0PbLDL8cY3APFAAAA&shmds=v1_AdeF8Kg1zJfCkCu8jSimprORYSmoI4KXsIVzjbx0ow-8NW9lFA&shem=damc,dimg2&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=yGlx2AN8UNHUkAcZAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965b6392e0ba80ab52bc3/images/d3336264a45327e8d9b676e86c1b35e122121e8abc2ed39389fa4297805d2f26.png', 'extensions': ['5 days ago', 'Full-time'], 'detected_extensions': {'posted_at': '5 days ago', 'schedule_type': 'Full-time'}, 'description': 'AWS Data Engineer job at Irvine Technology Corporation. United, PA. Job Description\n\nJob Description\n\nAWS Data Engineer\nOur client is seeking a AWS Data Engineer to build and optimize cloud-based data pipelines that power analytics and reporting across the enterprise. This role is focused on data engineering, with opportunities to explore AI/ML projects as the team grows. Youâ€™ll work in a collaborative environment where your expertise in Python, AWS services, and modern cloud data platforms will directly impact the way the business leverages data.\nâ€¢ Location: Remote (EST Hours)\nâ€¢ Compensation: This job is expected to pay about 60-65/HR W2\nâ€¢ No Visa Sponsorship Available for this role\n\nWhat Youâ€™ll Do:\nKey Responsibilities\nâ€¢ Design, build, and maintain data pipelines that move data from multiple sources into Snowflake for analytics and dashboarding\nâ€¢ Write efficient code in Python, including PySpark and AWS Glue, to transform and process large-scale datasets\nâ€¢ Develop and manage AWS Lambda functions to support real-time and event-driven data workflows\nâ€¢ Leverage AWS services such as IAM, S3, CloudWatch, and CloudFormation to ensure secure, scalable, and reliable data operations\nâ€¢ Partner with the team to enhance integration patterns, optimize data processing, and enable downstream consumption by analytics tools\n\nWhat Gets You the Job:\nQualifications\nâ€¢ Bachelorâ€™s degree in computer science or related field\nâ€¢ 5+ years of experience in data engineering with strong coding skills in Python (PySpark and Glue)\nâ€¢ Proven experience writing and deploying AWS Lambda functions\nâ€¢ Solid understanding of AWS services including IAM, S3, CloudWatch, and CloudFormation\nâ€¢ Hands-on experience with Snowflake (preferred) or Redshift for building and managing cloud-based data warehouses\nâ€¢ Strong SQL skills and experience with pipeline development, ingestion, and transformation of diverse data types\nâ€¢ Nice to have: familiarity with AI/ML concepts, LLMs, or machine learning frameworks (not a core requirement)\n\nAfter applying to this role, you may receive an invitation from our AI Recruiter, Avery to schedule a virtual meeting to learn more about your background as an initial screening for this role.\n\nIrvine Technology Corporation (ITC) connects top talent with exceptional opportunities in IT, Security, Engineering, and Design. From startups to Fortune 500s, we partner with leading companies nationwide. Our AI recruiter, Avery helps streamline the first step of your journeyâ€”so we can focus on what matters most: helping you grow. Join us. Let us ELEVATE your career!\n\nIrvine Technology Corporation provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, Irvine Technology Corporation complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities.', 'job_highlights': [{'title': 'Qualifications', 'items': ['Bachelorâ€™s degree in computer science or related field', '5+ years of experience in data engineering with strong coding skills in Python (PySpark and Glue)', 'Proven experience writing and deploying AWS Lambda functions', 'Solid understanding of AWS services including IAM, S3, CloudWatch, and CloudFormation', 'Strong SQL skills and experience with pipeline development, ingestion, and transformation of diverse data types', 'Nice to have: familiarity with AI/ML concepts, LLMs, or machine learning frameworks (not a core requirement)']}, {'title': 'Benefits', 'items': ['Compensation: This job is expected to pay about 60-65/HR W2']}, {'title': 'Responsibilities', 'items': ['This role is focused on data engineering, with opportunities to explore AI/ML projects as the team grows', 'Youâ€™ll work in a collaborative environment where your expertise in Python, AWS services, and modern cloud data platforms will directly impact the way the business leverages data', 'Location: Remote (EST Hours)', 'Design, build, and maintain data pipelines that move data from multiple sources into Snowflake for analytics and dashboarding', 'Write efficient code in Python, including PySpark and AWS Glue, to transform and process large-scale datasets', 'Develop and manage AWS Lambda functions to support real-time and event-driven data workflows', 'Leverage AWS services such as IAM, S3, CloudWatch, and CloudFormation to ensure secure, scalable, and reliable data operations', 'Partner with the team to enhance integration patterns, optimize data processing, and enable downstream consumption by analytics tools']}], 'apply_options': [{'title': 'Huari Delivery', 'link': 'https://huaridelivery.com/apply/job/aws-data-engineer-at-irvine-technology-corporation-united-pa-eVczQ0JUZk1kWWZrVjY1UmVGZUl1cVFya3c9PQ==?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJBV1MgRGF0YSBFbmdpbmVlciBhdCBJcnZpbmUgVGVjaG5vbG9neSBDb3Jwb3JhdGlvbiBVbml0ZWQsIFBBIiwiY29tcGFueV9uYW1lIjoiSXJ2aW5lIFRlY2hub2xvZ3kgQ29ycG9yYXRpb24iLCJhZGRyZXNzX2NpdHkiOiJQZW5uc3lsdmFuaWEiLCJodGlkb2NpZCI6InlHbHgyQU44VU5IVWtBY1pBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Principal/ Senior Principal Data Engineer - R10209414', 'company_name': 'Northrop Grumman', 'location': 'Linthicum Heights, MD', 'via': 'LinkedIn', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=6AF00omM-2WZy0diAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_z3IwQ4BMRCA4bh6BKe5ErZLOOC6siKI8ADSbSbtyHamaUfiyTwfLi5_vvzD92B4vGRiR8n2Bm7IJBn-BxqrFnbsiREzzOA6rxf1ejlffn2QDgra7AIIQyviexxtg2oqG2NK6Stf1Cq5ykk0wtjJyzykK7_cS7AZU28V74tV_aoS-8n4LFlDlgRtfsZoGYjhSKyB3DPCHskHLVM4NR_DJpc3uQAAAA&shmds=v1_AdeF8KjgZd4T-fbunSpyQ6JkbJ3B_qmTXOhggo7cecpvZ7ASNQ&shem=damc,dimg2&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=6AF00omM-2WZy0diAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965b6392e0ba80ab52bc3/images/d3336264a45327e87cd02fcafefe2065912531f5fbde0e2d23c8e631256f6fb5.png', 'extensions': ['116Kâ€“216K a year', 'Full-time', 'Paid time off', 'Health insurance', 'Dental insurance'], 'detected_extensions': {'salary': '116Kâ€“216K a year', 'schedule_type': 'Full-time', 'paid_time_off': True, 'health_insurance': True, 'dental_coverage': True}, 'description': "RELOCATION ASSISTANCE: Relocation assistance may be available\n\nCLEARANCE TYPE: Polygraph\n\nTRAVEL: Yes, 10% of the Time\n\nDescription\n\nAt Northrop Grumman, our employees have incredible opportunities to work on revolutionary systems that impact people's lives around the world today, and for generations to come. Our pioneering and inventive spirit has enabled us to be at the forefront of many technological advancements in our nation's history - from the first flight across the Atlantic Ocean, to stealth bombers, to landing on the moon. We look for people who have bold new ideas, courage and a pioneering spirit to join forces to invent the future, and have fun along the way. Our culture thrives on intellectual curiosity, cognitive diversity and bringing your whole self to work â€” and we have an insatiable drive to do what others think is impossible. Our employees are not only part of history, they're making history.\n\nOne of our most challenging new fields is Microelectronics Design and Applications (MDA), which combines the unique properties of superconductivity and quantum mechanics to develop radical new energy-efficient computing systems. MDA is seeking a Data Science Software Engineer with demonstrated ability to lead development of new technologies to support our innovative MDA team in support of emerging supercomputing technologies.\n\nAs a Principal/Sr. Principal Data Engineer, you will have an opportunity to be a part of a technology development organization that is collaborative, open, transparent, team-oriented, and flexible, where continuous learning is encouraged, all within a culture of design.\n\nYou will design, build, and maintain scalable data pipelines that collect, process, and store large datasets. Working closely with software engineers, data scientists, and architects, you will help deliver reliable, clean data that drives business-critical decisions across the organization.\n\nThe selected individual requires expert level data engineering skills and familiarity with software development. This role will be responsible for leading design teams in all phases of the development lifecycle including requirements, development, and test.\n\nCandidates should have a solid foundation with best practices in data architecture, ETL processes, and object-oriented programming. Candidates should be familiar with the Agile methodology of software development.\n\nRoles And Responsibilities\nâ€¢ Support internal stakeholders by driving solutions that are scalable, highly available, and consistent to optimize big data analysis capabilities.\nâ€¢ Collaborate with Product Owners, Architects, Developers, and other Data Engineers to facilitate design, definition, and implementation of functional features, conduct system level validation and verify features of product development for a new system architecture.\nâ€¢ Mentor other team members in a business technical environment and promote an environment that supports innovation and process improvement.\nâ€¢ Deliver technical design documentation, perform code reviews, and maintain robust state-of-the-art engineering practices.\n\nPlease note: Candidate's must be a US Citizen with the ability to obtain/ maintain a clearance.\n\nThis position is 100% onsite in Linthicum or Annapolis Junction, MD.\n\nBasic Qualifications For Principal Data Engineer\nâ€¢ Bachelor's degree in data science, software engineering, or related discipline with 5+ years of experience; masterâ€™s degree in data science, software engineering, or related discipline with 3+ years of experience; PhD and 1+ years of experience.\nâ€¢ Highly skilled in object-oriented Python required.\nâ€¢ Experience in optimizing SQL queries and performance tuning data pipelines.\nâ€¢ Proven ability to solve complex data problems, collaborate with cross-functional teams, and deliver high quality solutions.\nâ€¢ Experience in designing, monitoring, and troubleshooting data pipelines and infrastructure.\nâ€¢ Experience using relational databases, optimizing SQL queries, and performance tuning data pipelines.\nâ€¢ Proven ability to solve complex data problems, collaborate with cross-functional teams, and deliver high quality solutions.\nâ€¢ Deep understanding of data analysis and testing methodologies.\nâ€¢ Willingness and ability to work onsite full-time.\nâ€¢ Applicant must be a US citizen with the ability to obtain/maintain a security clearance.\n\nBasic Qualifications For Sr. Principal Data Engineer\nâ€¢ Bachelor's degree in data science, software engineering, or related discipline with 8+ years of experience; masterâ€™s degree in data science, software engineering, or related discipline with 6+ years of experience; PhD and 4+ years of experience.\nâ€¢ Highly skilled in object-oriented Python required.\nâ€¢ Experience in optimizing SQL queries and performance tuning data pipelines.\nâ€¢ Proven ability to solve complex data problems, collaborate with cross-functional teams, and deliver high quality solutions.\nâ€¢ Experience in designing, monitoring, and troubleshooting data pipelines and infrastructure.\nâ€¢ Experience using relational databases, optimizing SQL queries, and performance tuning data pipelines.\nâ€¢ Proven ability to solve complex data problems, collaborate with cross-functional teams, and deliver high quality solutions.\nâ€¢ Deep understanding of data analysis and testing methodologies.\nâ€¢ Willingness and ability to work onsite full-time.\nâ€¢ Applicant must be a US citizen with the ability to obtain/maintain a security clearance.\n\nPreferred Qualifications For Principal/Sr. Principal Data Engineer\nâ€¢ Experience working in an Agile environment.\nâ€¢ Experience working with business intelligence tools such as Tableau and JMP.\nâ€¢ Familiarity with unstructured databases such as MongoDB.\nâ€¢ Experience working with a geographically distributed team.\nâ€¢ Active clearance per business requirements.\n\nPrimary Level Salary Range: $115,800.00 - $173,800.00\n\nSecondary Level Salary Range: $144,200.00 - $216,400.00\n\nThe above salary range represents a general guideline; however, Northrop Grumman considers a number of factors when determining base salary offers such as the scope and responsibilities of the position and the candidate's experience, education, skills and current market conditions.\n\nDepending on the position, employees may be eligible for overtime, shift differential, and a discretionary bonus in addition to base pay. Annual bonuses are designed to reward individual contributions as well as allow employees to share in company results. Employees in Vice President or Director positions may be eligible for Long Term Incentives. In addition, Northrop Grumman provides a variety of benefits including health insurance coverage, life and disability insurance, savings plan, Company paid holidays and paid time off (PTO) for vacation and/or personal business.\n\nThe application period for the job is estimated to be 20 days from the job posting date. However, this timeline may be shortened or extended depending on business needs and the availability of qualified candidates.\n\nNorthrop Grumman is an Equal Opportunity Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class. For our complete EEO and pay transparency statement, please visit http://www.northropgrumman.com/EEO. U.S. Citizenship is required for all positions with a government clearance and certain other restricted positions.", 'job_highlights': [{'title': 'Qualifications', 'items': ['The selected individual requires expert level data engineering skills and familiarity with software development', 'Candidates should have a solid foundation with best practices in data architecture, ETL processes, and object-oriented programming', 'Candidates should be familiar with the Agile methodology of software development', "Please note: Candidate's must be a US Citizen with the ability to obtain/ maintain a clearance", 'Basic Qualifications For Principal Data Engineer', "Bachelor's degree in data science, software engineering, or related discipline with 5+ years of experience; masterâ€™s degree in data science, software engineering, or related discipline with 3+ years of experience; PhD and 1+ years of experience", 'Highly skilled in object-oriented Python required', 'Experience in optimizing SQL queries and performance tuning data pipelines', 'Proven ability to solve complex data problems, collaborate with cross-functional teams, and deliver high quality solutions', 'Experience in designing, monitoring, and troubleshooting data pipelines and infrastructure', 'Experience using relational databases, optimizing SQL queries, and performance tuning data pipelines', 'Proven ability to solve complex data problems, collaborate with cross-functional teams, and deliver high quality solutions', 'Deep understanding of data analysis and testing methodologies', 'Willingness and ability to work onsite full-time', 'Applicant must be a US citizen with the ability to obtain/maintain a security clearance', 'Basic Qualifications For Sr', "Bachelor's degree in data science, software engineering, or related discipline with 8+ years of experience; masterâ€™s degree in data science, software engineering, or related discipline with 6+ years of experience; PhD and 4+ years of experience", 'Highly skilled in object-oriented Python required', 'Experience in optimizing SQL queries and performance tuning data pipelines', 'Experience in designing, monitoring, and troubleshooting data pipelines and infrastructure', 'Experience using relational databases, optimizing SQL queries, and performance tuning data pipelines', 'Proven ability to solve complex data problems, collaborate with cross-functional teams, and deliver high quality solutions', 'Deep understanding of data analysis and testing methodologies', 'Willingness and ability to work onsite full-time', 'Applicant must be a US citizen with the ability to obtain/maintain a security clearance', 'Principal Data Engineer', 'Experience working in an Agile environment', 'Experience working with business intelligence tools such as Tableau and JMP', 'Familiarity with unstructured databases such as MongoDB', 'Experience working with a geographically distributed team', 'Active clearance per business requirements']}, {'title': 'Benefits', 'items': ['Primary Level Salary Range: $115,800.00 - $173,800.00', 'Secondary Level Salary Range: $144,200.00 - $216,400.00', "The above salary range represents a general guideline; however, Northrop Grumman considers a number of factors when determining base salary offers such as the scope and responsibilities of the position and the candidate's experience, education, skills and current market conditions", 'Depending on the position, employees may be eligible for overtime, shift differential, and a discretionary bonus in addition to base pay', 'Annual bonuses are designed to reward individual contributions as well as allow employees to share in company results', 'Employees in Vice President or Director positions may be eligible for Long Term Incentives', 'In addition, Northrop Grumman provides a variety of benefits including health insurance coverage, life and disability insurance, savings plan, Company paid holidays and paid time off (PTO) for vacation and/or personal business']}, {'title': 'Responsibilities', 'items': ['TRAVEL: Yes, 10% of the Time', 'Principal Data Engineer, you will have an opportunity to be a part of a technology development organization that is collaborative, open, transparent, team-oriented, and flexible, where continuous learning is encouraged, all within a culture of design', 'You will design, build, and maintain scalable data pipelines that collect, process, and store large datasets', 'Working closely with software engineers, data scientists, and architects, you will help deliver reliable, clean data that drives business-critical decisions across the organization', 'This role will be responsible for leading design teams in all phases of the development lifecycle including requirements, development, and test', 'Support internal stakeholders by driving solutions that are scalable, highly available, and consistent to optimize big data analysis capabilities', 'Collaborate with Product Owners, Architects, Developers, and other Data Engineers to facilitate design, definition, and implementation of functional features, conduct system level validation and verify features of product development for a new system architecture', 'Mentor other team members in a business technical environment and promote an environment that supports innovation and process improvement', 'Deliver technical design documentation, perform code reviews, and maintain robust state-of-the-art engineering practices', 'Proven ability to solve complex data problems, collaborate with cross-functional teams, and deliver high quality solutions']}], 'apply_options': [{'title': 'LinkedIn', 'link': 'https://www.linkedin.com/jobs/view/principal-senior-principal-data-engineer-r10209414-at-northrop-grumman-4308185776?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobrapido', 'link': 'https://us.jobrapido.com/jobpreview/4028700577293664256?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'BeBee', 'link': 'https://us.bebee.com/job/62bb167ee1cb058031df377c0e6d4ad5?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJQcmluY2lwYWwvIFNlbmlvciBQcmluY2lwYWwgRGF0YSBFbmdpbmVlciAtIFIxMDIwOTQxNCIsImNvbXBhbnlfbmFtZSI6Ik5vcnRocm9wIEdydW1tYW4iLCJhZGRyZXNzX2NpdHkiOiJMaW50aGljdW0gSGVpZ2h0cywgTUQiLCJodGlkb2NpZCI6IjZBRjAwb21NLTJXWnkwZGlBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Data Engineer Job in Plano, United States', 'company_name': 'Virtusa', 'location': 'Plano, TX', 'via': 'Virtusa', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=0fDZzK1Z8CvLzmY2AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_02NuwrCQBBFsU1tZTW16K4INtqkUAQrwQd2YRKHzco6E3ZGyI_4v0Yrm9ucw7nFe1SUWzSEHYfIRBkOUkNkOCZkmcGFo9EdToZGCvMfVcLctCAMe5GQaLJpzTpde6-aXNDBjY1r5OmFqZbeP6TW71TaYqYuDa1quVr0ruMwHV9jtpfi3-n59gGScyvzmAAAAA&shmds=v1_AdeF8Kifkfr0nFdJY4mBIwt7TpDGmC2COyv24iLp_9W3Z-Nd_w&shem=damc,dimg2&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=0fDZzK1Z8CvLzmY2AAAAAA%3D%3D', 'extensions': ['Full-time', 'No degree mentioned'], 'detected_extensions': {'schedule_type': 'Full-time', 'qualifications': 'No degree mentioned'}, 'description': 'Essential: 5 years of IT experience in Bigdata tech stack (Java Spark, Hadoop, Hive) Experience in writing SQL for any one relation database ex. Oracle Extensive knowledge in HiveSQL Hadoop, SQL Experience in DevOps process (CICD, Jira, Bitbucket)', 'job_highlights': [{'title': 'Qualifications', 'items': ['Essential: 5 years of IT experience in Bigdata tech stack (Java Spark, Hadoop, Hive) Experience in writing SQL for any one relation database ex', 'Oracle Extensive knowledge in HiveSQL Hadoop, SQL Experience in DevOps process (CICD, Jira, Bitbucket)']}], 'apply_options': [{'title': 'Virtusa', 'link': 'https://www.virtusa.com/careers/us/plano/data-platforms/data-engineer/creq221698?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIEpvYiBpbiBQbGFubywgVW5pdGVkIFN0YXRlcyIsImNvbXBhbnlfbmFtZSI6IlZpcnR1c2EiLCJhZGRyZXNzX2NpdHkiOiJQbGFubywgVFgiLCJodGlkb2NpZCI6IjBmRFp6SzFaOEN2THptWTJBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Principal Data Engineer', 'company_name': 'PepsiCo', 'location': 'Plano, TX', 'via': 'PepsiCo Careers', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=RmkdugALEr-RiVCuAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEoQ4CMQwA0GBPo1DVBDZCggEJhAR1AoG7dEuz7TLaZa04z48TnnjDdzW4sReOpWGFGxrCnVNhog57eEoAJewxgzA8RFKlzSWbNT17r1pdUkMr0UX5eGEKsvhZgv6bNGOnVtFoOp4Oi2uctuuRmparQGEYK7Ls4PX-AZwIBoKGAAAA&shmds=v1_AdeF8KhwsvAJzCDVP71qCjWycylxXfvSmRCM6NYLnsFq8Mtusg&shem=damc,dimg2&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=RmkdugALEr-RiVCuAAAAAA%3D%3D', 'extensions': ['Full-time', 'No degree mentioned', 'Health insurance', 'Paid time off', 'Dental insurance'], 'detected_extensions': {'schedule_type': 'Full-time', 'qualifications': 'No degree mentioned', 'health_insurance': True, 'paid_time_off': True, 'dental_coverage': True}, 'description': "Overview\n\nAs a member of the data engineering team, you will be the key technical expert developing and overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be an empowered member of a team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products\n\nResponsibilities\nâ€¢ Active contributor to code development in projects and services\nâ€¢ Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products\nâ€¢ Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance\nâ€¢ Responsible for implementing best practices around systems integration\nâ€¢ Security, performance and data management\nâ€¢ Empower the business by creating value through the increased adoption of data\nâ€¢ Data science and business intelligence landscape\nâ€¢ Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions\n\nCompensation and Benefits:\nâ€¢ The expected compensation range for this position is between $89,000 - $149,000\nâ€¢ Location, confirmed job-related skills, experience, and education will be considered in setting actual starting salary. Your recruiter can share more about the specific salary range during the hiring process\nâ€¢ Bonus based on performance and eligibility target payout is 10% of annual salary paid out annually\nâ€¢ Paid time off subject to eligibility, including paid parental leave, vacation, sick, and bereavement\nâ€¢ In addition to salary, PepsiCo offers a comprehensive benefits package to support our employees and their families, subject to elections and eligibility: Medical, Dental, Vision, Disability, Health, and Dependent Care Reimbursement Accounts, Employee Assistance Program (EAP), Insurance (Accident, Group Legal, Life), Defined Contribution Retirement Plan\n\nQualifications\nâ€¢ 6+ years of overall technology experience that includes at least 4+ years of hands-on software development, data engineering, and systems architecture.\nâ€¢ 4+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.\nâ€¢ 4+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).\nâ€¢ 2+ years in cloud data engineering experience in Azure(Azure Data Factory(ADF), ADLS-2, Databricks(Lakehouse, Workflow SQL, Unity catalog).\nâ€¢ Fluent with Azure cloud services. Azure or Databricks Certification is a plus.\nâ€¢ Experience with integration of multi cloud services with on-premises technologies.\nâ€¢ Experience with data modeling, data warehousing, and building high-volume ETL/ELT pipelines.\nâ€¢ Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.\nâ€¢ Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.\nâ€¢ Experience with at least one MPP database technology such as Redshift, Synapse or SnowFlake.\nâ€¢ Experience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.\nâ€¢ Experience with version control systems like Github and deployment & CI tools.\nâ€¢ Experience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.\nâ€¢ Experience with Statistical/ML techniques is a plus.\nâ€¢ Experience with building solutions in the Supply chain space(Digital Procurement, Manufacturing, Cost, Warehouse, Network Design) is a plus.\nâ€¢ Understanding of metadata management, data lineage, and data glossaries is a plus.\nâ€¢ Working knowledge of agile development, including DevOps and DataOps concepts.\nâ€¢ Familiarity with business intelligence tools (such as PowerBI).\n\nEEO Statement\n\nOur Company will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the Fair Credit Reporting Act, and all other applicable laws, including but not limited to, San Francisco Police Code Sections 4901-4919, commonly referred to as the San Francisco Fair Chance Ordinance; and Chapter XVII, Article 9 of the Los Angeles Municipal Code, commonly referred to as the Fair Chance Initiative for Hiring Ordinance.\n\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.\n\nPepsiCo is an Equal Opportunity Employer: Female / Minority / Disability / Protected Veteran / Sexual Orientation / Gender Identity\n\nIf you'd like more information about your EEO rights as an applicant under the law, please download the available EEO is the Law & EEO is the Law Supplement documents. View PepsiCo EEO Policy.\n\nPlease view our Pay Transparency Statement", 'job_highlights': [{'title': 'Qualifications', 'items': ['6+ years of overall technology experience that includes at least 4+ years of hands-on software development, data engineering, and systems architecture', '4+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools', '4+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.)', '2+ years in cloud data engineering experience in Azure(Azure Data Factory(ADF), ADLS-2, Databricks(Lakehouse, Workflow SQL, Unity catalog)', 'Fluent with Azure cloud services', 'Experience with integration of multi cloud services with on-premises technologies', 'Experience with data modeling, data warehousing, and building high-volume ETL/ELT pipelines', 'Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations', 'Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets', 'Experience with at least one MPP database technology such as Redshift, Synapse or SnowFlake', 'Experience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes', 'Experience with version control systems like Github and deployment & CI tools', 'Experience with Azure Data Factory, Azure Databricks and Azure Machine learning tools', 'Working knowledge of agile development, including DevOps and DataOps concepts', 'Familiarity with business intelligence tools (such as PowerBI)']}, {'title': 'Benefits', 'items': ['The expected compensation range for this position is between $89,000 - $149,000', 'Location, confirmed job-related skills, experience, and education will be considered in setting actual starting salary', 'Your recruiter can share more about the specific salary range during the hiring process', 'Bonus based on performance and eligibility target payout is 10% of annual salary paid out annually', 'Paid time off subject to eligibility, including paid parental leave, vacation, sick, and bereavement', 'In addition to salary, PepsiCo offers a comprehensive benefits package to support our employees and their families, subject to elections and eligibility: Medical, Dental, Vision, Disability, Health, and Dependent Care Reimbursement Accounts, Employee Assistance Program (EAP), Insurance (Accident, Group Legal, Life), Defined Contribution Retirement Plan']}, {'title': 'Responsibilities', 'items': ["As a member of the data engineering team, you will be the key technical expert developing and overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business", "You'll be an empowered member of a team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company", "As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products", 'Active contributor to code development in projects and services', 'Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products', 'Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance', 'Responsible for implementing best practices around systems integration', 'Security, performance and data management', 'Empower the business by creating value through the increased adoption of data', 'Data science and business intelligence landscape', 'Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions']}], 'apply_options': [{'title': 'PepsiCo Careers', 'link': 'https://www.pepsicojobs.com/main/jobs/399518?lang=en-us&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Indeed', 'link': 'https://www.indeed.com/viewjob?jk=fc6edd2cf29a5e8f&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'ZipRecruiter', 'link': 'https://www.ziprecruiter.com/c/PepsiCo/Job/Principal-Data-Engineer/-in-Plano,TX?jid=1caef10879608725&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Glassdoor', 'link': 'https://www.glassdoor.com/job-listing/principal-data-engineer-pepsico-JV_IC1140045_KO0,23_KE24,31.htm?jl=1009887936280&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Ladders', 'link': 'https://www.theladders.com/job/principal-data-engineer-pepsico-plano-tx_83595549?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Iitjobs', 'link': 'https://www.iitjobs.com/job/principal-data-engineer-usa-45809?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Diversity.com', 'link': 'https://jobs.diversity.com/career/2090008/principal-data-engineer-texas-tx-plano?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'BeBee', 'link': 'https://us.bebee.com/job/f609fd9b1db71462e54a5ce7294017bf?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJQcmluY2lwYWwgRGF0YSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IlBlcHNpQ28iLCJhZGRyZXNzX2NpdHkiOiJQbGFubywgVFgiLCJodGlkb2NpZCI6IlJta2R1Z0FMRXItUmlWQ3VBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Data Engineer (SQL/DBT/Python/Airflow/Databricks)', 'company_name': 'E-Solutions', 'location': 'Rahway, NJ', 'via': 'WhatJobs', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=VModKElxsfOteaHiAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKsQrCMBAAUFz7BzrdqGIbEVx0UlqUIqLWvaQhJtF4V3KRtr_jl4rLm17yHSXHXEYJBRqHWgeYVteTyPd3cRmiJRQ7Fx6eOvFfTXDqxTNIoaQGWMugLBDCgch4PdnaGFveCMHsM8NRRqcyRW9BqBvqxZMa_lOzlUG3XkZdr9bLPmvRzMdFWpH_REfI4BBu0nZyWMC5_AEVhjYrpQAAAA&shmds=v1_AdeF8KhdiXQiB--QnD4IPjpxSujxlX659pmSXrDkKfG5D7h4EQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=VModKElxsfOteaHiAAAAAA%3D%3D', 'extensions': ['2 days ago', 'Full-time', 'No degree mentioned'], 'detected_extensions': {'posted_at': '2 days ago', 'schedule_type': 'Full-time', 'qualifications': 'No degree mentioned'}, 'description': 'Job description:\nStrong proficiency in SQL and relational database concepts. Hands-on experience with DBT for data modeling and transformations. Solid knowledge of Python for scripting and automation. Expertise in Airflow for workflow orchestration. Experience with Databricks and distributed data processing. Familiarity with cloud platforms (AWS, Azure, or GCP) and data lakes .\nRahway, NJ or Westpoint, PA', 'job_highlights': [{'title': 'Qualifications', 'items': ['Strong proficiency in SQL and relational database concepts', 'Hands-on experience with DBT for data modeling and transformations', 'Solid knowledge of Python for scripting and automation', 'Expertise in Airflow for workflow orchestration', 'Experience with Databricks and distributed data processing', 'Familiarity with cloud platforms (AWS, Azure, or GCP) and data lakes ', 'Rahway, NJ or Westpoint, PA']}], 'apply_options': [{'title': 'WhatJobs', 'link': 'https://www.whatjobs.com/jobs/data-engineer-sql-dbt-python-airflow-databricks?id=2262742400&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Americanlisted.com', 'link': 'https://rahway.americanlisted.com/jobs/data-scientistautomation-engineer_8976823442.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChTUUwvREJUL1B5dGhvbi9BaXJmbG93L0RhdGFicmlja3MpIiwiY29tcGFueV9uYW1lIjoiRS1Tb2x1dGlvbnMiLCJhZGRyZXNzX2NpdHkiOiJSYWh3YXksIE5KIiwiaHRpZG9jaWQiOiJWTW9kS0VseHNmT3RlYUhpQUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': 'Cloud Data Engineer (DIRECT HIRE SECRET CLEARANCE)', 'company_name': 'Kelly Services', 'location': 'La Jolla, CA', 'via': 'MyKelly', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=T9Cq3ma3M7N6YGG6AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNsQ6CMBRA0bjyCQ7mjWqUGhMXnUhpFDUOwE5KfSk1tY-01eAf-Znics94k-8kKbil1x1yGSUIp41D9DDPi1LwGk4jUAleihr4VWRlduNiAWs4UwsBpVcdkIMjkbY4PXQx9mHPWAg21SHKaFSq6MnIYUsDe1Ab_mlCJz32VkZstrvNkPZOL2cXtPYDFfq3URjAOLjK8WOtXAHPfsJNAqSrAAAA&shmds=v1_AdeF8KhKjtHG_e7uTZelm5LExydZZlIPzDII_E5JlT8ZZ7Y5DA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=T9Cq3ma3M7N6YGG6AAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965b94c437f54538f07a4/images/c96b123ff18233f3337f991cf16b215cb6994765fccd874277072ff9e737408c.png', 'extensions': ['10 days ago', '115K a year', 'Full-time', 'Health insurance', 'Paid time off', 'Dental insurance'], 'detected_extensions': {'posted_at': '10 days ago', 'salary': '115K a year', 'schedule_type': 'Full-time', 'health_insurance': True, 'paid_time_off': True, 'dental_coverage': True}, 'description': 'Cloud Data Engineer (Direct Hire)\nâ€¢ *Up to $115K/year**\nâ€¢ 100% onsite at DISA in Ft. Meade, MD*\nâ€¢ Requires an active SECRET security clearance to start*\n\nKelly Government Solutions is hiring for a cloud data engineer supporting DISA Nexus at Ft. Meade, MD. This is a direct hire opportunity.\n\nFunctional Responsibilities:\nâ€¢ Design, develop, and maintain cloud-based data pipelines and ETL workflows using Azure Data Factory, AWS Glue, or Databricks\nâ€¢ Integrate and manage structured and unstructured data from multiple systems into centralized, secure repositories\nâ€¢ Develop and optimize data models, warehouses, and data lakes to support analytics and mission reporting\nâ€¢ Implement and enforce data governance, integrity, and access controls in compliance with DoD policies\nâ€¢ Collaborate with technical and cybersecurity teams to support data modernization and mission objectives\n\nRequired Qualifications & Experience:\nâ€¢ Bachelorâ€™s degree in Data Science, Computer Science, or related field\nâ€¢ Five (5) yearsâ€™ experience supporting IT and cloud environments preferred\nâ€¢ Active Secret Clearance required\nâ€¢ Familiarity with SQL, Power BI, and cloud data tools (Azure Data Factory, AWS Glue) preferred\n\nBenefits include CIGNA Medical Guardian for Medical, Vision, Dental, two (2) weeks of PTO, 11 federal holidays, and 401k!\n\nWhat happens next:\n\nOnce you apply, youâ€™ll proceed to next steps if your skills and experience look like a good fit. But donâ€™t worry â€“ even if this position doesnâ€™t work out, youâ€™re still in our network. That means our team of expert recruiters will have access to your profile, making your opportunities limitless.\n\n#government\n\n10030457', 'job_highlights': [{'title': 'Qualifications', 'items': ['Bachelorâ€™s degree in Data Science, Computer Science, or related field', 'Active Secret Clearance required']}, {'title': 'Benefits', 'items': ['Benefits include CIGNA Medical Guardian for Medical, Vision, Dental, two (2) weeks of PTO, 11 federal holidays, and 401k!']}, {'title': 'Responsibilities', 'items': ['Requires an active SECRET security clearance to start*', 'Design, develop, and maintain cloud-based data pipelines and ETL workflows using Azure Data Factory, AWS Glue, or Databricks', 'Integrate and manage structured and unstructured data from multiple systems into centralized, secure repositories', 'Develop and optimize data models, warehouses, and data lakes to support analytics and mission reporting', 'Implement and enforce data governance, integrity, and access controls in compliance with DoD policies', 'Collaborate with technical and cybersecurity teams to support data modernization and mission objectives']}], 'apply_options': [{'title': 'MyKelly', 'link': 'https://www.mykelly.com/job/10030457-cloud-data-engineer-direct-hire-secret-clearance-la-jolla-ca-united-states/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobcase', 'link': 'https://www.jobcase.com/jobs/view/n/U-124445141145?jlsrc=3&utm_term=Business+Analyst+(DIRECT+HIRE+SECRET+CLEARANCE)&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'WhatJobs', 'link': 'https://www.whatjobs.com/jobs/security-clearance?id=2259014700&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJDbG91ZCBEYXRhIEVuZ2luZWVyIChESVJFQ1QgSElSRSBTRUNSRVQgQ0xFQVJBTkNFKSIsImNvbXBhbnlfbmFtZSI6IktlbGx5IFNlcnZpY2VzIiwiYWRkcmVzc19jaXR5IjoiTGEgSm9sbGEsIENBIiwiaHRpZG9jaWQiOiJUOUNxM21hM003TjZZR0c2QUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': 'Data Engineer III', 'company_name': "Sam's Club", 'location': 'Springdale, AR', 'via': 'ZipRecruiter', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=nKzRUbKxU5h_qSbhAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCMBAAUFz7CeJwmyKaiOCik6hIHe0HlEs9kkh6F3IR-gP-t_iG13xnzeqKFeHGPjJRgbZtYQsPcaCEZQggDHcRn2h-CrVmPVqrmozXijUOZpDRCpOTyb7F6b9eAxbKCSv1-8NuMpn9etHhuFS4pI-DyNDlEtm_MNEGzs8f_aGxqIgAAAA&shmds=v1_AdeF8KgQXceiBxW9OC9gM_ospUsstz8loHBhY7QHSh-LGZlgoA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=nKzRUbKxU5h_qSbhAAAAAA%3D%3D', 'extensions': ['6 days ago', '90Kâ€“180K a year', 'Full-time and Part-time', 'Paid time off', 'Health insurance', 'Dental insurance'], 'detected_extensions': {'posted_at': '6 days ago', 'salary': '90Kâ€“180K a year', 'schedule_type': 'Full-time and Part-time', 'paid_time_off': True, 'health_insurance': True, 'dental_coverage': True}, 'description': "Position Summary...The Data Engineer III is responsible for designing, building, and optimizing scalable big data pipelines, architectures, and datasets that enable advanced analytics and data-driven decision-making. This role involves developing efficient data transformation and processing frameworks, managing data structures, metadata, dependencies, and workloads, and ensuring the reliability and performance of the data ecosystem. The engineer will also work extensively with unstructured datasets, applying analytical techniques to extract insights and improve data accessibility across the organization.\n\nWhat you'll do...\nâ€¢ Data Modeling: Designing and implementing data models to support structured and unstructured datasets, ensuring data integrity and efficiency.\nâ€¢ Data Extraction: Developing and optimizing data extraction processes from various sources including databases, APIs, and logs.\nâ€¢ Data Cleaning: Preprocessing and cleaning data to remove inconsistencies and improve data quality.\nâ€¢ Data Screening: Implementing data validation and quality checks to ensure accuracy and completeness of data.\nâ€¢ Data Exploration: Conducting exploratory data analysis to understand patterns, trends, and correlations in the data.\nâ€¢ Data Visualization: Creating visualizations using tools like Tableau, PowerBI, or Looker to communicate insights and findings effectively.\nâ€¢ Big Data Technologies: Utilizing tools and frameworks such as Spark, Spark SQL, PySpark, HDFS, and MapReduce for processing large datasets efficiently.\nâ€¢ Cloud Services: Leveraging cloud platforms like GCP, Azure/AWS, Databricks, Azure HD Insights, ADF for data storage, processing, and analytics.\nâ€¢ Data Querying: Writing advanced SQL queries to extract and manipulate data from relational databases and other data stores.\nâ€¢ Data Pipeline Development: Building and optimizing scalable data pipelines and architectures to move and transform data across systems.\nâ€¢ Data Transformation: Developing processes for data transformation, structure, metadata, dependency, and workload management.\nâ€¢ Enterprise Software Development: Contributing to the development of enterprise-level software products related to data engineering and analytics.\n\nWhat you'll bring:\nâ€¢ Cross-functional Collaboration: Working closely with cross-functional teams including data scientists, analysts, and software engineers to achieve common goals.\nâ€¢ Programming Languages: Proficiency in at least one scripting language like Python or Scala for automation, data manipulation, and tool development.\nâ€¢ Agile Environment: Collaborating effectively in an Agile environment, participating in sprints, and adapting to changing\nâ€¢ Analytical Skills: Applying strong analytical skills to work with complex and unstructured datasets, extracting valuable insights and actionable information. project requirements.\nâ€¢ Big Data Data Stores: Implementing and managing highly scalable big data stores to efficiently store and access large volumes of data.\nâ€¢ Data Value Extraction: Manipulating, processing, and extracting value from large, diverse datasets to drive business decisions and innovation.\nâ€¢ Big Data Technologies: Experience utilizing tools and frameworks such as Spark, Spark SQL, PySpark, HDFS, and MapReduce for processing large datasets efficiently.\nâ€¢ Cloud Services: Experience leveraging cloud platforms like GCP, Azure/AWS, Databricks, Azure HD Insights, ADF for data storage, processing, and analytics.\n\nAbout Walmart General/Not Function Specific\nSam Walton opened the first Sam's Club in 1983 to meet a growing need among customers who wanted to buy merchandise in bulk. Since then, Sam's Club has grown rapidly, opening more than 600 clubs in the U.S. and 100 clubs internationally. By offering affordable, wholesale merchandise to members, Sam's Club helps make saving simple for families and small business owners. Sam's Club employs about 110,000 associates in the U.S. The average club is 134,000 square feet and offers bulk groceries and general merchandise. Most clubs also have specialty services, such as a pharmacy, an optical department, a photo center, or a tire and battery center.\n\n\u200b\u200b\u200bFuture Ways of Working:\nOur company's success can be attributed to our employees. While technology has allowed us to be effective while working remotely, there is no substitute for being in the office together; it helps to shape our culture, collaborate, innovate, build relationships, and move more quickly. We strive to provide flexibility in order to promote a healthy work-life balance but recognize that in-person interactions are important to our culture and shared success. We'll meet in person on a regular and purposeful basis.\n\nBenefits:\nBenefits: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more.\n\nEqual Opportunity Employer:\nWalmart, Inc. is an Equal Opportunity Employer â€“ By Choice. We believe we are best equipped to help our associates, customers, and the communities we serve live better when we really know them. That means understanding, respecting, and valuing unique styles, experiences, identities, ideas, and opinions â€“ while being inclusive of all people.\n\nThe above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.\n\nAt Sam's Club, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet!\n\n\u200e\n- Health benefits include medical, vision and dental coverage\n\n\u200e\n- Financial benefits include 401(k), stock purchase and company-paid life insurance\n\n\u200e\n- Paid time off benefits include PTO, parental leave, family care leave, bereavement, jury duty, and voting. You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes. The amount you receive depends on your job classification and length of employment. It will meet or exceed the requirements of paid sick leave laws, where applicable.\n\n\u200e\n\nFor information about PTO, see https://one.walmart.com/notices.\n\n\u200e\n- Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more.\n\n\u200e\nLive Better U is a company paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities. Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates. Tuition, books, and fees are completely paid for by Walmart.\n\n\u200e\nEligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to a specific plan or program terms.\n\n\u200e\n\nFor information about benefits and eligibility, see One.Walmart.\n\n\u200e\nThe annual salary range for this position is $90,000.00-$180,000.00\n\n\u200e\nAdditional compensation includes annual or quarterly performance bonuses.\n\n\u200e\n\n\u200e\n\n\u200e\n\n\u200e\n\n\u200e\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nOption 1: Bachelorâ€™s degree in Computer Science and 2 years' experience in software engineering or related field. Option 2: 4 yearsâ€™ experience in\nsoftware engineering or related field. Option 3: Master's degree in Computer Science.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nData engineering, database engineering, business intelligence, or business analytics, Masterâ€™s degree in Computer Science or related field and 2 years' experience in software engineering or related field, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly. The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmartâ€™s accessibility standards and guidelines for supporting an inclusive culture.\n\nPrimary Location...\n\n2101 Se Simple Savings Dr, Bentonville, AR 72712-4304, United States of America\n\nWalmart and its subsidiaries are committed to maintaining a drug-free workplace and has a no tolerance policy regarding the use of illegal drugs and alcohol on the job. This policy applies to all employees and aims to create a safe and productive work environment.", 'job_highlights': [{'title': 'Qualifications', 'items': ['Cross-functional Collaboration: Working closely with cross-functional teams including data scientists, analysts, and software engineers to achieve common goals', 'Programming Languages: Proficiency in at least one scripting language like Python or Scala for automation, data manipulation, and tool development', 'Agile Environment: Collaborating effectively in an Agile environment, participating in sprints, and adapting to changing', 'Analytical Skills: Applying strong analytical skills to work with complex and unstructured datasets, extracting valuable insights and actionable information', 'Big Data Technologies: Experience utilizing tools and frameworks such as Spark, Spark SQL, PySpark, HDFS, and MapReduce for processing large datasets efficiently', "Option 1: Bachelorâ€™s degree in Computer Science and 2 years' experience in software engineering or related field", 'Option 2: 4 yearsâ€™ experience in', 'software engineering or related field', "Option 3: Master's degree in Computer Science", "Data engineering, database engineering, business intelligence, or business analytics, Masterâ€™s degree in Computer Science or related field and 2 years' experience in software engineering or related field, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly", 'The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmartâ€™s accessibility standards and guidelines for supporting an inclusive culture']}, {'title': 'Benefits', 'items': ['Benefits: Beyond our great compensation package, you can receive incentive awards for your performance', 'Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more', 'Health benefits include medical, vision and dental coverage', 'Financial benefits include 401(k), stock purchase and company-paid life insurance', 'Paid time off benefits include PTO, parental leave, family care leave, bereavement, jury duty, and voting', 'You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes', 'The amount you receive depends on your job classification and length of employment', 'It will meet or exceed the requirements of paid sick leave laws, where applicable', 'Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more', "Live Better U is a company paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities", "Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates", 'Tuition, books, and fees are completely paid for by Walmart', 'Benefits are subject to change and may be subject to a specific plan or program terms', 'The annual salary range for this position is $90,000.00-$180,000.00', 'Additional compensation includes annual or quarterly performance bonuses']}, {'title': 'Responsibilities', 'items': ['Position Summary...The Data Engineer III is responsible for designing, building, and optimizing scalable big data pipelines, architectures, and datasets that enable advanced analytics and data-driven decision-making', 'This role involves developing efficient data transformation and processing frameworks, managing data structures, metadata, dependencies, and workloads, and ensuring the reliability and performance of the data ecosystem', 'The engineer will also work extensively with unstructured datasets, applying analytical techniques to extract insights and improve data accessibility across the organization', 'Data Modeling: Designing and implementing data models to support structured and unstructured datasets, ensuring data integrity and efficiency', 'Data Extraction: Developing and optimizing data extraction processes from various sources including databases, APIs, and logs', 'Data Cleaning: Preprocessing and cleaning data to remove inconsistencies and improve data quality', 'Data Screening: Implementing data validation and quality checks to ensure accuracy and completeness of data', 'Data Exploration: Conducting exploratory data analysis to understand patterns, trends, and correlations in the data', 'Data Visualization: Creating visualizations using tools like Tableau, PowerBI, or Looker to communicate insights and findings effectively', 'Big Data Technologies: Utilizing tools and frameworks such as Spark, Spark SQL, PySpark, HDFS, and MapReduce for processing large datasets efficiently', 'Cloud Services: Leveraging cloud platforms like GCP, Azure/AWS, Databricks, Azure HD Insights, ADF for data storage, processing, and analytics', 'Data Querying: Writing advanced SQL queries to extract and manipulate data from relational databases and other data stores', 'Data Pipeline Development: Building and optimizing scalable data pipelines and architectures to move and transform data across systems', 'Data Transformation: Developing processes for data transformation, structure, metadata, dependency, and workload management', 'Enterprise Software Development: Contributing to the development of enterprise-level software products related to data engineering and analytics', 'Big Data Data Stores: Implementing and managing highly scalable big data stores to efficiently store and access large volumes of data', 'Data Value Extraction: Manipulating, processing, and extracting value from large, diverse datasets to drive business decisions and innovation', 'Cloud Services: Experience leveraging cloud platforms like GCP, Azure/AWS, Databricks, Azure HD Insights, ADF for data storage, processing, and analytics', "We'll meet in person on a regular and purposeful basis", 'That means understanding, respecting, and valuing unique styles, experiences, identities, ideas, and opinions â€“ while being inclusive of all people']}], 'apply_options': [{'title': 'ZipRecruiter', 'link': 'https://www.ziprecruiter.com/c/Sams-Club/Job/Data-Engineer-III/-in-Springdale,AR?jid=f06f637bea60ef59&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'The Muse', 'link': 'https://www.themuse.com/jobs/walmart/usa-data-engineer-iii-6724ac?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobs Trabajo.org', 'link': 'https://us.trabajo.org/job-3642-e33a8e72d810aed0517f743be00dcd1a?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIElJSSIsImNvbXBhbnlfbmFtZSI6IlNhbSdzIENsdWIiLCJhZGRyZXNzX2NpdHkiOiJTcHJpbmdkYWxlLCBBUiIsImh0aWRvY2lkIjoibkt6UlViS3hVNWhfcVNiaEFBQUFBQT09IiwiaGwiOiJlbiJ9'}, {'title': 'Principal Data Engineer', 'company_name': 'Warner Bros. Discovery', 'location': 'Atlanta, GA', 'via': 'WBD Careers - Warner Bros. Discovery', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=s-rX_h0IsS1-xHCaAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMsQrCMBAAUFz7CU63uIgmIrjoVKkUnNwcyzUcaSTehdwh9T_8YHV542s-i8bdauKQCmbo0BAuHBMTVdjCVUZQwhomEIZeJGZaniazokfvVbOLamgpuCBPL0yjzP4ho_4ZdMJKJaPRsD_sZlc4rld3rPy7z1XUQZc0yIvqGxJDaxnZcAN9-wXxwNBGlwAAAA&shmds=v1_AdeF8KiDTlbO1j4wROmj0MpZ6QiuV8bbRnoZQ77pf2zsZ36nVA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=s-rX_h0IsS1-xHCaAAAAAA%3D%3D', 'extensions': ['2 days ago', 'Full-time'], 'detected_extensions': {'posted_at': '2 days ago', 'schedule_type': 'Full-time'}, 'description': 'Welcome to Warner Bros. Discoveryâ€¦ the stuff dreams are made of.\n\nWho We Areâ€¦\n\nWhen we say, â€œthe stuff dreams are made of,â€ weâ€™re not just referring to the world of wizards, dragons and superheroes, or even to the wonders of Planet Earth. Behind WBDâ€™s vast portfolio of iconic content and beloved brands, are the storytellers bringing our characters to life, the creators bringing them to your living rooms and the dreamers creating whatâ€™s nextâ€¦\n\nFrom brilliant creatives, to technology trailblazers, across the globe, WBD offers career defining opportunities, thoughtfully curated benefits, and the tools to explore and grow into your best selves. Here you are supported, here you are celebrated, here you can thrive.\n\nYour New Role:\n\nWarner Bros. Discovery seeks a Principal Data Engineer â€“ Data for the Data & Shared Services organization. We are looking for a talented Principal Data Engineer to join our Data Engineering Team. The person works across multiple functional and technical groups, educates technical and non-technical stakeholders, effectively manages expectations, and help establish priorities. They must drive project delivery within established timeframes, and scope while delivering business value and simultaneously planning for and managing future requirements. Strong analytical skills and passion for Data is a must.\n\nYour Role Accountabilities:\nâ€¢ Leads Engineering, prototypes, and builds complex data management systems that combine core data sources into data warehouses, cloud data lake, data science platforms or other accessible structures.\nâ€¢ Leads Data Modeling, collaborates with data architecture and Enterprise Integration teams, Data Governance and Data compliance team members.\nâ€¢ Analyze business processes to create requirements, specifications, and systems solution designs in partnership with engineering team members.\nâ€¢ Develop and enforce best practices for data management, adhering to architectural standards, security, and compliance etc.\nâ€¢ Expert knowledge of relational databases, data warehouses and SQL skills.\nâ€¢ Expert knowledge of data integration, data pipeline and data delivery.\nâ€¢ Recommends and implements data reliability, efficiency, and quality improvements.\nâ€¢ Build data pipelines: create, maintain, and optimize workloads from development to production for specific use cases.\nâ€¢ Enthusiasm to learn and find opportunities to enhance and adapt in daily engineering activities is highly desired.\nâ€¢ Execute and oversee the analysis and remediation of root causes, including deficiencies in technology, process, or resource capabilities.\nâ€¢ Lead the team in monitoring and tuning applicable code to assure optimal availability, performance, and utilization of resources.\nâ€¢ Knowledge of data visualization and business intelligence.\nâ€¢ Lead the team in monitoring and tuning application code to assure optimal availability, performance, and utilization of resources.\n\nQualifications & Experiences:\nâ€¢ Bachelorâ€™s degree in science, Statistics, Engineering, Business Administration or similar field of study\nâ€¢ 14+ years of experience building and scaling data platforms.\nâ€¢ Knowledge of supporting data sets for Home Entertainment, Games DVD/Digital business, Content sales and Licensing.\nâ€¢ Knowledge of SAP supply chain, APO, Order management, Trade spends, promotions, POS, Royalty, Forecasting and cash collections.\nâ€¢ Expert in one or modern data replication and data integration tools (for example Informatica,\nâ€¢ Attunity, AWS DMS, etc.)\nâ€¢ Expert in one or more modern data lake and data warehouse tools (for example Teradata, Redshift, Snowflake, etc.)\nâ€¢ Expert in one or more modern advanced analytics tools (for example Spark, Data Bricks, Python, AWS Glue, etc.)\nâ€¢ Experience in one or more modern advanced business intelligence & visualization tools (for example Business Objects, Tableau, Looker, etc.)\nâ€¢ Experience in software delivery through continuous integration (for example git, bitbucket,\nâ€¢ Jenkins, etc.)\nâ€¢ Experience in one or more automation and scheduling tools (for example Redwood, Airflow, etc.)\nâ€¢ Experience with Atlassian Suite (JIRA, Confluence)\nâ€¢ Experience with Excel spreadsheets, modeling, and reporting.\nâ€¢ Knowledge of cloud-based environments like AWS is a plus.\nâ€¢ Must be able to pay close attention to complex details and understand written and oral instructions.\nâ€¢ Must be able to handle multiple tasks with changing priorities, communicating changes in scope and schedule to all parties concerned.\nâ€¢ Must be able to work independently and Mentor junior data engineers/developers.\n\nNot Required but preferred experience:\nâ€¢ Public speaking and presentation skills.\nâ€¢ Experience with data science tools such as NumPy, Pandas, R, MATLAB etc.\nâ€¢ Experience with data processing engines like spark\nâ€¢ Experience with DBT.\n\nHow We Get Things Doneâ€¦\n\nThis last bit is probably the most important! Here at WBD, our guiding principles are the core values by which we operate and are central to how we get things done. You can find them at www.wbd.com/guiding-principles/ along with some insights from the team on what they mean and how they show up in their day to day. We hope they resonate with you and look forward to discussing them during your interview.\n\nChampioning Inclusion at WBD\nWarner Bros. Discovery embraces the opportunity to build a workforce that reflects a wide array of perspectives, backgrounds and experiences. Being an equal opportunity employer means that we take seriously our responsibility to consider qualified candidates on the basis of merit, without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law.\n\nIf youâ€™re a qualified candidate with a disability and you require adjustments or accommodations during the job application and/or recruitment process, please visit our accessibility page for instructions to submit your request.', 'job_highlights': [{'title': 'Qualifications', 'items': ['We are looking for a talented Principal Data Engineer to join our Data Engineering Team', 'They must drive project delivery within established timeframes, and scope while delivering business value and simultaneously planning for and managing future requirements', 'Strong analytical skills and passion for Data is a must', 'Bachelorâ€™s degree in science, Statistics, Engineering, Business Administration or similar field of study', '14+ years of experience building and scaling data platforms', 'Knowledge of supporting data sets for Home Entertainment, Games DVD/Digital business, Content sales and Licensing', 'Knowledge of SAP supply chain, APO, Order management, Trade spends, promotions, POS, Royalty, Forecasting and cash collections', 'Expert in one or modern data replication and data integration tools (for example Informatica,', 'Attunity, AWS DMS, etc.)', 'Expert in one or more modern data lake and data warehouse tools (for example Teradata, Redshift, Snowflake, etc.)', 'Expert in one or more modern advanced analytics tools (for example Spark, Data Bricks, Python, AWS Glue, etc.)', 'Experience in one or more modern advanced business intelligence & visualization tools (for example Business Objects, Tableau, Looker, etc.)', 'Experience in software delivery through continuous integration (for example git, bitbucket,', 'Jenkins, etc.)', 'Experience in one or more automation and scheduling tools (for example Redwood, Airflow, etc.)', 'Experience with Atlassian Suite (JIRA, Confluence)', 'Experience with Excel spreadsheets, modeling, and reporting', 'Must be able to pay close attention to complex details and understand written and oral instructions', 'Must be able to handle multiple tasks with changing priorities, communicating changes in scope and schedule to all parties concerned', 'Must be able to work independently and Mentor junior data engineers/developers']}, {'title': 'Responsibilities', 'items': ['The person works across multiple functional and technical groups, educates technical and non-technical stakeholders, effectively manages expectations, and help establish priorities', 'Leads Engineering, prototypes, and builds complex data management systems that combine core data sources into data warehouses, cloud data lake, data science platforms or other accessible structures', 'Leads Data Modeling, collaborates with data architecture and Enterprise Integration teams, Data Governance and Data compliance team members', 'Analyze business processes to create requirements, specifications, and systems solution designs in partnership with engineering team members', 'Develop and enforce best practices for data management, adhering to architectural standards, security, and compliance etc', 'Expert knowledge of relational databases, data warehouses and SQL skills', 'Expert knowledge of data integration, data pipeline and data delivery', 'Recommends and implements data reliability, efficiency, and quality improvements', 'Build data pipelines: create, maintain, and optimize workloads from development to production for specific use cases', 'Enthusiasm to learn and find opportunities to enhance and adapt in daily engineering activities is highly desired', 'Execute and oversee the analysis and remediation of root causes, including deficiencies in technology, process, or resource capabilities', 'Lead the team in monitoring and tuning applicable code to assure optimal availability, performance, and utilization of resources', 'Knowledge of data visualization and business intelligence', 'Lead the team in monitoring and tuning application code to assure optimal availability, performance, and utilization of resources']}], 'apply_options': [{'title': 'WBD Careers - Warner Bros. Discovery', 'link': 'https://careers.wbd.com/jp/ja/job/R000100225/Principal-Data-Engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'ZipRecruiter', 'link': 'https://www.ziprecruiter.com/c/Worth-AI/Job/Principal-Data-Engineer/-in-Atlanta,GA?jid=12652116f4238a48&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'LinkedIn', 'link': 'https://www.linkedin.com/jobs/view/principal-data-engineer-at-trella-health-4318532213?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Indeed', 'link': 'https://www.indeed.com/viewjob?jk=86eb0919528bd61f&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'MyGwork', 'link': 'https://mygwork.com/jobs/warner-bros-discovery-principal-data-engineer-12?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Glassdoor', 'link': 'https://www.glassdoor.com/job-listing/principal-data-engineer-warner-bros-discovery-JV_IC1155583_KO0,23_KE24,45.htm?jl=1009936734738&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'SimplyHired', 'link': 'https://www.simplyhired.com/job/-gML9SJonM-ZTCVGeBIP4IpSqqnYU8P7AmJC6qHtG1ToOu3N5skORw?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Snagajob', 'link': 'https://www.snagajob.com/jobs/1185795066?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJQcmluY2lwYWwgRGF0YSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6Ildhcm5lciBCcm9zLiBEaXNjb3ZlcnkiLCJhZGRyZXNzX2NpdHkiOiJBdGxhbnRhLCBHQSIsImh0aWRvY2lkIjoicy1yWF9oMElzUzEteEhDYUFBQUFBQT09IiwiaGwiOiJlbiJ9'}, {'title': 'Senior Data Engineer, Analytics Platform', 'company_name': 'PENN Interactive', 'location': 'Philadelphia, PA', 'via': 'LinkedIn', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=O4FDIfuNZK2wLPmSAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMsQrCMBCAYVz7CE43OEltRHDRQQoW0aEEfIByrWcSSe9CEqS-jw9qXf7t-4vvojjdiZ1EOGNGaNg4Jool1Iz-k92QQHvMT4kjbOAmPSTCOFgQhouI8bQ82pxDOiiVkq9MyjirapBRCVMvk3pJn_7pksVIYb5Rt9tvpyqwWa9007Zw5UwRh-zeBI5BW-fxQT5YhyXo-geWKcf_pwAAAA&shmds=v1_AdeF8KjYS_tjosv8Kh5iuBKxoMm4MnFD-ZrSw2ZenYnN-mN9wg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=O4FDIfuNZK2wLPmSAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965b94c437f54538f07a4/images/c96b123ff18233f364eb17fc9dae27dcc87225324a82c9fa67b9d543ef8ce39b.jpeg', 'extensions': ['4 days ago', 'Full-time'], 'detected_extensions': {'posted_at': '4 days ago', 'schedule_type': 'Full-time'}, 'description': "PENN Entertainment, Inc. is North Americaâ€™s leading provider of integrated entertainment, sports content, and casino gaming experiences. From casinos and racetracks to online gaming, sports betting and entertainment content, we deliver the experiences people want, how and where they want them.\n\nWeâ€™re always on the lookout for those who are passionate about creating and delivering cutting-edge online gaming and sports media products. Whether itâ€™s through Hollywood Casino, theScore Bet Sportsbook, or theScore media app, weâ€™re excited to push the boundaries of whatâ€™s possible. These state-of-the-art platforms are powered by proprietary in-house technology, a key component of PENNâ€™s omnichannel gaming and entertainment strategy.\n\nWhen you join PENN Entertainmentâ€™s digital team, youâ€™ll not only work on these cutting-edge platforms through theScore and PENN Interactive, but youâ€™ll also be part of a company that truly cares about your career growth. Weâ€™re committed to supporting you as you expand your skills and explore new opportunities.\n\nWith locations throughout North America, you can build a future at PENN Entertainment wherever you are. If you want to challenge conventions in gaming, media and entertainment, we want to talk to you.\n\nAbout The Role & Team\u202f\n\nAs a Data Engineer, youâ€™ll work hand-in-hand with the Data Services team to build, validate and deliver transformed data to meet stakeholder requirements. The ideal candidate has strong SQL skills, a passion for data, a technical background and an interest in professional sports, betting & eSports.\n\nAbout The Work\n\nAs a key member of our Analytics team, you will:\nâ€¢ Liaise with business stakeholders to identify data driven projects and opportunities.\nâ€¢ Liaise with data engineers to build data pipelines and solve related issues.\nâ€¢ Use your SQL expertise to develop dbt models.\nâ€¢ Manage Airflow tasks and scheduling.\nâ€¢ Assist team members in solving data issues related to both analysis and modeling.\nâ€¢ Complete analyses and present clear recommendations to business stakeholders.\nâ€¢ Oversee team's analysis to ensure data and insights shared are accurate and informative.\nâ€¢ Help with implementation & adoption of a data exploration tool across the organization\nâ€¢ Provide support on data engineer hiring, onboarding and continued professional development. Mentor others.\nâ€¢ Other duties as required\n\nAbout The You\nâ€¢ University degree in Computer Science, Statistics, Business, or related field.\nâ€¢ You possess a minimum of 4+ years of experience in analytics engineering/data engineering,\nâ€¢ You bring hands-on expertise with our technology stack: BigQuery, dbt, Airflow, Python, and relational databases.\nâ€¢ You have a proven track record in building and optimizing data transformation pipelines\nâ€¢ You excel in managing relationships with both internal and external stakeholders, ensuring alignment and transparency throughout project lifecycles.\nâ€¢ You excel in fast-paced environments, employing a pragmatic and adaptive approach to problem-solving.\nâ€¢ Experience with a data exploration tool such as Looker or Mode.\nâ€¢ Interest in Sports and consumer products\n\nWhat We Offer\nâ€¢ Competitive compensation package\u202f\nâ€¢ Fun, relaxed work environment\u202f\nâ€¢ Education and conference reimbursements.\u202f\nâ€¢ Opportunities for career progression and mentoring others\u202f\n\nSalary Range\n\n$135,000â€”$180,000 USD\n\nPenn Interactive is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, genetic information, or any other basis protected by applicable law.Base pay is one part of the Total Rewards that Penn Interactive provides to compensate and recognize employees for their work. Most sales positions are eligible for a Commission under the terms of an applicable plan, while most non-sales positions are eligible for a Bonus. Additionally, Penn Interactive provides best-in-class benefits to eligible employees. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. Thatâ€™s why we provide an array of options, expert guidance and always-on tools, that are personalized to meet the needs of your reality â€“ to help support you physically, financially and emotionally through the big milestones and in your everyday life.", 'job_highlights': [{'title': 'Qualifications', 'items': ['The ideal candidate has strong SQL skills, a passion for data, a technical background and an interest in professional sports, betting & eSports', 'University degree in Computer Science, Statistics, Business, or related field', 'You possess a minimum of 4+ years of experience in analytics engineering/data engineering,', 'You bring hands-on expertise with our technology stack: BigQuery, dbt, Airflow, Python, and relational databases', 'You have a proven track record in building and optimizing data transformation pipelines', 'You excel in fast-paced environments, employing a pragmatic and adaptive approach to problem-solving', 'Experience with a data exploration tool such as Looker or Mode', 'Interest in Sports and consumer products']}, {'title': 'Benefits', 'items': ['Competitive compensation package\u202f', 'Fun, relaxed work environment\u202f', 'Education and conference reimbursements.\u202f', 'Opportunities for career progression and mentoring others\u202f', 'Salary Range', '$135,000â€”$180,000 USD', 'Most sales positions are eligible for a Commission under the terms of an applicable plan, while most non-sales positions are eligible for a Bonus', 'Additionally, Penn Interactive provides best-in-class benefits to eligible employees']}, {'title': 'Responsibilities', 'items': ['As a Data Engineer, youâ€™ll work hand-in-hand with the Data Services team to build, validate and deliver transformed data to meet stakeholder requirements', 'Liaise with business stakeholders to identify data driven projects and opportunities', 'Liaise with data engineers to build data pipelines and solve related issues', 'Use your SQL expertise to develop dbt models', 'Manage Airflow tasks and scheduling', 'Assist team members in solving data issues related to both analysis and modeling', 'Complete analyses and present clear recommendations to business stakeholders', "Oversee team's analysis to ensure data and insights shared are accurate and informative", 'Help with implementation & adoption of a data exploration tool across the organization', 'Provide support on data engineer hiring, onboarding and continued professional development', 'Mentor others', 'Other duties as required', 'You excel in managing relationships with both internal and external stakeholders, ensuring alignment and transparency throughout project lifecycles']}], 'apply_options': [{'title': 'LinkedIn', 'link': 'https://www.linkedin.com/jobs/view/senior-data-engineer-analytics-platform-at-penn-interactive-4339765856?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Built In', 'link': 'https://builtin.com/job/senior-data-engineer-analytics-platform/7668876?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Ladders', 'link': 'https://www.theladders.com/job/senior-data-engineer-analytics-platform-boomtown-philadelphia-pa_84385753?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Startup Jobs', 'link': 'https://startup.jobs/senior-data-engineer-analytics-platform-penn-interactive-7457975?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'BeBee', 'link': 'https://us.bebee.com/job/2821023537282d224b22a15e34383d8d?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBFbmdpbmVlciwgQW5hbHl0aWNzIFBsYXRmb3JtIiwiY29tcGFueV9uYW1lIjoiUEVOTiBJbnRlcmFjdGl2ZSIsImFkZHJlc3NfY2l0eSI6IlBoaWxhZGVscGhpYSwgUEEiLCJodGlkb2NpZCI6Ik80RkRJZnVOWksyd0xQbVNBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Junior Data Engineer', 'company_name': 'Systems Planning and Analysis', 'location': 'United States', 'via': 'Careers - SPA', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=oKspHKF6ndCKUdNKAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMsQrCMBAAUFz7CU43KyYiuOgkKEInoTiXa3ukkfQu5E5of8WvVZc3vuqzqrb1m6MUuKIh3DhEJiqwg1o6UMLSjyAMd5GQaH0ezbKevFdNLqihxd71Mnlh6mT2L-n0T6sjFsoJjdrDcT-7zGHjmkWNJoVHQubIAZAHuDCmRaNCZHhyNBqg-b2kX6jcHZedAAAA&shmds=v1_AdeF8Kj5M9-gs_BazoNE0nk9zewGX9TpEAEle_bAbfZVviF6iw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=oKspHKF6ndCKUdNKAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965b94c437f54538f07a4/images/c96b123ff18233f361425b757404443cdbd8cf8d73cc2f1e6f8f34192d04b1f7.png', 'extensions': ['26 days ago', 'Full-time', 'Dental insurance', 'Health insurance'], 'detected_extensions': {'posted_at': '26 days ago', 'schedule_type': 'Full-time', 'dental_coverage': True, 'health_insurance': True}, 'description': "Overview\n\nIntrepid, an SPA Company, brings more than 20 years of experience supporting the Department of Defense and U.S. Government, consistently setting the standard for excellence in the federal marketplace. Committed to advancing the mission of the U.S. Warfighter, Intrepid leverages technological superiority to deliver innovative solutions across air, space, land, and sea domains. We are proud to foster a collaborative, dynamic work environment, offering competitive compensation and an industry-leading 401k contribution. Our team is built through merit and achievement, and weâ€™re always looking for the best and brightest to join us in our growth. We treat our people like family, we are mission-focused, and we give back! Join us today.\n\nAt SPA, we strive to deliver a robust total compensation package that will attract and retain the top talent. Elements of the compensation package include competitive base pay and variable compensation opportunities.\n\nSPA provides eligible employees with an opportunity to enroll in a variety of benefit programs, generally including health insurance, flexible spending accounts, health savings accounts, retirement savings plans, life and disability insurance programs, and a number of programs that provide for both paid and unpaid time away from work.\n\nThe specific programs and options available to any given employee may vary depending on eligibility factors such as geographic location, date of hire, etc. Please note that the salary information shown below is a general guideline only. Salaries are commensurate with experience and qualifications, as well as market and business considerations. Salary Range: $76k to $107k\n\nSPA has a need for a Junior Data Engineer position with Databrick experience.\n\nResponsibilities\nâ€¢ Sustaining data feeds between Army systems and target Joint data platforms.\nâ€¢ New data feeds between Army systems and target Joint data platforms.\nâ€¢ Documentation in Army identified project management software (e.g., Confluence) on implemented data feeds.\nâ€¢ Developing and maintaining data interfaces from source Army FM systems to Joint data platforms.\nâ€¢ Scheduling data pipeline operations and maintenance.\nâ€¢ Automating pipelines and configuring data loads.\nâ€¢ Implementing data modeling as identified and developed by project requirements.\nâ€¢ Coordinating with user community for improvements and bug fixes as required.\nâ€¢ Maintaining and configuring structured databases in cloud environment.\nâ€¢ Other duties as assigned.\n\nQualifications\nâ€¢ A bachelor's degree\nâ€¢ 3-5 years' relevant experience\nâ€¢ Experience with Python / pyspark and SQL coding\nâ€¢ Experience with Databricks\nâ€¢ Experience with developing interfaces between systems\nâ€¢ Basic knowledge of Palantir tool suite\nâ€¢ US Citizenship and an active SECRET security clearance", 'job_highlights': [{'title': 'Qualifications', 'items': ['SPA has a need for a Junior Data Engineer position with Databrick experience', "A bachelor's degree", "3-5 years' relevant experience", 'Experience with Python / pyspark and SQL coding', 'Experience with Databricks', 'Experience with developing interfaces between systems', 'Basic knowledge of Palantir tool suite', 'US Citizenship and an active SECRET security clearance']}, {'title': 'Benefits', 'items': ['Elements of the compensation package include competitive base pay and variable compensation opportunities', 'SPA provides eligible employees with an opportunity to enroll in a variety of benefit programs, generally including health insurance, flexible spending accounts, health savings accounts, retirement savings plans, life and disability insurance programs, and a number of programs that provide for both paid and unpaid time away from work', 'The specific programs and options available to any given employee may vary depending on eligibility factors such as geographic location, date of hire, etc', 'Salaries are commensurate with experience and qualifications, as well as market and business considerations', 'Salary Range: $76k to $107k']}, {'title': 'Responsibilities', 'items': ['Sustaining data feeds between Army systems and target Joint data platforms', 'New data feeds between Army systems and target Joint data platforms', 'Documentation in Army identified project management software (e.g., Confluence) on implemented data feeds', 'Developing and maintaining data interfaces from source Army FM systems to Joint data platforms', 'Scheduling data pipeline operations and maintenance', 'Automating pipelines and configuring data loads', 'Implementing data modeling as identified and developed by project requirements', 'Coordinating with user community for improvements and bug fixes as required', 'Maintaining and configuring structured databases in cloud environment', 'Other duties as assigned']}], 'apply_options': [{'title': 'Careers - SPA', 'link': 'https://talent.spa.com/jobs/20589?lang=en-us&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Workable Jobs', 'link': 'https://apply.workable.com/mod-op/j/388BF537E9?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobs', 'link': 'https://jobs.ashbyhq.com/cylinderhealth/a75dbad9-203f-47ca-b96d-2f0d3a309034?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Indeed', 'link': 'https://www.indeed.com/viewjob?jk=2ac039ced2598c08&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Glassdoor', 'link': 'https://www.glassdoor.com/job-listing/junior-data-engineer-itransition-JV_KO0,20_KE21,32.htm?jl=1009796417162&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'LinkedIn', 'link': 'https://www.linkedin.com/jobs/view/junior-data-engineer-at-contexture-4312351208?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Himalayas.app', 'link': 'https://himalayas.app/companies/blue-coding/jobs/junior-data-engineer-v?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobgether', 'link': 'https://jobgether.com/offer/67d0265e5195a9c4765fcd05-junior-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJKdW5pb3IgRGF0YSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IlN5c3RlbXMgUGxhbm5pbmcgYW5kIEFuYWx5c2lzIiwiYWRkcmVzc19jaXR5IjoiVW5pdGVkIFN0YXRlcyIsImh0aWRvY2lkIjoib0tzcEhLRjZuZENLVWROS0FBQUFBQT09IiwiaGwiOiJlbiJ9'}, {'title': 'Sr. Manager - Data Engineer', 'company_name': 'CVS Health', 'location': 'North Carolina', 'via': 'Hispanic Alliance For Career Enhancement', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=RC0e5P-ex15llpwMAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_x3HvwrCMBCAcVz7COJws2AiBRcdqyiCLgXXci1HEol3IXdD38JX9s_y8fua96Jp--rghoyBKmzgiIZw4pCY_n-VEZSwThGE4SwSMi0P0azo3nvV7IIaWprcJC8vTKPM_imj_jJoxEolo9HQ7razKxzWq-7Rw4UwW4TEcJf6RYdVcmL8AJb3L-eSAAAA&shmds=v1_AdeF8KhJKVNv3it2_AuxoHmxeSMlGJZKS2EQNOtp0eXUqYf7FA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=RC0e5P-ex15llpwMAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965b94c437f54538f07a4/images/c96b123ff18233f327f74dcf2445af8baa31bebc93723264fbe72e010545e057.png', 'extensions': ['5 days ago', 'Full-time', 'Paid time off', 'Health insurance'], 'detected_extensions': {'posted_at': '5 days ago', 'schedule_type': 'Full-time', 'paid_time_off': True, 'health_insurance': True}, 'description': "At CVS Health, we're building a world of health around every consumer and surrounding ourselves with dedicated colleagues who are passionate about transforming health care.\n\nAs the nation's leading health solutions company, we reach millions of Americans through our local presence, digital channels and more than 300,000 purpose-driven colleagues - caring for people where, when and how they choose in a way that is uniquely more connected, more convenient and more compassionate. And we do it all with heart, each and every day.\n\nPosition Summary\n\nWe are seeking a seasoned data engineering professional to lead our data modeling and data product delivery. Candidates will possess a deep understanding of data architecture and data modeling. They will be responsible for managing a team of data engineers, ensuring timely delivery of projects and providing technical leadership and guidance.\n\nThis role involves collaborating with other engineering teams and product owners, taking ownership of the team's work, and influencing the technical strategy and architecture of the platform. The position is a blend of leadership and hands-on technical guidance and requires an understanding of data engineering best practices and the ability to translate business requirements into technical solutions.\n\nYou will lead a dedicated team creating datasets for analytic and data science workloads.\nâ€¢ Data Pipeline Development: Ovre see the design and buildout of ETL/ELT data pipelines to ingest, process, and transform large datasets from multiple sources.\nâ€¢ Performance Optimization: Implement best practices for performance tuning, partitioning, and clustering to optimize data queries and reduce costs.\nâ€¢ Data Quality & Governance: Establish and enforce data quality standards, data governance frameworks, and security policies for data storage and access.\nâ€¢ Documentation & Knowledge Sharing: Create comprehensive documentation for data pipelines, workflows, and processes. Share best practices and mentor junior data engineers.\nâ€¢ Design and architect data infrastructure analytical workloads.\n\nRequired Qualifications\nâ€¢ 7 years of applicable experience.\nâ€¢ Proficiency in Python, specifically with ETL pipelines.\nâ€¢ Strong proficiency in SQL and experience in developing complex queries.\nâ€¢ Familiarity with pySpark, DBT, or other similar frameworks.\nâ€¢ Experience deploying data pipelines in a cloud environment (Azure, AWS, GCP).\nâ€¢ Understanding of data warehousing concepts, dimensional modeling, and building data marts.\nâ€¢ Excellent communication and interpersonal skills, with the ability to collaborate effectively with data scientists, analysts, and product owners.\n\nPreferred Qualifications\nâ€¢ Experience leading technical teams\nâ€¢ Experience working with stakeholders and product owners to create a technical roadmap from project requirements.\nâ€¢ Experience working with healthcare data (Epic, Tuva, or OMOP Models a plus).\nâ€¢ Knowledge of data governance best practices in a cloud environment.\nâ€¢ Experience with machine learning flows on GCP.\nâ€¢ Experience with data design in BigQuery.\n\nEducation\nCollege degree or certifications in relevant areas.\n\nPay Range\n\nThe typical pay range for this role is:\n\n$106,605.00 - $284,280.00\n\nThis pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above. This position also includes an award target in the company's equity award program.\n\nOur people fuel our future. Our teams reflect the customers, patients, members and communities we serve and we are committed to fostering a workplace where every colleague feels valued and that they belong.\n\nGreat benefits for great people\n\nWe take pride in our comprehensive and competitive mix of pay and benefits - investing in the physical, emotional and financial wellness of our colleagues and their families to help them be the healthiest they can be. In addition to our competitive wages, our great benefits include:\nâ€¢ Affordable medical plan options, a 401(k) plan (including matching company contributions), and an employee stock purchase plan.\nâ€¢ No-cost programs for all colleagues including wellness screenings, tobacco cessation and weight management programs, confidential counseling and financial coaching.\nâ€¢ Benefit solutions that address the different needs and preferences of our colleagues including paid time off, flexible work schedules, family leave, dependent care resources, colleague assistance programs, tuition assistance, retiree medical access and many other benefits depending on eligibility.\n\nFor more information, visit https://jobs.cvshealth.com/us/en/benefits\n\nWe anticipate the application window for this opening will close on: 11/14/2025\n\nQualified applicants with arrest or conviction records will be considered for employment in accordance with all federal, state and local laws.", 'job_highlights': [{'title': 'Qualifications', 'items': ['The position is a blend of leadership and hands-on technical guidance and requires an understanding of data engineering best practices and the ability to translate business requirements into technical solutions', '7 years of applicable experience', 'Proficiency in Python, specifically with ETL pipelines', 'Strong proficiency in SQL and experience in developing complex queries', 'Familiarity with pySpark, DBT, or other similar frameworks', 'Experience deploying data pipelines in a cloud environment (Azure, AWS, GCP)', 'Understanding of data warehousing concepts, dimensional modeling, and building data marts', 'Excellent communication and interpersonal skills, with the ability to collaborate effectively with data scientists, analysts, and product owners', 'College degree or certifications in relevant areas']}, {'title': 'Benefits', 'items': ['Pay Range', '$106,605.00 - $284,280.00', 'This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls', 'The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors', 'This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above', "This position also includes an award target in the company's equity award program", 'We take pride in our comprehensive and competitive mix of pay and benefits - investing in the physical, emotional and financial wellness of our colleagues and their families to help them be the healthiest they can be', 'In addition to our competitive wages, our great benefits include:', 'Affordable medical plan options, a 401(k) plan (including matching company contributions), and an employee stock purchase plan', 'No-cost programs for all colleagues including wellness screenings, tobacco cessation and weight management programs, confidential counseling and financial coaching', 'Benefit solutions that address the different needs and preferences of our colleagues including paid time off, flexible work schedules, family leave, dependent care resources, colleague assistance programs, tuition assistance, retiree medical access and many other benefits depending on eligibility']}, {'title': 'Responsibilities', 'items': ['Candidates will possess a deep understanding of data architecture and data modeling', 'They will be responsible for managing a team of data engineers, ensuring timely delivery of projects and providing technical leadership and guidance', "This role involves collaborating with other engineering teams and product owners, taking ownership of the team's work, and influencing the technical strategy and architecture of the platform", 'You will lead a dedicated team creating datasets for analytic and data science workloads', 'Data Pipeline Development: Ovre see the design and buildout of ETL/ELT data pipelines to ingest, process, and transform large datasets from multiple sources', 'Performance Optimization: Implement best practices for performance tuning, partitioning, and clustering to optimize data queries and reduce costs', 'Data Quality & Governance: Establish and enforce data quality standards, data governance frameworks, and security policies for data storage and access', 'Documentation & Knowledge Sharing: Create comprehensive documentation for data pipelines, workflows, and processes', 'Share best practices and mentor junior data engineers', 'Design and architect data infrastructure analytical workloads']}], 'apply_options': [{'title': 'Hispanic Alliance For Career Enhancement', 'link': 'https://jobs.haceonline.org/job/sr-manager-data-engineer/81124632/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTci4gTWFuYWdlciAtIERhdGEgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJDVlMgSGVhbHRoIiwiYWRkcmVzc19jaXR5IjoiTm9ydGggQ2Fyb2xpbmEiLCJodGlkb2NpZCI6IlJDMGU1UC1leDE1bGxwd01BQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Documentum Data Engineer', 'company_name': 'VirtualVocations', 'location': 'Fullerton, CA', 'via': 'Talent.com', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=qQ9mdKPUNwIRwGV9AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXLsQrCMBAAUFz7CU7nKtqI4KKTWBX8gK7lGo40ktyV3AX6G_6xurztNZ9V4zrxNRNbzdChIdw5RCYqsIeXjKCExU8gDE-RkGh9mcxmPTunmtqghhZ96yU7YRplcW8Z9c-gExaaExoNx9NhaWcO200fi1VMvfjfE1aIDI-aEhUT3sHt-gWLMmtVlAAAAA&shmds=v1_AdeF8KijEmwEPqh9Q7Bx1gMV4v6Zu6bc0eBruophvGVn-Qd6Hg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=qQ9mdKPUNwIRwGV9AAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965b94c437f54538f07a4/images/c96b123ff18233f3e2cacbdda43b77977df5e0a83262a4b0d49b13be21484ee1.jpeg', 'extensions': ['6 days ago', 'Full-time', 'No degree mentioned'], 'detected_extensions': {'posted_at': '6 days ago', 'schedule_type': 'Full-time', 'qualifications': 'No degree mentioned'}, 'description': 'A company is looking for a Data Engineer - Documentum.\n\nKey Responsibilities\n\nAdminister Documentum for large user bases, ensuring efficient operation and support\n\nManage upgrades, patching, and integration of Documentum data with EDW pipelines\n\nOversee security permissions management within the Documentum environment\n\nRequired Qualifications\n\nUS Citizen or Green Card Holder; eligible for Public Trust Clearance\n\nDocumentum Certified (Content Server Administration) or OpenText specialist certification\n\n5+ years of experience administering Documentum for large user bases (approximately 600+ users)', 'job_highlights': [{'title': 'Qualifications', 'items': ['US Citizen or Green Card Holder; eligible for Public Trust Clearance', 'Documentum Certified (Content Server Administration) or OpenText specialist certification', '5+ years of experience administering Documentum for large user bases (approximately 600+ users)']}, {'title': 'Responsibilities', 'items': ['Administer Documentum for large user bases, ensuring efficient operation and support', 'Manage upgrades, patching, and integration of Documentum data with EDW pipelines', 'Oversee security permissions management within the Documentum environment']}], 'apply_options': [{'title': 'Talent.com', 'link': 'https://www.talent.com/view?id=2afbc7251b6d&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEb2N1bWVudHVtIERhdGEgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJWaXJ0dWFsVm9jYXRpb25zIiwiYWRkcmVzc19jaXR5IjoiRnVsbGVydG9uLCBDQSIsImh0aWRvY2lkIjoicVE5bWRLUFVOd0lSd0dWOUFBQUFBQT09IiwiaGwiOiJlbiJ9'}, {'title': 'Senior Data Engineer', 'company_name': 'Infrastructure Consulting & Engineering', 'location': 'Abingdon, VA', 'via': 'JobTarget', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=cbJ16svHjGIEr58CAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_z2NuwrCQBAAsc0nWG1l4SMngghaBRXRVrANd-fmcnLuhtsN5H_8UU1jM8xUU3wmxeKOFDnDyaqFM4VIiBlWcGMHgjb7FpjgwhwSTg-taid7Y0RSGUStRl96fhsmdDyYFzsZUUtrM3bJKtab7XooOwrz3ZWabEVz77XPCEcm6ZNGCjD7n8eKBJX7yZNpCY_qC348OTWmAAAA&shmds=v1_AdeF8KgIgSIF39FdfuZ7yYA4Qh3iUpv90JNeMgeGlCcCT-fAdA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=cbJ16svHjGIEr58CAAAAAA%3D%3D', 'extensions': ['Full-time'], 'detected_extensions': {'schedule_type': 'Full-time'}, 'description': "Senior Data Engineer\u200b â€“ Design Engineering Division\n\nPosition Summary:\n\nICE is seeking an experienced Senior Data Engineer to provide critical technical leadership in supporting pavement management, pavement design, survey, and related engineering services. This role will focus on processing, analyzing, and integrating complex datasets into various engineering software platforms and tools used by our team. The Senior Data Engineer will collaborate with cross-disciplinary teams, including engineers and surveyors, to ensure data accuracy and usability for strategic decision-making and project delivery. This is an ideal position for a highly skilled professional with a strong background in civil engineering, geomatics, and data analysis, coupled with experience in software and hardware implementation, bid analysis, and performance management.\n\nKey Responsibilities:\nâ€¢ Process, analyze, and manage large and complex datasets related to pavement management, pavement design, construction, survey/geomatics, and transportation assets.\nâ€¢ Integrate data into specialized engineering software tools for pavement analysis, construction management, and survey applications, ensuring accuracy, consistency, and usability.\nâ€¢ Develop and streamline data workflows, including data validation and quality control processes, to support the engineering and construction teams.\nâ€¢ Provide technical leadership and guidance to internal teams regarding best practices for data handling and integration across various platforms.\nâ€¢ Collaborate with engineers, surveyors, and asset managers to translate data into actionable insights and assist in developing data-driven strategies for infrastructure projects.\nâ€¢ Support statewide and regional initiatives, including the development of specifications, standards, and documentation for data and asset management.\nâ€¢ Work closely with software vendors and IT teams to implement and optimize engineering software and survey technologies, including hardware upgrades and system integrations.\nâ€¢ Participate in and contribute to national committees and research projects related to data acquisition, construction, and asset management, as appropriate.\nâ€¢ Maintain comprehensive documentation of data processes, methodologies, and workflows to support knowledge sharing and continuity.\n\nQualifications:\n\nRequired:\nâ€¢ Bachelor's or advanced degree in Civil Engineering, Geomatics, Computer Engineering, or related technical field.\nâ€¢ Minimum of 10+ years of experience in civil engineering, geomatics, or data engineering roles supporting transportation infrastructure.\nâ€¢ Strong expertise in data analysis, construction and bid data evaluation, and performance management.\nâ€¢ Proven experience with survey and geomatics technologies, including managing data for large-scale construction and infrastructure projects.\nâ€¢ Familiarity with pavement management systems, construction survey data, and engineering software (e.g., CAD, GIS, or pavement analysis platforms).\nâ€¢ Experience in software and hardware implementation and optimization for engineering applications.\nâ€¢ Excellent problem-solving, project management, and communication skills, with the ability to work collaboratively across teams.\nâ€¢ Licensed Professional Engineer (P.E.) preferred.\n\nPreferred:\nâ€¢ Experience contributing to or leading state or national-level committees (e.g., AASHTO, TRB) on topics related to construction, data standardization, and geospatial technologies.\nâ€¢ Familiarity with transportation asset management and performance-based planning.\nâ€¢ Knowledge of SQL, Python, or other data processing and automation tools.\nâ€¢ Advanced expertise in data visualization and reporting tools to support technical and management teams.\n\nEqual Opportunity Employer (EEO): ICE is an Equal Opportunity Employer and encourages qualified veterans and individuals with disabilities to apply. We do not discriminate on the basis of race, sex, color, religion, gender, sexual orientation or preference, gender identity or expression, national origin or ethnicity, age, marital status, pregnancy, genetic information, disability, or veteran status, in accordance with applicable federal, state, and local laws.\n\nDiversity and Inclusion: Diversity and inclusion are critical values at ICE. We welcome applicants from all backgrounds, including those from underrepresented communities, and are committed to creating an inclusive environment where everyone feels valued, respected, and supported. We believe that diversity enriches our community and strengthens our ability to achieve our goals. By fostering a culture of inclusivity, we aim to create a community that celebrates differences and promotes equity and justice for all.", 'job_highlights': [{'title': 'Qualifications', 'items': ['This is an ideal position for a highly skilled professional with a strong background in civil engineering, geomatics, and data analysis, coupled with experience in software and hardware implementation, bid analysis, and performance management', "Bachelor's or advanced degree in Civil Engineering, Geomatics, Computer Engineering, or related technical field", 'Minimum of 10+ years of experience in civil engineering, geomatics, or data engineering roles supporting transportation infrastructure', 'Strong expertise in data analysis, construction and bid data evaluation, and performance management', 'Proven experience with survey and geomatics technologies, including managing data for large-scale construction and infrastructure projects', 'Familiarity with pavement management systems, construction survey data, and engineering software (e.g., CAD, GIS, or pavement analysis platforms)', 'Experience in software and hardware implementation and optimization for engineering applications', 'Excellent problem-solving, project management, and communication skills, with the ability to work collaboratively across teams']}, {'title': 'Responsibilities', 'items': ['ICE is seeking an experienced Senior Data Engineer to provide critical technical leadership in supporting pavement management, pavement design, survey, and related engineering services', 'This role will focus on processing, analyzing, and integrating complex datasets into various engineering software platforms and tools used by our team', 'The Senior Data Engineer will collaborate with cross-disciplinary teams, including engineers and surveyors, to ensure data accuracy and usability for strategic decision-making and project delivery', 'Process, analyze, and manage large and complex datasets related to pavement management, pavement design, construction, survey/geomatics, and transportation assets', 'Integrate data into specialized engineering software tools for pavement analysis, construction management, and survey applications, ensuring accuracy, consistency, and usability', 'Develop and streamline data workflows, including data validation and quality control processes, to support the engineering and construction teams', 'Provide technical leadership and guidance to internal teams regarding best practices for data handling and integration across various platforms', 'Collaborate with engineers, surveyors, and asset managers to translate data into actionable insights and assist in developing data-driven strategies for infrastructure projects', 'Support statewide and regional initiatives, including the development of specifications, standards, and documentation for data and asset management', 'Work closely with software vendors and IT teams to implement and optimize engineering software and survey technologies, including hardware upgrades and system integrations', 'Participate in and contribute to national committees and research projects related to data acquisition, construction, and asset management, as appropriate', 'Maintain comprehensive documentation of data processes, methodologies, and workflows to support knowledge sharing and continuity']}], 'apply_options': [{'title': 'JobTarget', 'link': 'https://www.jobtarget.com/jobs/jt-ghssji8ln1/senior-data-engineer-abingdon-virginia?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IkluZnJhc3RydWN0dXJlIENvbnN1bHRpbmcgXHUwMDI2IEVuZ2luZWVyaW5nIiwiYWRkcmVzc19jaXR5IjoiQWJpbmdkb24sIFZBIiwiaHRpZG9jaWQiOiJjYkoxNnN2SGpHSUVyNThDQUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': 'Data Engineer, Senior', 'company_name': 'Booz Allen', 'location': 'Springfield, VA', 'via': 'Indeed', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=s8NAWq4ecvnRpeeIAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCMBAAUFz7CYJws9REBBedKorgWnAtSTyTk3gXchmKn-BXi2943XfR9WfXHFw4EiPWHkZkkgobuIkHRVdDAmG4isSMy2NqrejBWtVsojbXKJggbyuMXmb7Eq__Jk2uYsmu4bTbb2dTOK5XJ5EPDDkjAzGMpRLHJ2F-9HAffhPTJ-qNAAAA&shmds=v1_AdeF8KgK-j8D1ie3cEvtVnHk4F2x9tYYHqwz_DWP1xmBNRmEhA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=s8NAWq4ecvnRpeeIAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965b94c437f54538f07a4/images/c96b123ff18233f350627e9f8aaf79f598db1f31c6dfe9e6130d1ca24bf61edd.jpeg', 'extensions': ['29 days ago', '77.6Kâ€“176K a year', 'Full-time', 'Health insurance'], 'detected_extensions': {'posted_at': '29 days ago', 'salary': '77.6Kâ€“176K a year', 'schedule_type': 'Full-time', 'health_insurance': True}, 'description': 'Data Engineer, Senior\n\nThe Opportunity:\n\nEver-expanding technology like IoT, machine learning, and artificial intelligence means that thereâ€™s more structured and unstructured data available today than ever before. As a data engineer, you know that organizing data can yield pivotal insights when itâ€™s gathered from disparate sources. We need an experienced data engineer like you to help our clients find answers in their data to impact important missions from fraud detection to cancer research, to national intelligence.\n\nAs a data engineer at Booz Allen, youâ€™ll help build advanced technology solutions and implement data engineering activities on some of the most mission-driven projects in the industry. Youâ€™ll deploy and develop pipelines and platforms that organize and make disparate data meaningful.\n\nHere, youâ€™ll work with and guide a multi-disciplinary team of analysts, data engineers, developers, and data consumers in a fast-paced, agile environment. Youâ€™ll use your experience in analytical exploration and data examination while you manage the assessment, design, building, and maintenance of scalable platforms for your clients.\n\nWork with us to use data for good.\n\nJoin us. The world canâ€™t wait.\n\nYou Have:\nâ€¢ 5+ years of experience in application development\nâ€¢ 5+ years of experience designing, developing, operationalizing, and maintaining complex data applications at enterprise scale\nâ€¢ 3+ years of experience creating software for retrieving, parsing, and processing structured and unstructured data\nâ€¢ 3+ years of experience building scalable ETL/ELT workflows for reporting and analytics\nâ€¢ Experience creating solutions within a collaborative, cross-functional team environment\nâ€¢ Ability to develop scripts and programs for converting various types of data into usable formats, and support project team to scale, monitor, and operate data platforms\nâ€¢ TS/SCI clearance\nâ€¢ Bachelorâ€™s degree\n\nNice If You Have:\nâ€¢ Experience with Python, SQL, Scala, or Java\nâ€¢ Experience with UNIX or Linux, including basic commands and Shell scripting\nâ€¢ Experience with a public cloud, including AWS, Microsoft Azure, or Google Cloud\nâ€¢ Experience with distributed data or computing tools, including Spark, Databricks, Hadoop, Hive, AWS EMR, or Kafka\nâ€¢ Experience working on real-time data and streaming applications\nâ€¢ Experience with NoSQL implementation, including MongoDB or Cassandra\nâ€¢ Experience with data warehousing using AWS Redshift, MySQL, or Snowflake\nâ€¢ Experience with Agile engineering practices\nâ€¢ TS/SCI clearance with a polygraph\n\nClearance:\n\nApplicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; TS/SCI clearance is required.\n\nCompensation\n\nAt Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allenâ€™s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page.\n\nSalary at Booz Allen is determined by various factors, including but not limited to location, the individualâ€™s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $77,600.00 to $176,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allenâ€™s total compensation package for employees. This posting will close within 90 days from the Posting Date.\n\nIdentity Statement\n\nAs part of the application process, you are expected to be on camera during interviews and assessments. We reserve the right to take your picture to verify your identity and prevent fraud.\n\nWork Model\nOur people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely.\nâ€¢ If this position is listed as remote or hybrid, youâ€™ll periodically work from a Booz Allen or client site facility.\nâ€¢ If this position is listed as onsite, youâ€™ll work with colleagues and clients in person, as needed for the specific role.\n\nCommitment to Non-Discrimination\n\nAll qualified applicants will receive consideration for employment without regard to disability, status as a protected veteran or any other status protected by applicable federal, state, local, or international law.', 'job_highlights': [{'title': 'Qualifications', 'items': ['5+ years of experience in application development', '5+ years of experience designing, developing, operationalizing, and maintaining complex data applications at enterprise scale', '3+ years of experience creating software for retrieving, parsing, and processing structured and unstructured data', '3+ years of experience building scalable ETL/ELT workflows for reporting and analytics', 'Experience creating solutions within a collaborative, cross-functional team environment', 'Ability to develop scripts and programs for converting various types of data into usable formats, and support project team to scale, monitor, and operate data platforms', 'TS/SCI clearance', 'Bachelorâ€™s degree', 'Experience with Python, SQL, Scala, or Java', 'Experience with UNIX or Linux, including basic commands and Shell scripting', 'Experience with a public cloud, including AWS, Microsoft Azure, or Google Cloud', 'Experience with distributed data or computing tools, including Spark, Databricks, Hadoop, Hive, AWS EMR, or Kafka', 'Experience working on real-time data and streaming applications', 'Experience with NoSQL implementation, including MongoDB or Cassandra', 'Experience with data warehousing using AWS Redshift, MySQL, or Snowflake', 'Experience with Agile engineering practices', 'TS/SCI clearance with a polygraph', 'Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; TS/SCI clearance is required']}, {'title': 'Benefits', 'items': ['At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being', 'Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care', 'Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allenâ€™s benefit programs', 'Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits', 'Salary at Booz Allen is determined by various factors, including but not limited to location, the individualâ€™s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements', 'The projected compensation range for this position is $77,600.00 to $176,000.00 (annualized USD)']}, {'title': 'Responsibilities', 'items': ['As a data engineer at Booz Allen, youâ€™ll help build advanced technology solutions and implement data engineering activities on some of the most mission-driven projects in the industry', 'Youâ€™ll deploy and develop pipelines and platforms that organize and make disparate data meaningful', 'Here, youâ€™ll work with and guide a multi-disciplinary team of analysts, data engineers, developers, and data consumers in a fast-paced, agile environment', 'Youâ€™ll use your experience in analytical exploration and data examination while you manage the assessment, design, building, and maintenance of scalable platforms for your clients', 'If this position is listed as remote or hybrid, youâ€™ll periodically work from a Booz Allen or client site facility', 'If this position is listed as onsite, youâ€™ll work with colleagues and clients in person, as needed for the specific role']}], 'apply_options': [{'title': 'Indeed', 'link': 'https://www.indeed.com/viewjob?jk=18de42405910f6da&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Ladders', 'link': 'https://www.theladders.com/job/data-engineer-senior-booz-allen-hamilton-springfield-va_84117890?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'SimplyHired', 'link': 'https://www.simplyhired.com/job/E8Y38R3G05vvxxU26ZM_Wwbv3VWN6fxijxvSRgoDUTDrFlokdaGPgQ?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'BeBee', 'link': 'https://us.bebee.com/job/c33a19c4414f3433dfdc9bd22f55f668?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobs Trabajo.org', 'link': 'https://us.trabajo.org/job-3624-04e010739959dca35e44abadd7ed1073?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Learn4Good', 'link': 'https://www.learn4good.com/jobs/springfield/virginia/info_technology/4530767589/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyLCBTZW5pb3IiLCJjb21wYW55X25hbWUiOiJCb296IEFsbGVuIiwiYWRkcmVzc19jaXR5IjoiU3ByaW5nZmllbGQsIFZBIiwiaHRpZG9jaWQiOiJzOE5BV3E0ZWN2blJwZWVJQUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': '(USA) Data Engineer III', 'company_name': 'Wal-Mart Stores Texas, LL', 'location': 'United States', 'via': 'Workday', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=v8Zq7oEVx7CKUYYsAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXLsQrCMBCAYVz7CE43arGJCC46CYpU6lSLY7nWI43EXMnd0Ffxba3Lv3z82XeRmVVTn9ZwRkW4ROcjUYKyLKGAG3cghKkfgCNcmV2g5XFQHeVgrUgwThTV96bnj-VIHU_2zZ3808qAicaASu1uv53MGF2ePzEUd0wKtXIigQdNKBuoKvARmuiVXjPNj_wAw9sHs5wAAAA&shmds=v1_AdeF8KhCB8f9jB2g1YeYAbCTIQNZ1TLhrL10hS-dPf96mIcCcw&shem=damc&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=v8Zq7oEVx7CKUYYsAAAAAA%3D%3D', 'extensions': ['Full-time', 'Dental insurance', 'Health insurance', 'Paid time off'], 'detected_extensions': {'schedule_type': 'Full-time', 'dental_coverage': True, 'health_insurance': True, 'paid_time_off': True}, 'description': "Position Summary... Join Walmart/VIZIO Data Lake Team as a Data Engineer III! This role offers the opportunity to work with cutting-edge technology to drive data-driven decisions and innovation within our company. If you have a passion for big data and cloud-based ecosystems, this is the perfect opportunity to elevate your career. What you'll do... About the Team: The VIZIO Data Lake Team operates at the forefront of Walmart's data initiatives, focusing on building and maintaining high-performance, high-availability data structures. Our team is pivotal in enabling data insights that drive strategic business decisions across various domains. As a Data Engineer III, you will directly impact our efficiency and data governance, ensuring that our data processes align with business goals. What Youâ€™ll Do: Extract and transform data from various internal and external sources. Develop and maintain data pipelines and ETL processes. Implement data governance practices and ensure data quality. Design and develop data models, both logical and physical. Lead projects and mentor junior team members. Collaborate with cross-functional teams to support business needs. Stay updated with current data science and analytics trends. What Youâ€™ll Bring: BS or MS in Computer Science or a related field. 3+ years of experience in data engineering. Proficiency in Python, Pyspark, SQL, and/or Scala. Experience with relational SQL and NoSQL databases. Strong understanding of in-memory processing and data formats (Avro, Parquet, JSON, etc.). Experience with AWS cloud services (EC2, MSK, S3, RDS, SNS, SQS). Knowledge of stream-processing systems (Storm, Spark-Structured-Streaming, Kafka). Familiarity with data pipeline and workflow management tools (Apache Airflow, AWS Data Pipeline). Bonus: Experience with Databricks, Snowflake, and Thoughtspot. At Walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet. Health benefits include medical, vision and dental coverage. Financial benefits include 401(k), stock purchase and company-paid life insurance. Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting. Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more. \u200e \u200e \u200e You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes. The amount you receive depends on your job classification and length of employment. It will meet or exceed the requirements of paid sick leave laws, where applicable. \u200e For information about PTO, see https://one.walmart.com/notices. \u200e \u200e Live Better U is a Walmart-paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities. Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates. Tuition, books, and fees are completely paid for by Walmart. \u200e Eligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to a specific plan or program terms. \u200e For information about benefits and eligibility, see One.Walmart. \u200e The annual salary range for this position is $99,000.00-$198,000.00 \u200e Additional compensation includes annual or quarterly performance bonuses. \u200e Additional compensation for certain positions may also include: \u200e \u200e - Stock \u200e \u200e Minimum Qualifications... Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications. Option 1: Bachelorâ€™s degree in Computer Science and 2 years' experience in software engineering or related field. Option 2: 4 yearsâ€™ experience in software engineering or related field. Option 3: Master's degree in Computer Science. Preferred Qualifications... Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications. Data engineering, database engineering, business intelligence, or business analytics, Masterâ€™s degree in Computer Science or related field and 2 years' experience in software engineering or related field, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly. The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmartâ€™s accessibility standards and guidelines for supporting an inclusive culture. Primary Location... 55 Madison St, Denver, CO 80206-5419, United States of America About Walmart: Fifty years ago, Sam Walton started a single mom-and-pop shop and transformed it into the world's biggest retailer. Since those founding days, one thing has remained consistent: our commitment to helping our customers save money so they can live better. Today, we're reinventing the shopping experience and our associates are at the heart of it. You'll play a crucial role in shaping the future of retail, improving millions of lives around the world. This is that place where your passions meet purpose. Join our family and create a career you're proud of.", 'job_highlights': [{'title': 'Qualifications', 'items': ['What Youâ€™ll Bring: BS or MS in Computer Science or a related field', '3+ years of experience in data engineering', 'Proficiency in Python, Pyspark, SQL, and/or Scala', 'Experience with relational SQL and NoSQL databases', 'Strong understanding of in-memory processing and data formats (Avro, Parquet, JSON, etc.). Experience with AWS cloud services (EC2, MSK, S3, RDS, SNS, SQS)', 'Knowledge of stream-processing systems (Storm, Spark-Structured-Streaming, Kafka)', 'Familiarity with data pipeline and workflow management tools (Apache Airflow, AWS Data Pipeline)', "Option 1: Bachelorâ€™s degree in Computer Science and 2 years' experience in software engineering or related field", 'Option 2: 4 yearsâ€™ experience in software engineering or related field', "Option 3: Master's degree in Computer Science", "Data engineering, database engineering, business intelligence, or business analytics, Masterâ€™s degree in Computer Science or related field and 2 years' experience in software engineering or related field, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly", 'The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmartâ€™s accessibility standards and guidelines for supporting an inclusive culture']}, {'title': 'Benefits', 'items': ['At Walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet', 'Health benefits include medical, vision and dental coverage', 'Financial benefits include 401(k), stock purchase and company-paid life insurance', 'Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting', 'Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more', '\u200e \u200e \u200e You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes', 'The amount you receive depends on your job classification and length of employment', 'It will meet or exceed the requirements of paid sick leave laws, where applicable', "\u200e \u200e Live Better U is a Walmart-paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities", "Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates", 'Tuition, books, and fees are completely paid for by Walmart', '\u200e Eligibility requirements apply to some benefits and may depend on your job classification and length of employment', 'Benefits are subject to change and may be subject to a specific plan or program terms', '\u200e The annual salary range for this position is $99,000.00-$198,000.00 \u200e Additional compensation includes annual or quarterly performance bonuses', '\u200e Additional compensation for certain positions may also include: \u200e \u200e - Stock \u200e \u200e Minimum Qualifications..']}, {'title': 'Responsibilities', 'items': ['As a Data Engineer III, you will directly impact our efficiency and data governance, ensuring that our data processes align with business goals', 'What Youâ€™ll Do: Extract and transform data from various internal and external sources', 'Develop and maintain data pipelines and ETL processes', 'Implement data governance practices and ensure data quality', 'Design and develop data models, both logical and physical', 'Lead projects and mentor junior team members', 'Collaborate with cross-functional teams to support business needs', 'Stay updated with current data science and analytics trends']}], 'apply_options': [{'title': 'Workday', 'link': 'https://walmart.wd5.myworkdayjobs.com/en-US/WalmartExternal/job/XMLNAME--USA--Data-Engineer-III_R-2182936?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiIoVVNBKSBEYXRhIEVuZ2luZWVyIElJSSIsImNvbXBhbnlfbmFtZSI6IldhbC1NYXJ0IFN0b3JlcyBUZXhhcywgTEwiLCJhZGRyZXNzX2NpdHkiOiJVbml0ZWQgU3RhdGVzIiwiaHRpZG9jaWQiOiJ2OFpxN29FVng3Q0tVWVlzQUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': 'Software Engineer, Data New York, New York, United States', 'company_name': 'Kalshi Inc.', 'location': 'Snowflake, AZ', 'via': 'Jobrapido', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=AJHRI3Ldfcd3msKRAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_02NsQrCMBRFce0niMObpSYiuOgkKKKCCMVBF0njaxMb3yt5gfan_Efr5nI5w-Hc7DPKLgVXqTMRYUe1J8SYw9YkA2fs4Maxyf_oSj7hE4pkEgrM4MglCJpoHTDBnrkOOF67lFpZaS0SVC2D662y_NZMWHKvX1zKbx7ihts2DK3HYjnvVUv1dHIyQZyHA1kFnqAg7qpgGsxhc_8C8YmOxLAAAAA&shmds=v1_AdeF8Kj5iZAiE4jv0UjJ51QAWGLI_rvl2Syzc994NKxahBpVbA&shem=damc&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=AJHRI3Ldfcd3msKRAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965bbb6a867d1069fe7ba/images/5282d117823d9751b50072e836128153590ed48b7dec7ef09ba54f180aafd1e8.jpeg', 'extensions': ['2 days ago', 'Full-time', 'Health insurance'], 'detected_extensions': {'posted_at': '2 days ago', 'schedule_type': 'Full-time', 'health_insurance': True}, 'description': "Kalshi has defined a new category: prediction markets. Kalshi allows people to trade on the outcome of any events and turn any question about the future into a financial asset. Kalshi fought for years and legalized prediction markets in the US for the first time in history. Kalshi is currently the fastest growing financial market in America, and has thousands of markets across politics, economics, financials, weather, tech, AI, culture and more.\n\nWe believe prediction markets have the potential to be the largest financial market because they turn anything into a financial position.\nâ€¢ Our vision: wellâ€¦ build the largest financial market on the planet.\nâ€¢ Our mission: bring more truth to the world through the power of markets.\n\nBuilding a new category is hardâ€¦ like really hard. But itâ€™s beautiful and deeply fulfilling. Our culture is simple: we hire really talented people, work really hard, and enjoy the climb. We are looking for ambitious and exceptional people to join our (relatively small) team to help us build the next generation of financial markets.\nRole Roadmap\n\nAs our go-to data person at Kalshi, you will build and operate the full data stack at Kalshi. This role is one part data engineering, one part data analysis.\n\nSome of the projects you may work on:\nâ€¢ Partner closely across the business to find improvements/opportunities and influence decisions using data science methodologies and tools\nâ€¢ Drive the collection of new data and the refinement of existing data sources and pipelines\nâ€¢ Build actionable KPIs, create production-quality dashboards and notebooks to convey insights\nâ€¢ Analyze large, complex datasets to extract insights and decide on the appropriate techniques and data representation\nâ€¢ Define and advance best practices within an experiment-driven culture\nâ€¢ Inform product engineering roadmap through analysis of marketplace, user behavior, and product trends\n\nTechnology:\n\nOur tech stack includes Python, SQL, DBT for data analytics and pipelining, Spark for big data processing, and visualization through Superset. We leverage cloud platforms such as AWS for scalable data storage and computational needs.\nWhat youâ€™ll do:\nâ€¢ Partner with backend engineering teams to design, develop and implement large scale, high volume, high performance data models and pipelines for Data Lake and Data Warehouse\nâ€¢ Design and maintain robust ETL processes, focusing on data quality, error handling, and automated monitoring while continuously optimizing performance and resource utilization\nâ€¢ Work with product teams to identify gaps in our end-to-end data insights workflow and lead engineering efforts to improve it\nâ€¢ Partner with data scientists to implement and optimize feature engineering pipelines, support experimentation workflows, and build scalable infrastructure for model deployment and training data preparation\nâ€¢ Build intuitive internal tools and dashboards that enable operations and growth teams to efficiently query and analyze data without requiring deep technical expertise\nWhat weâ€™re looking for:\nâ€¢ Bachelor's degree in Computer Science or equivalent professional experience, with 4+ years of hands-on software development\nâ€¢ Strong command of key programming languages (Python, Golang, Java)\nâ€¢ Production experience with real-time data streaming platforms like Apache Kafka and Flink\nâ€¢ Hands-on expertise with major cloud data warehouses (Snowflake, Redshift) and traditional databases like PostgreSQL\nâ€¢ Strong data visualization skills using industry-standard tools (Tableau, Superset, Looker)\nâ€¢ Proven track record of taking initiative and thriving in fast-paced environments\nâ€¢ Sharp analytical skills with a focus on market dynamics and consumer insights\nâ€¢ Passionate about your craft and committed to delivering exceptional results\nâ€¢ Spearheaded data engineering initiatives for a high-growth consumer platform, delivering end-to-end data solutions from ingestion to insights\nOur Culture\n\nMeritocracy is at our core, and we value people who take ownership and figure (usually hard) things out. We dream big. We love our craft deeply and are proud of what we put out in the world. We are committed to our vision of building something bigâ€¦ but also useful: a product that brings more truth through the power of markets.\n\nKalshians are Kalshiâ€™s most important asset: we pick Kalshians carefully, so we trust them fully on day 1.\nNYC Pay Transparency Disclosure:\n\nSalary Range: $100,000 to $250,000 annually plus equity and benefits.\n\nThis salary range is based on the current available market data and represents the expected salary range for this role. Kalshi has minimal hierarchy and few titles, but a broad range of experience is represented within roles. Should you have compensation expectations that exceed these bands, we'd love to hear from you and would welcome you to reach out to discuss further.\nCommitment to Equal Opportunity\n\nKalshi is committed to creating a culture of inclusion and belonging, and we are proud to be an equal opportunity employer. We believe it is our collective responsibility to uphold these values and encourage candidates from all backgrounds to join us in our mission. All qualified applicants will be treated with respect and receive equal consideration for employment without regard to race, color, creed, religion, sex, gender identity, sexual orientation, national origin, disability, uniform service, veteran status, age, or any other protected characteristic per federal, state, or local law. If you are passionate about what you do and want to use your talents to support our mission and values, weâ€™d love to hear from you.\nApply for this job\nâ€¢ indicates a required field\n\nFirst Name *\n\nLast Name *\n\nEmail *\n\nPhone *\n\nResume/CV *\n\nLinkedIn Profile *\n\nWebsite\n\nWhy Kalshi? *\n\nAre you legally authorized to work in the United States? * Select...\n\nWill you now or in the future require sponsorship for employment visa status (e.g., H-1B visa status)? * Select...\n\nDo you live in New York City? This role requires 3-4 days a week in office. *\n\n#J-18808-Ljbffr", 'job_highlights': [{'title': 'Qualifications', 'items': ["Bachelor's degree in Computer Science or equivalent professional experience, with 4+ years of hands-on software development", 'Strong command of key programming languages (Python, Golang, Java)', 'Production experience with real-time data streaming platforms like Apache Kafka and Flink', 'Hands-on expertise with major cloud data warehouses (Snowflake, Redshift) and traditional databases like PostgreSQL', 'Strong data visualization skills using industry-standard tools (Tableau, Superset, Looker)', 'Proven track record of taking initiative and thriving in fast-paced environments', 'Sharp analytical skills with a focus on market dynamics and consumer insights', 'Passionate about your craft and committed to delivering exceptional results', 'Spearheaded data engineering initiatives for a high-growth consumer platform, delivering end-to-end data solutions from ingestion to insights', 'indicates a required field']}, {'title': 'Benefits', 'items': ['Salary Range: $100,000 to $250,000 annually plus equity and benefits', 'This salary range is based on the current available market data and represents the expected salary range for this role']}, {'title': 'Responsibilities', 'items': ['As our go-to data person at Kalshi, you will build and operate the full data stack at Kalshi', 'This role is one part data engineering, one part data analysis', 'Partner closely across the business to find improvements/opportunities and influence decisions using data science methodologies and tools', 'Drive the collection of new data and the refinement of existing data sources and pipelines', 'Build actionable KPIs, create production-quality dashboards and notebooks to convey insights', 'Analyze large, complex datasets to extract insights and decide on the appropriate techniques and data representation', 'Define and advance best practices within an experiment-driven culture', 'Inform product engineering roadmap through analysis of marketplace, user behavior, and product trends', 'Partner with backend engineering teams to design, develop and implement large scale, high volume, high performance data models and pipelines for Data Lake and Data Warehouse', 'Design and maintain robust ETL processes, focusing on data quality, error handling, and automated monitoring while continuously optimizing performance and resource utilization', 'Work with product teams to identify gaps in our end-to-end data insights workflow and lead engineering efforts to improve it', 'Partner with data scientists to implement and optimize feature engineering pipelines, support experimentation workflows, and build scalable infrastructure for model deployment and training data preparation', 'Build intuitive internal tools and dashboards that enable operations and growth teams to efficiently query and analyze data without requiring deep technical expertise']}], 'apply_options': [{'title': 'Jobrapido', 'link': 'https://us.jobrapido.com/jobpreview/7594915277255475200?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTb2Z0d2FyZSBFbmdpbmVlciwgRGF0YSBOZXcgWW9yaywgTmV3IFlvcmssIFVuaXRlZCBTdGF0ZXMiLCJjb21wYW55X25hbWUiOiJLYWxzaGkgSW5jLiIsImFkZHJlc3NfY2l0eSI6IlNub3dmbGFrZSwgQVoiLCJodGlkb2NpZCI6IkFKSFJJM0xkZmNkM21zS1JBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Software Engineering Manager - Data, Burger King', 'company_name': 'Restaurant Brands International', 'location': 'Miami, FL', 'via': 'Careers At RBI - Restaurant Brands International', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=PmOobKEr9dNBeVQ7AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWNSw6CQBBE45YjuOq1QcaYuNEdUYkfNnoA0mA7jBm6yUwTOZgHFDYvqVReVfJbJMVT3vrFQHBi65goOLZQIqOlAGs4omIK-RDmeJu7NVylhkgYmhaEoRCxnpaHVrWPe2Ni9JmNiuqarJHOCFMto_lIHWdUsZ3eeo9K1Xa3GbOe7Sp90GQMAVkhn_iKcGGlwNOKMHpwDKXDzqVwvv8BqMRINbcAAAA&shmds=v1_AdeF8KhrxlmmNsyS3CE4LbiN5cumCQbrxSm3WlZPoBfvf_Cx5g&shem=damc&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=PmOobKEr9dNBeVQ7AAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965bbb6a867d1069fe7ba/images/5282d117823d97517ca92bf1b2b14e33655f938dc552805c1fee9ed274e3715b.png', 'extensions': ['6 days ago', 'Full-time'], 'detected_extensions': {'posted_at': '6 days ago', 'schedule_type': 'Full-time'}, 'description': "Ready to make your next big professional move? Join us on our journey to achieve our big dream of building the most loved restaurant brands in the world.\n\nRestaurant Brands International Inc. is one of the world's largest quick service restaurant companies with nearly $45 billion in annual system-wide sales and over 32,000 restaurants in more than 120 countries and territories.\n\nRBI owns four of the world's most prominent and iconic quick service restaurant brands â€“ TIM HORTONSÂ®, BURGER KINGÂ®, POPEYESÂ®, and FIREHOUSE SUBSÂ®. These independently operated brands have been serving their respective guests, franchisees and communities for decades. Through its Restaurant Brands for Good framework, RBI is improving sustainable outcomes related to its food, the planet, and people and communities.\n\nRBI is committed to growing the TIM HORTONSÂ®, BURGER KINGÂ®, POPEYESÂ® and FIREHOUSE SUBSÂ® brands by leveraging their respective core values, employee and franchisee relationships, and long track records of community support. Each brand benefits from the global scale and shared best practices that come from ownership by Restaurant Brands International Inc.\n\nThe Data Engineering Manager is responsible for leading a team of data engineers to design, build, and maintain robust data pipelines and infrastructure for The Burger King Company in the US. This role oversees the development and optimization of scalable data solutions that handle millions of transactions per day, enabling actionable insights and supporting strategic business initiatives. The ideal candidate will have extensive technical expertise, leadership experience, and a passion for innovation in data engineering.\n\nThis position is based in Miami, FL and is in the office 5 days a week.\n\nRole & Responsibilities:\nâ€¢ Lead and mentor a team of data engineers, fostering professional growth and knowledge sharing.\nâ€¢ Oversee day-to-day activities of the data engineering team, ensuring timely delivery of projects.\nâ€¢ Collaborate with cross-functional teams, including data scientists, analysts, product managers, and IT, to develop and implement data solutions that meet business needs.\nâ€¢ Design, build, and maintain scalable and efficient data pipelines and ETL processes.\nâ€¢ Implement best practices for data integration, modeling, quality, and governance.\nâ€¢ Ensure data reliability, integrity, and availability for critical analytics workflows.\nâ€¢ Drive the development of the overall data engineering strategy and optimize processes to ensure efficiency and scalability.\nâ€¢ Plan and execute data engineering projects, ensuring alignment with organizational objectives.\nâ€¢ Gather and document requirements for projects, create data migration plans, and communicate progress with stakeholders.\nâ€¢ Manage relationships with technology vendors and oversee service-level agreements.\nâ€¢ Continuously evaluate emerging technologies and methodologies to improve data engineering capabilities.\nâ€¢ Encourage a culture of innovation and continuous improvement within the team.\n\nQualifications & Skills:\nâ€¢ 5+ years of progressive experience in data engineering,\nâ€¢ 2+ years leading people and engineering teams.\nâ€¢ Bachelorâ€™s or masterâ€™s degree in Computer Science, Engineering, Information Technology, or a related field.\nâ€¢ Strong programming skills in Python and SQL.\nâ€¢ Extensive experience with AWS services (e.g., S3, Glue, Athena, EMR, SNS, SQS, KMS).\nâ€¢ Experience with data warehousing solutions like Snowflake or Redshift.\nâ€¢ Proficiency with workflow orchestration tools (e.g., Apache Airflow, Dagster).\nâ€¢ Familiarity with reporting tools (e.g., Tableau, MicroStrategy).\nâ€¢ Experience with Infrastructure as Code (e.g., Terraform).\nâ€¢ Knowledge of NoSQL databases like DynamoDB.\nâ€¢ Experience with Retool development and ticketing systems such as JIRA.\nâ€¢ Demonstrated ability to lead and manage multiple complex projects in a fast-paced environment.\nâ€¢ Excellent verbal and written communication skills, with the ability to translate technical concepts for non-technical stakeholders.\nâ€¢ Strong analytical and problem-solving abilities.\nâ€¢ Ability to build and maintain cross-functional relationships while managing competing priorities.\nâ€¢ Self-motivated and driven to continuously improve processes and outcomes.\n\n#BurgerKing\n\nBenefits at all of our global offices are focused on physical, mental and financial wellness. We offer unique and progressive benefits, including a comprehensive global paid parental leave program that supports employees as they expand their families, free telemedicine and mental wellness support.\n\nRestaurant Brands International and all of its affiliated companies (collectively, RBI) are equal opportunity and affirmative action employers that do not discriminate on the basis of race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or veteran status, or any other characteristic protected by local, state, provincial or federal laws, rules, or regulations. RBI's policy applies to all terms and conditions of employment. Accommodation is available for applicants with disabilities upon request.", 'job_highlights': [{'title': 'Qualifications', 'items': ['The ideal candidate will have extensive technical expertise, leadership experience, and a passion for innovation in data engineering', '5+ years of progressive experience in data engineering,', '2+ years leading people and engineering teams', 'Bachelorâ€™s or masterâ€™s degree in Computer Science, Engineering, Information Technology, or a related field', 'Strong programming skills in Python and SQL', 'Extensive experience with AWS services (e.g., S3, Glue, Athena, EMR, SNS, SQS, KMS)', 'Experience with data warehousing solutions like Snowflake or Redshift', 'Proficiency with workflow orchestration tools (e.g., Apache Airflow, Dagster)', 'Familiarity with reporting tools (e.g., Tableau, MicroStrategy)', 'Experience with Infrastructure as Code (e.g., Terraform)', 'Knowledge of NoSQL databases like DynamoDB', 'Experience with Retool development and ticketing systems such as JIRA', 'Demonstrated ability to lead and manage multiple complex projects in a fast-paced environment', 'Excellent verbal and written communication skills, with the ability to translate technical concepts for non-technical stakeholders', 'Strong analytical and problem-solving abilities', 'Ability to build and maintain cross-functional relationships while managing competing priorities', 'Self-motivated and driven to continuously improve processes and outcomes']}, {'title': 'Benefits', 'items': ['We offer unique and progressive benefits, including a comprehensive global paid parental leave program that supports employees as they expand their families, free telemedicine and mental wellness support']}, {'title': 'Responsibilities', 'items': ['The Data Engineering Manager is responsible for leading a team of data engineers to design, build, and maintain robust data pipelines and infrastructure for The Burger King Company in the US', 'This role oversees the development and optimization of scalable data solutions that handle millions of transactions per day, enabling actionable insights and supporting strategic business initiatives', 'This position is based in Miami, FL and is in the office 5 days a week', 'Lead and mentor a team of data engineers, fostering professional growth and knowledge sharing', 'Oversee day-to-day activities of the data engineering team, ensuring timely delivery of projects', 'Collaborate with cross-functional teams, including data scientists, analysts, product managers, and IT, to develop and implement data solutions that meet business needs', 'Design, build, and maintain scalable and efficient data pipelines and ETL processes', 'Implement best practices for data integration, modeling, quality, and governance', 'Ensure data reliability, integrity, and availability for critical analytics workflows', 'Drive the development of the overall data engineering strategy and optimize processes to ensure efficiency and scalability', 'Plan and execute data engineering projects, ensuring alignment with organizational objectives', 'Gather and document requirements for projects, create data migration plans, and communicate progress with stakeholders', 'Manage relationships with technology vendors and oversee service-level agreements', 'Continuously evaluate emerging technologies and methodologies to improve data engineering capabilities', 'Encourage a culture of innovation and continuous improvement within the team']}], 'apply_options': [{'title': 'Careers At RBI - Restaurant Brands International', 'link': 'https://careers.rbi.com/global/en/job/R2436/Software-Engineering-Manager-Data-Burger-King?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Indeed', 'link': 'https://www.indeed.com/viewjob?jk=e5066bd242b455f9&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'LinkedIn', 'link': 'https://www.linkedin.com/jobs/view/software-engineering-manager-data-burger-king-at-burger-king-4335572105?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'SimplyHired', 'link': 'https://www.simplyhired.com/job/ev1gRZCU72SNuU1nWAYeqS9yuwWu1UKR5aml6AorbJL1LlxYKyVbuA?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Ladders', 'link': 'https://www.theladders.com/job/manager-of-software-engineering-controls-neocis-miami-fl_80712284?ir=1&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Qureos', 'link': 'https://app.qureos.com/jobs/software-engineering-manager-data-burger-king-1762952467401?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Talentify', 'link': 'https://www.talentify.io/job/data-engineering-manager-burger-king-miami-florida-us-popeyes-louisiana-kitchen-r2436?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'WhatJobs', 'link': 'https://www.whatjobs.com/jobs/technical-leadership?id=2262729044&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTb2Z0d2FyZSBFbmdpbmVlcmluZyBNYW5hZ2VyIC0gRGF0YSwgQnVyZ2VyIEtpbmciLCJjb21wYW55X25hbWUiOiJSZXN0YXVyYW50IEJyYW5kcyBJbnRlcm5hdGlvbmFsIiwiYWRkcmVzc19jaXR5IjoiTWlhbWksIEZMIiwiaHRpZG9jaWQiOiJQbU9vYktFcjlkTkJlVlE3QUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': 'Senior Analytics Data Engineer', 'company_name': 'Boston Scientific', 'location': 'Maple Grove, MN', 'via': 'Jobs At Boston Scientific', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=5Hm8PKlFOHNT-sUnAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMMQrCQBBAUWxzBKsprESzIqTRSlECQmxygDBZxmRlnVl2BolX8bTG5nfvF99FUbXEQTKcGOPHgle4oCFceQhMlGELN-lBCbMfQRhqkSHS8jiaJT04pxrLQQ1nWnp5OWHqZXJP6fWfTkfMlCIadftqN5WJh_XqLGrzq_WB2MIjeAgMDaZIUGd50waa-w8avPfunQAAAA&shmds=v1_AdeF8KhRPoWTabitctLmEaiqLxnjOxFBtofxd9ZPPXyM-NuCZg&shem=damc&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=5Hm8PKlFOHNT-sUnAAAAAA%3D%3D', 'extensions': ['25 days ago', 'Full-time', 'Health insurance'], 'detected_extensions': {'posted_at': '25 days ago', 'schedule_type': 'Full-time', 'health_insurance': True}, 'description': 'Additional Location(s): N/A\n\nDiversity - Innovation - Caring - Global Collaboration - Winning Spirit - High Performance\n\nAt Boston Scientific, weâ€™ll give you the opportunity to harness all thatâ€™s within you by working in teams of diverse and high-performing employees, tackling some of the most important health industry challenges. With access to the latest tools, information and training, weâ€™ll help you in advancing your skills and career. Here, youâ€™ll be supported in progressing â€“ whatever your ambitions.\n\nAbout the role:\n\nWe are seeking a Senior Data Analytics Engineer to join our Cardiology Marketing & Digital Enablement (CMDE) team. In this role, you will lead the design and development of scalable, analytics-ready data models in Snowflake using dbt and other modern data transformation tools. Your work will empower marketing and commercial teams with trusted, curated datasets that drive data-informed decision-making, advanced analytics, and AI-powered marketing initiatives.\n\nAs a technical leader, you will shape the data transformation layer of our marketing analytics ecosystemâ€”turning complex, multi-source data into business-ready models that fuel dashboards, audience segmentation, and campaign optimization.\n\nWork Mode:\nAt Boston Scientific, we value collaboration and synergy. This role follows a hybrid work model, requiring employees to be in our Maple Grove, MN office at least three days per week.\n\nVisa Sponsorship:\nBoston Scientific will not offer, transfer sponsorship or take over sponsorship of an employment visa for this position at this time.\n\nYour responsibilities will include:\nâ€¢ Lead the\u202ftransformation and modeling\u202fof marketing, commercial, and healthcare data in\u202fSnowflake\u202fusing\u202fdbt, ensuring scalability, reliability, and documentation.\nâ€¢ Collaborate with analysts, data scientists, and marketers to\u202ftranslate business needs into well-defined data models\u202fand reusable data products.\nâ€¢ Implement\u202fbest practices in data modeling, testing, and version control to ensure high data quality and governance.\nâ€¢ Drive\u202fdata governance and standardization\u202fefforts to improve data trust, consistency, and accessibility, to ensure all data processes align with internal governance standards, privacy requirements, and documentation protocols.\nâ€¢ Partner with data platforms and architecture teams to enhance data workflows, orchestration, and automation.\nâ€¢ Serve as a\u202fmentor and thought leader\u202fin dbt, analytics engineering, and modern data development practices.\n\nRequired qualifications:\nâ€¢ Bachelorâ€™s degree in information technology, Computer Science, Engineering, or a related field.\nâ€¢ 5+ years of professional experience in data engineering or a similar technical role focused on transforming and modeling data for business use.\nâ€¢ Strong expertise in dbt (Data Build Tool), SQL, and Snowflake.\nâ€¢ Proven experience developing\u202fmodular, documented, and tested dbt models\u202fin production environments.\nâ€¢ Familiarity with\u202fGit-based workflows, CI/CD pipelines, and leading data test case creation to validate data quality and integrity.\nâ€¢ Understanding of\u202fmarketing and commercial data\u202fdomains (CRM, campaign data, digital engagement) preferred, ideally Salesforce.\nâ€¢ Familiarity with working in regulated industries or with sensitive business data (e.g., healthcare, life sciences).\n\nPreferred qualifications:\nâ€¢ Excellent communication and documentation skills.\nâ€¢ Experience with marketing data (e.g., campaign performance, lead funnel, audience segmentation, web/media analytics).\nâ€¢ Experience with healthcare claims and prescription data.\nâ€¢ Experience with Python for data transformation or workflow automation.\nâ€¢ Familiarity with semantic layers and AI/ML data enablement.\n\nRequisition ID: 616896\n\nMinimum Salary: $ 82600\n\nMaximum Salary: $ 156900\n\nThe anticipated compensation listed above and the value of core and optional employee benefits offered by Boston Scientific (BSC) â€“ see www.bscbenefitsconnect.comâ€”will vary based on actual location of the position and other pertinent factors considered in determining actual compensation for the role. Compensation will be commensurate with demonstrable level of experience and training, pertinent education including licensure and certifications, among other relevant business or organizational needs. At BSC, it is not typical for an individual to be hired near the bottom or top of the anticipated salary range listed above.\n\nCompensation for non-exempt (hourly), non-sales roles may also include variable compensation from time to time (e.g., any overtime and shift differential) and annual bonus target (subject to plan eligibility and other requirements).\n\nCompensation for exempt, non-sales roles may also include variable compensation, i.e., annual bonus target and long-term incentives (subject to plan eligibility and other requirements).\n\nFor MA positions: It is unlawful to require or administer a lie detector test for employment. Violators are subject to criminal penalties and civil liability.\n\nAs a leader in medical science for more than 40 years, we are committed to solving the challenges that matter most â€“ united by a deep caring for human life. Our mission to advance science for life is about transforming lives through innovative medical solutions that improve patient lives, create value for our customers, and support our employees and the communities in which we operate. Now more than ever, we have a responsibility to apply those values to everything we do â€“ as a global business and as a global corporate citizen.\n\nSo, choosing a career with Boston Scientific (NYSE: BSX) isnâ€™t just business, itâ€™s personal. And if youâ€™re a natural problem-solver with the imagination, determination, and spirit to make a meaningful difference to people worldwide, we encourage you to apply and look forward to connecting with you!\n\nAt Boston Scientific, we recognize that nurturing a diverse and inclusive workplace helps us be more innovative and it is important in our work of advancing science for life and improving patient health. That is why we stand for inclusion, equality, and opportunity for all. By embracing the richness of our unique backgrounds and perspectives, we create a better, more rewarding place for our employees to work and reflect the patients, customers, and communities we serve.\n\nBoston Scientific Corporation has been and will continue to be an equal opportunity employer. To ensure full implementation of its equal employment policy, the Company will continue to take steps to assure that recruitment, hiring, assignment, promotion, compensation, and all other personnel decisions are made and administered without regard to race, religion, color, national origin, citizenship, sex, sexual orientation, gender identity, gender expression, veteran status, age, mental or physical disability, genetic information or any other protected class.\n\nPlease be advised that certain US based positions, including without limitation field sales and service positions that call on hospitals and/or health care centers, require acceptable proof of COVID-19 vaccination status. Candidates will be notified during the interview and selection process if the role(s) for which they have applied require proof of vaccination as a condition of employment. Boston Scientific continues to evaluate its policies and protocols regarding the COVID-19 vaccine and will comply with all applicable state and federal law and healthcare credentialing requirements. As employees of the Company, you will be expected to meet the ongoing requirements for your roles, including any new requirements, should the Companyâ€™s policies or protocols change with regard to COVID-19 vaccination.\n\nAmong other requirements, Boston Scientific maintains specific prohibited substance test requirements for safety-sensitive positions. This role is deemed safety-sensitive and, as such, candidates will be subject to a prohibited substance test as a requirement. The goal of the prohibited substance testing is to increase workplace safety in compliance with the applicable law.', 'job_highlights': [{'title': 'Qualifications', 'items': ['Bachelorâ€™s degree in information technology, Computer Science, Engineering, or a related field', '5+ years of professional experience in data engineering or a similar technical role focused on transforming and modeling data for business use', 'Strong expertise in dbt (Data Build Tool), SQL, and Snowflake', 'Proven experience developing\u202fmodular, documented, and tested dbt models\u202fin production environments', 'Familiarity with\u202fGit-based workflows, CI/CD pipelines, and leading data test case creation to validate data quality and integrity', 'Familiarity with working in regulated industries or with sensitive business data (e.g., healthcare, life sciences)']}, {'title': 'Benefits', 'items': ['With access to the latest tools, information and training, weâ€™ll help you in advancing your skills and career', 'Minimum Salary: $ 82600', 'Maximum Salary: $ 156900', 'The anticipated compensation listed above and the value of core and optional employee benefits offered by Boston Scientific (BSC) â€“ see www.bscbenefitsconnect.comâ€”will vary based on actual location of the position and other pertinent factors considered in determining actual compensation for the role', 'Compensation will be commensurate with demonstrable level of experience and training, pertinent education including licensure and certifications, among other relevant business or organizational needs', 'At BSC, it is not typical for an individual to be hired near the bottom or top of the anticipated salary range listed above', 'Compensation for non-exempt (hourly), non-sales roles may also include variable compensation from time to time (e.g., any overtime and shift differential) and annual bonus target (subject to plan eligibility and other requirements)', 'Compensation for exempt, non-sales roles may also include variable compensation, i.e., annual bonus target and long-term incentives (subject to plan eligibility and other requirements)']}, {'title': 'Responsibilities', 'items': ['In this role, you will lead the design and development of scalable, analytics-ready data models in Snowflake using dbt and other modern data transformation tools', 'Your work will empower marketing and commercial teams with trusted, curated datasets that drive data-informed decision-making, advanced analytics, and AI-powered marketing initiatives', 'As a technical leader, you will shape the data transformation layer of our marketing analytics ecosystemâ€”turning complex, multi-source data into business-ready models that fuel dashboards, audience segmentation, and campaign optimization', 'Lead the\u202ftransformation and modeling\u202fof marketing, commercial, and healthcare data in\u202fSnowflake\u202fusing\u202fdbt, ensuring scalability, reliability, and documentation', 'Collaborate with analysts, data scientists, and marketers to\u202ftranslate business needs into well-defined data models\u202fand reusable data products', 'Implement\u202fbest practices in data modeling, testing, and version control to ensure high data quality and governance', 'Drive\u202fdata governance and standardization\u202fefforts to improve data trust, consistency, and accessibility, to ensure all data processes align with internal governance standards, privacy requirements, and documentation protocols', 'Partner with data platforms and architecture teams to enhance data workflows, orchestration, and automation', 'Serve as a\u202fmentor and thought leader\u202fin dbt, analytics engineering, and modern data development practices', 'Requisition ID: 616896']}], 'apply_options': [{'title': 'Jobs At Boston Scientific', 'link': 'https://jobs.bostonscientific.com/job/Maple-Grove-Senior-Analytics-Data-Engineer-MN-55311/1336876400/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'ZipRecruiter', 'link': 'https://www.ziprecruiter.com/c/Boston-Scientific/Job/Senior-Analytics-Data-Engineer/-in-Maple-Grove,MN?jid=3c24718738d9afb9&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Glassdoor', 'link': 'https://www.glassdoor.com/job-listing/senior-analytics-data-engineer-boston-scientific-JV_IC1162297_KO0,30_KE31,48.htm?jl=1009915597890&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'LinkedIn', 'link': 'https://www.linkedin.com/jobs/view/senior-analytics-data-engineer-at-boston-scientific-4331867440?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobright', 'link': 'https://jobright.ai/jobs/info/68fe9838e04ac838fb5b47ae?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Ladders', 'link': 'https://www.theladders.com/job/senior-analytics-data-engineer-bostonscientific-maple-grove-mn_82167476?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Local Job Network', 'link': 'https://jobs.localjobnetwork.com/j/t-Senior-Analytics-Data-Engineer-e-Boston-Scientific-l-Maple-Grove,-MN-jobs-j85267750.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Talentify', 'link': 'https://www.talentify.io/job/senior-analytics-data-engineer-maple-grove-minnesota-us-boston-scientific-616896?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTZW5pb3IgQW5hbHl0aWNzIERhdGEgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJCb3N0b24gU2NpZW50aWZpYyIsImFkZHJlc3NfY2l0eSI6Ik1hcGxlIEdyb3ZlLCBNTiIsImh0aWRvY2lkIjoiNUhtOFBLbEZPSE5ULXNVbkFBQUFBQT09IiwiaGwiOiJlbiJ9'}, {'title': 'Principal Data Engineer - Snowflake (US)', 'company_name': 'AND Digital', 'location': 'Florida City, FL', 'via': 'SimplyHired', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=h3X8UMijjNxLFUX2AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNsQrCMBCAYVz7CC7eqKKNCC46iFgriIhQnMs1xvQ05kJyYH0aX1Vd_uFb_uzTy9bnSF5TQAcFCsLOW_LGRJhC5fl1c_gwMLxUox8cuIFkMOoW2MOe2TrTX7UiIS2VSsnlNgkK6VzzU7E3DXfqzk36p04tRhMciqnni1mXB2_Hg82pgIIsye9PHkrHka4IW5L3BMrjF3mWGZSiAAAA&shmds=v1_AdeF8KhBLQsUJ4o8-1azHHaM7jB0N9zdKZR5wD0oYn6U2so_VQ&shem=damc&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=h3X8UMijjNxLFUX2AAAAAA%3D%3D', 'extensions': ['11 days ago', 'Full-time', 'No degree mentioned'], 'detected_extensions': {'posted_at': '11 days ago', 'schedule_type': 'Full-time', 'qualifications': 'No degree mentioned'}, 'description': 'Principal Data Engineer - AND Digital (US)\n\nLocation: East Coast (NY/NJ, Atlanta, Charlotte, or South Florida)\n\nWho We Are:\n\nAND Digital is a tech company dedicated to accelerating digital delivery and closing the digital skills gap. Since 2014, we have supported organisations in building better digital products and stronger digital teams.\n\nWe believe our work should always create a remarkable impact for our clients. Through our regional offices, known as â€˜Clubs,â€™ we build strong relationships with our partners, ensuring they are prioritised by teams located nearby.\n\nThis unique model has driven success for both our clients and ourselves, reflected in our remarkable organic growth since 2014. Today, we are over 1,300 people strong, with Clubs across the UK, Europe, and the USAâ€”and plans for global expansion in the coming years.\n\nJoin us and help fulfil our mission to close the worldâ€™s digital skills gap.\n\nPrincipal Data Engineer\n\nAND Digital is seeking a Principal Data Engineer to lead data strategy, platform architecture, and client delivery across its fast-scaling US business. This is a senior, client-facing role that blends hands-on technical depth in Snowflake with consulting delivery, helping organisations modernise their data ecosystems to power AI, analytics, and digital transformation.\n\nYouâ€™ll work closely with the Solutions Director of Data & AI to ensure data architectures are designed with AI-readiness in mind - enabling clients to unlock value from their data through scalable, cloud-based platforms. Clients typically span Financial Services, Travel, and Technology, with projects centred on data modernisation, automation, and actionable intelligence.\n\nKey Responsibilities\n\nSnowflake Strategy & Architecture\nâ€¢ Define and own the Snowflake architecture strategy for US clients - from ingestion to transformation and modelling.\nâ€¢ Lead end-to-end design and implementation of Snowflake-based data platforms, ensuring scalability, performance, and security.\nâ€¢ Establish best practices for schema design, query optimisation, and performance tuning within Snowflake.\nâ€¢ Guide clients on Snowflake feature adoption (e.g. Snowpark, Streams, Tasks, Data Sharing, and Governance).\nâ€¢ Ensure platforms are optimised for integration with downstream AI, analytics, and digital products.\n\nData Engineering & Delivery Leadership\nâ€¢ Lead client-facing data engineering and strategy engagements, ensuring technical excellence and commercial success.\nâ€¢ Oversee development of data pipelines and transformations feeding Snowflake environments.\nâ€¢ Promote consistency in design patterns, CI/CD, and version control across teams and projects.\nâ€¢ Drive operational excellence through measurable engineering standards and quality frameworks.\n\nClient Engagement & Consulting\nâ€¢ Work directly with senior client stakeholders to define data modernisation roadmaps and business outcomes.\nâ€¢ Collaborate closely with AI, Product, and Cloud leads to align Snowflake data foundations with broader solution delivery.\nâ€¢ Represent the data practice in proposals, workshops, and technical discovery sessions.\n\nLeadership, Capability & Growth\nâ€¢ Mentor and develop data engineers and architects across US and nearshore teams.\nâ€¢ Contribute to building AND Digitalâ€™s Snowflake delivery capability - frameworks, accelerators, and training.\nâ€¢ Support business development and pre-sales conversations with Snowflake-aligned solution expertise (partner-sales alliances approaches put into practice)\n\nKey Evaluation Criteria\nâ€¢ Proven experience designing and delivering Snowflake data platforms at scale.\nâ€¢ Deep technical fluency in Snowflake architecture, data modelling, and SQL optimisation.\nâ€¢ Strong understanding of data engineering best practices - ingestion, orchestration, transformation, and governance.\nâ€¢ Demonstrated success in consulting or delivery-led environments, balancing technical rigour with client outcomes.\nâ€¢ Experience leading cross-functional teams (data, AI, analytics) across multiple geographies.\nâ€¢ Excellent communicator â€” credible with both technical and business stakeholders.\nâ€¢ Minimum 2+ years in a leadership capacity within data or platform engineering.\n\nExperience & Skills\nâ€¢ 8-12 years in data engineering or architecture, with at least 3 years focused on Snowflake.\nâ€¢ Technical agnostic expertise, such as SQL, Python, and data orchestration tools (Airflow, dbt, or similar).\nâ€¢ Understanding of how Snowflake underpins AI and analytics initiatives.\nâ€¢ Experience managing distributed or nearshore data delivery teams.\nâ€¢ Excellent problem-solving and client-facing skills.\nâ€¢ Must reside on the East Coast (NY/NJ, Atlanta, or South Florida).\n\nWhy join AND Digital?\n\nWe have three values: wonder, share, and delight. These values inform how we work with clients, and our culture: what it feels like to work for AND. We believe collaboration, ambition, curiosity and fun can drive innovation by creating a better environment for problem-solving.\n\nBy joining AND, weâ€™ll provide:\nâ€¢ Opportunities to work on projects with big clients and the chance to produce meaningful work that makes a difference to peopleâ€™s lives.\nâ€¢ A â€œBlended Workingâ€ model, meaning you will be able to work in a range of locations from; your home, in your clubhouse, on a client, as well as just a change of scenery.\nâ€¢ A dedicated career scrum team, designed to help you reach your career goals and develop the skills you need to be your best self.\nâ€¢ An annual budget for training and upskilling, including allocated days off so you donâ€™t have to study in your own free time.\nâ€¢ Monthly and quarterly team socials - on us - ranging from after work drinks, to driving experience days with your fellow club members.\nâ€¢ A safe environment for you to be yourself and challenge yourself.\n\nEqual Opportunities Statement\n\nAt AND Digital we embrace diversity and are committed to equal opportunities. We are actively recruiting for a diverse and inclusive workforce so want to ensure we do everything we can to support your application.\n\nWe want you to feel safe and empowered to let us know if you require any adjustments to be made to your application or interview process so please speak to our recruitment team.', 'job_highlights': [{'title': 'Qualifications', 'items': ['Proven experience designing and delivering Snowflake data platforms at scale', 'Deep technical fluency in Snowflake architecture, data modelling, and SQL optimisation', 'Strong understanding of data engineering best practices - ingestion, orchestration, transformation, and governance', 'Demonstrated success in consulting or delivery-led environments, balancing technical rigour with client outcomes', 'Experience leading cross-functional teams (data, AI, analytics) across multiple geographies', 'Excellent communicator â€” credible with both technical and business stakeholders', 'Minimum 2+ years in a leadership capacity within data or platform engineering', 'Experience & Skills', '8-12 years in data engineering or architecture, with at least 3 years focused on Snowflake', 'Technical agnostic expertise, such as SQL, Python, and data orchestration tools (Airflow, dbt, or similar)', 'Understanding of how Snowflake underpins AI and analytics initiatives', 'Experience managing distributed or nearshore data delivery teams', 'Excellent problem-solving and client-facing skills', 'Must reside on the East Coast (NY/NJ, Atlanta, or South Florida)']}, {'title': 'Benefits', 'items': ['Opportunities to work on projects with big clients and the chance to produce meaningful work that makes a difference to peopleâ€™s lives', 'A â€œBlended Workingâ€ model, meaning you will be able to work in a range of locations from; your home, in your clubhouse, on a client, as well as just a change of scenery', 'A dedicated career scrum team, designed to help you reach your career goals and develop the skills you need to be your best self', 'An annual budget for training and upskilling, including allocated days off so you donâ€™t have to study in your own free time', 'Monthly and quarterly team socials - on us - ranging from after work drinks, to driving experience days with your fellow club members', 'A safe environment for you to be yourself and challenge yourself']}, {'title': 'Responsibilities', 'items': ['This is a senior, client-facing role that blends hands-on technical depth in Snowflake with consulting delivery, helping organisations modernise their data ecosystems to power AI, analytics, and digital transformation', 'Youâ€™ll work closely with the Solutions Director of Data & AI to ensure data architectures are designed with AI-readiness in mind - enabling clients to unlock value from their data through scalable, cloud-based platforms', 'Clients typically span Financial Services, Travel, and Technology, with projects centred on data modernisation, automation, and actionable intelligence', 'Define and own the Snowflake architecture strategy for US clients - from ingestion to transformation and modelling', 'Lead end-to-end design and implementation of Snowflake-based data platforms, ensuring scalability, performance, and security', 'Establish best practices for schema design, query optimisation, and performance tuning within Snowflake', 'Guide clients on Snowflake feature adoption (e.g. Snowpark, Streams, Tasks, Data Sharing, and Governance)', 'Ensure platforms are optimised for integration with downstream AI, analytics, and digital products', 'Data Engineering & Delivery Leadership', 'Lead client-facing data engineering and strategy engagements, ensuring technical excellence and commercial success', 'Oversee development of data pipelines and transformations feeding Snowflake environments', 'Promote consistency in design patterns, CI/CD, and version control across teams and projects', 'Drive operational excellence through measurable engineering standards and quality frameworks', 'Client Engagement & Consulting', 'Work directly with senior client stakeholders to define data modernisation roadmaps and business outcomes', 'Collaborate closely with AI, Product, and Cloud leads to align Snowflake data foundations with broader solution delivery', 'Represent the data practice in proposals, workshops, and technical discovery sessions', 'Mentor and develop data engineers and architects across US and nearshore teams', 'Contribute to building AND Digitalâ€™s Snowflake delivery capability - frameworks, accelerators, and training', 'Support business development and pre-sales conversations with Snowflake-aligned solution expertise (partner-sales alliances approaches put into practice)']}], 'apply_options': [{'title': 'SimplyHired', 'link': 'https://www.simplyhired.com/job/3APZuDsMblL34HzJt5_BTxU0i0aGIsmVyfRuJRn3SyfAHS7jukD5Ag?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Recruit.net', 'link': 'https://www.recruit.net/job/data-engineer-digital-us-jobs/F819FBD2A5774B91?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Expertini', 'link': 'https://us.expertini.com/jobs/job/principal-data-engineer-and-digital-us--florida-city-florid-and-digital-229960759/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJQcmluY2lwYWwgRGF0YSBFbmdpbmVlciAtIFNub3dmbGFrZSAoVVMpIiwiY29tcGFueV9uYW1lIjoiQU5EIERpZ2l0YWwiLCJhZGRyZXNzX2NpdHkiOiJGbG9yaWRhIENpdHksIEZMIiwiaHRpZG9jaWQiOiJoM1g4VU1pampOeExGVVgyQUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': 'Remote Data Engineer', 'company_name': 'Shuvel', 'location': 'Oakton, VA', 'via': 'Jooble', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=RyJPTPtdNSCAa1QXAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEPQsCMQwAUFxvdnLK7EcrgotOgiK4CAquR-4IbbWXlCbKjf508Q2v-U6axY0GMYIjGsKJQ2KiCiu4SAdKWPsIwnAWCZlm-2hWdOe9anZBDS31rpfBC1Mno39Kp_9ajVipZDRqN9v16AqH-fQe3x_KkBiu-DLhJTwOPy-pg56DAAAA&shmds=v1_AdeF8Ki_pLsDxIan3ngH96uBI6cYOx_5qeRsdsUXYmBa0Uerow&shem=damc&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=RyJPTPtdNSCAa1QXAAAAAA%3D%3D', 'extensions': ['5 days ago', 'Full-time', 'No degree mentioned'], 'detected_extensions': {'posted_at': '5 days ago', 'schedule_type': 'Full-time', 'qualifications': 'No degree mentioned'}, 'description': 'Required Skills and Experience:\nâ€¢ Google Cloud Platform: Experience with GCP services, particularly Cloud Functions, Cloud SQL, BigQuery, and database management.\nâ€¢ Python Scripting: Capable of automating tasks or developing utilities using Python.\nâ€¢ ETL Pipelines: Comfortable with data cleaning, transformation, and integration workflows.\nâ€¢ Ability to design, create, and manage APIs on GCP.\nâ€¢ Desired Skills: Geospatial Technologies Mapping Libraries: Proficiency with Leaflet and/or Esri technologies.\nâ€¢ React: Proficiency in React and JavaScript.\nâ€¢ Frontend: Familiarity with Material-UI (MUI) or U.S. Web Design System (USWDS).', 'job_highlights': [{'title': 'Qualifications', 'items': ['Google Cloud Platform: Experience with GCP services, particularly Cloud Functions, Cloud SQL, BigQuery, and database management', 'Python Scripting: Capable of automating tasks or developing utilities using Python', 'ETL Pipelines: Comfortable with data cleaning, transformation, and integration workflows', 'Ability to design, create, and manage APIs on GCP', 'React: Proficiency in React and JavaScript', 'Frontend: Familiarity with Material-UI (MUI) or U.S. Web Design System (USWDS)']}], 'apply_options': [{'title': 'Jooble', 'link': 'https://jooble.org/jdp/3838348580443472925?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobrapido', 'link': 'https://us.jobrapido.com/jobpreview/995708448689618944?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJSZW1vdGUgRGF0YSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IlNodXZlbCIsImFkZHJlc3NfY2l0eSI6Ik9ha3RvbiwgVkEiLCJodGlkb2NpZCI6IlJ5SlBUUHRkTlNDQWExUVhBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Senior Lead Data Engineer (Python, EMR, Databricks, AWS) (m/w/d)', 'company_name': 'Capital One', 'location': 'Charlottesville, VA', 'via': 'SaluteMyJob', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=OPyOc58p-anHwJnHAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_x3KuwrCQBBAUWz9BKvBSiVmRbDRSlQEURQFLcMkDsnqOhN2Bh-_5tf5aG5zbvPdaJ4OxF4irAnPMEdDWHDpmShCZ_eySjiBxWaf_C2PvrhqAtPToQudm3u4cxf6sJIclDAWFQjDUqQM1JpUZrWOnVMNaamG5ou0kJsTplye7iK5_pJphZHqgEbZcDR4pjWXvfYMa28YYMsEnmH2fYKYkd59CJTAcfoBbtT8Y70AAAA&shmds=v1_AdeF8KgMGBW6Jlq_kJlPRtp0U7hgab4PzbCvpH9_Why6TXisGg&shem=damc&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=OPyOc58p-anHwJnHAAAAAA%3D%3D', 'extensions': ['5 days ago', 'Full-time and Part-time', 'Health insurance'], 'detected_extensions': {'posted_at': '5 days ago', 'schedule_type': 'Full-time and Part-time', 'health_insurance': True}, 'description': "Senior Lead Data Engineer (Python, EMR, Databricks, AWS)\n\nDo you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative,inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineerswho are passionate about marrying data with emerging technologies. As a Capital One Senior Lead Data Engineer, you'll have the opportunity to be on the forefront of driving a major transformation within Capital One.\n\nWhat You'll Do:\nâ€¢ Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies\nâ€¢ Lead a team of developers with deep experience in machine learning, distributed microservices, and full stack systems\nâ€¢ Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake\nâ€¢ Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community\nâ€¢ Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment\nâ€¢ Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance\n\nBasic Qualifications:\nâ€¢ Bachelor's Degree\nâ€¢ At least 6 years of experience in application development (Internship experience does not apply)\nâ€¢ At least 2 years of experience in big data technologies\nâ€¢ At least 1 year experience with cloud computing (AWS, Microsoft Azure, Google Cloud)\n\nPreferred Qualifications:\nâ€¢ Master's Degree\nâ€¢ 9+ years of experience in application development including Python, SQL, Scala, or Java\nâ€¢ 4+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)\nâ€¢ 5+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)\nâ€¢ 4+ year experience working on real-time data and streaming applications\nâ€¢ 4+ years of experience with NoSQL implementation (Mongo, Cassandra)\nâ€¢ 4+ years of data warehousing experience (Redshift or Snowflake)\nâ€¢ 4+ years of experience with UNIX/Linux including basic commands and shell scripting\nâ€¢ 2+ years of experience with Agile engineering practices\n\nCapital One will consider sponsoring a new qualified applicant for employment authorization for this position.\n\nThe minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.Cambridge, MA: $225,400 - $257,200 for Sr. Lead Data Engineer\n\nMcLean, VA: $225,400 - $257,200 for Sr. Lead Data Engineer\n\nPlano, TX: $204,900 - $233,800 for Sr. Lead Data Engineer\n\nRichmond, VA: $204,900 - $233,800 for Sr. Lead Data Engineer\n\nCandidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate's offer letter.This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n\nCapital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at theCapital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer (EOE, including disability/vet) committed to non-discrimination in compliance with applicable federal, state, and local laws. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City's Fair Chance Act; Philadelphia's Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n\nIf you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.\n\nFor technical support or questions about Capital One's recruiting process, please send an email to Careers@\n\nCapital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.\n\nCapital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).", 'job_highlights': [{'title': 'Qualifications', 'items': ["Bachelor's Degree", 'At least 6 years of experience in application development (Internship experience does not apply)', 'At least 2 years of experience in big data technologies', 'At least 1 year experience with cloud computing (AWS, Microsoft Azure, Google Cloud)']}, {'title': 'Benefits', 'items': ['Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.Cambridge, MA: $225,400 - $257,200 for Sr', "Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate's offer letter", 'This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI)', 'Incentives could be discretionary or non discretionary depending on the plan', 'Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being']}, {'title': 'Responsibilities', 'items': ['Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies', 'Lead a team of developers with deep experience in machine learning, distributed microservices, and full stack systems', 'Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake', 'Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community', 'Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment', 'Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance']}], 'apply_options': [{'title': 'SaluteMyJob', 'link': 'https://salutemyjob.com/jobs/senior-lead-data-engineer-python-emr-databricks-aws-m-w-d-charlottesville-virginia/2457058630-2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobs For Stevenage Fans', 'link': 'https://jobs.stevenagefc.com/jobs/senior-lead-data-engineer-python-emr-databricks-aws-m-w-d-charlottesville-virginia/2457058630-2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'KGET Jobs', 'link': 'https://jobs.kget.com/jobs/senior-lead-data-engineer-python-emr-databricks-aws-m-w-d-charlottesville-virginia/2457058630-2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'KSNT Jobs', 'link': 'https://jobs.ksnt.com/jobs/senior-lead-data-engineer-python-emr-databricks-aws-m-w-d-charlottesville-virginia/2457058630-2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'KHON2 Jobs', 'link': 'https://jobs.khon2.com/jobs/senior-lead-data-engineer-python-emr-databricks-aws-m-w-d-charlottesville-virginia/2457058630-2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Aspire Media Group Jobs', 'link': 'https://jobs.aspiremediagroup.net/jobs/senior-lead-data-engineer-python-emr-databricks-aws-m-w-d-charlottesville-virginia/2457058630-2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'FOX8 Jobs', 'link': 'https://jobs.fox8.com/jobs/senior-lead-data-engineer-python-emr-databricks-aws-m-w-d-charlottesville-virginia/2457058630-2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'WDTN Jobs', 'link': 'https://jobs.wdtn.com/jobs/senior-lead-data-engineer-python-emr-databricks-aws-m-w-d-charlottesville-virginia/2457058630-2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTZW5pb3IgTGVhZCBEYXRhIEVuZ2luZWVyIChQeXRob24sIEVNUiwgRGF0YWJyaWNrcywgQVdTKSAobS93L2QpIiwiY29tcGFueV9uYW1lIjoiQ2FwaXRhbCBPbmUiLCJhZGRyZXNzX2NpdHkiOiJDaGFybG90dGVzdmlsbGUsIFZBIiwiaHRpZG9jaWQiOiJPUHlPYzU4cC1hbkh3Sm5IQUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': '#20499 - SAP Data Engineer', 'company_name': 'Qualitest', 'location': 'Atlanta, GA', 'via': 'LinkedIn', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=oEnCI91EWo6m9ctiAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXIsQrCQAwAUFz7BeIUcBPt1WKH6lRQCk6KH1DSEu5OzqQ0EfoNfrW6vOFln0W2X5fFoa5hB4_mBmc0hAv7yETT767SgxJOQwBhaEV8otUpmI16dE415V4NLQ75IC8nTL3M7im9_uk04ERjQqOurIo5H9lvlvc3pmikBpGhsYRsuIW2-QKoEKCtjQAAAA&shmds=v1_AdeF8Kg0flt5VFt5BS-DOieSyqGPF4_jkcx28innMWVzM2sd0Q&shem=damc&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=oEnCI91EWo6m9ctiAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965bbb6a867d1069fe7ba/images/5282d117823d97519d1145babe814b242aea1de4f9faee8014fad773e4711242.jpeg', 'extensions': ['5 days ago', 'Full-time', 'Health insurance', 'Paid time off'], 'detected_extensions': {'posted_at': '5 days ago', 'schedule_type': 'Full-time', 'health_insurance': True, 'paid_time_off': True}, 'description': 'Job description:\n\nQualitest is seeking a SAP Data Engineer to join our Data Center of Excellence (COE) within the Enterprise division. In this role, you will design, develop, and maintain scalable data pipelines and integration frameworks that connect SAP and non-SAP ecosystems, enabling enterprise-wide insights and automation.\n\nAs a key contributor to the enterprise data strategy, youâ€™ll ensure data quality, consistency, and accessibility across systems while working closely with AI/ML, analytics, and business teams. You will help shape the data foundation that powers predictive insights, intelligent automation, and data-driven decision-making.\n\nThis position offers an exciting opportunity for an engineer who thrives at the intersection of SAP technologies, cloud data engineering, and advanced analytics â€” building the infrastructure that drives digital transformation and business intelligence.\n\nLocation: Remote â€“ U.S. (CST Preferred)\nEmployment Type: Full-time, Permanent\n\nCore Focus Areas\nâ€¢ SAP Data Integration Expertise\nâ€¢ Hands-on experience with SAP data tools: BODS, SLT, SDI, CPI-DS\nâ€¢ Integration of SAP platforms (S/4HANA, BW/4HANA, Ariba) with enterprise and cloud systems\nâ€¢ ETL & Data Modeling\nâ€¢ Proven ability to design, optimize, and automate ETL/ELT workflows\nâ€¢ Experience developing data models and architectures to support analytics and AI/ML initiatives\nâ€¢ Cloud & Analytics Integration\nâ€¢ Practical experience with Azure, AWS, or GCP data services\nâ€¢ Familiarity with SAP Analytics Cloud (SAC), Power BI, or similar BI/reporting tools\n\nKey Responsibilities\nâ€¢ Design, build, and maintain end-to-end data pipelines integrating SAP (S/4HANA, BW/4HANA, Ariba) with enterprise and cloud platforms.\nâ€¢ Develop and optimize ETL/ELT workflows using SAP BODS, SLT, SDI, CPI-DS, and related integration tools.\nâ€¢ Partner with AI/ML teams to enable data-driven automation, predictive analytics, and intelligent process optimization.\nâ€¢ Collaborate with business and analytics stakeholders to define efficient data models and semantic layers for reporting.\nâ€¢ Support SAP analytics initiatives using SAC, Power BI, and other visualization tools.\nâ€¢ Implement and enforce data governance, validation, and compliance standards (HIPAA, audit, security).\nâ€¢ Design and optimize data architectures for scalability, performance, and cloud readiness.\nâ€¢ Ensure end-to-end data lineage, integrity, and traceability across SAP and non-SAP systems.\nâ€¢ Diagnose and resolve complex data integration and performance issues.\nâ€¢ Maintain technical documentation, reusable frameworks, and process standards for consistency and knowledge sharing.\nâ€¢ Stay current with emerging SAP, data engineering, and AI technologies to drive innovation and best practices.\n\nQualifications\nâ€¢ Bachelorâ€™s degree in Computer Science, Information Systems, Data Engineering, or a related field (or equivalent work experience).\nâ€¢ 5â€“8 years of experience in SAP Data Engineering, ETL Development, or Analytics Enablement.\nâ€¢ Strong expertise in SQL, data modeling, and integration of SAP HANA and BW/4HANA environments.\nâ€¢ Proven proficiency with SAP data integration tools (BODS, SLT, SDI, CPI-DS).\nâ€¢ Experience with SAP Analytics Cloud (SAC), Power BI, or equivalent BI tools.\nâ€¢ Exposure to AI/ML data pipelines or collaboration with data science teams is a plus.\nâ€¢ Working knowledge of Python, Spark, or cloud data platforms (Azure, AWS, GCP).\nâ€¢ Understanding of data governance, master data management (MDM), and data lifecycle management principles.\nâ€¢ Excellent analytical thinking, communication, and problem-solving skills with a strong sense of ownership and collaboration.\n\nWe offer:\n\nQualitest is seeking a SAP Data Engineer to join our Data Center of Excellence (COE) within the Enterprise division. In this role, you will design, develop, and maintain scalable data pipelines and integration frameworks that connect SAP and non-SAP ecosystems, enabling enterprise-wide insights and automation.\n\nAs a key contributor to the enterprise data strategy, youâ€™ll ensure data quality, consistency, and accessibility across systems while working closely with AI/ML, analytics, and business teams. You will help shape the data foundation that powers predictive insights, intelligent automation, and data-driven decision-making.\n\nThis position offers an exciting opportunity for an engineer who thrives at the intersection of SAP technologies, cloud data engineering, and advanced analytics â€” building the infrastructure that drives digital transformation and business intelligence.\n\nLocation: Remote â€“ U.S. (CST Preferred)\nEmployment Type: Full-time, Permanent\n\nCore Focus Areas\nâ€¢ SAP Data Integration Expertise\nâ€¢ Hands-on experience with SAP data tools: BODS, SLT, SDI, CPI-DS\nâ€¢ Integration of SAP platforms (S/4HANA, BW/4HANA, Ariba) with enterprise and cloud systems\nâ€¢ ETL & Data Modeling\nâ€¢ Proven ability to design, optimize, and automate ETL/ELT workflows\nâ€¢ Experience developing data models and architectures to support analytics and AI/ML initiatives\nâ€¢ Cloud & Analytics Integration\nâ€¢ Practical experience with Azure, AWS, or GCP data services\nâ€¢ Familiarity with SAP Analytics Cloud (SAC), Power BI, or similar BI/reporting tools\n\nKey Responsibilities\nâ€¢ Design, build, and maintain end-to-end data pipelines integrating SAP (S/4HANA, BW/4HANA, Ariba) with enterprise and cloud platforms.\nâ€¢ Develop and optimize ETL/ELT workflows using SAP BODS, SLT, SDI, CPI-DS, and related integration tools.\nâ€¢ Partner with AI/ML teams to enable data-driven automation, predictive analytics, and intelligent process optimization.\nâ€¢ Collaborate with business and analytics stakeholders to define efficient data models and semantic layers for reporting.\nâ€¢ Support SAP analytics initiatives using SAC, Power BI, and other visualization tools.\nâ€¢ Implement and enforce data governance, validation, and compliance standards (HIPAA, audit, security).\nâ€¢ Design and optimize data architectures for scalability, performance, and cloud readiness.\nâ€¢ Ensure end-to-end data lineage, integrity, and traceability across SAP and non-SAP systems.\nâ€¢ Diagnose and resolve complex data integration and performance issues.\nâ€¢ Maintain technical documentation, reusable frameworks, and process standards for consistency and knowledge sharing.\nâ€¢ Stay current with emerging SAP, data engineering, and AI technologies to drive innovation and best practices.\n\nQualifications\nâ€¢ Bachelorâ€™s degree in Computer Science, Information Systems, Data Engineering, or a related field (or equivalent work experience).\nâ€¢ 5â€“8 years of experience in SAP Data Engineering, ETL Development, or Analytics Enablement.\nâ€¢ Strong expertise in SQL, data modeling, and integration of SAP HANA and BW/4HANA environments.\nâ€¢ Proven proficiency with SAP data integration tools (BODS, SLT, SDI, CPI-DS).\nâ€¢ Experience with SAP Analytics Cloud (SAC), Power BI, or equivalent BI tools.\nâ€¢ Exposure to AI/ML data pipelines or collaboration with data science teams is a plus.\nâ€¢ Working knowledge of Python, Spark, or cloud data platforms (Azure, AWS, GCP).\nâ€¢ Understanding of data governance, master data management (MDM), and data lifecycle management principles.\nâ€¢ Excellent analytical thinking, communication, and problem-solving skills with a strong sense of ownership and collaboration.\n\nBenefits\nWhy QualiTest?\nâ€¢ Be a part of a company who strives to support for diversity and inclusion in the workplace â€“ we are one, we are many at Qualitest. Celebrate culture, share knowledge with engineers from around the globe, and inspire each other through our differences.\u202fWe have more than 40% women and around 120 different nationalities.\nâ€¢ Local and global opportunities â€“ we offer you internal rotation and international mobility opportunities to grow your career.\nâ€¢ Clear view of your career and progression with the company â€“ Qualitest is growing massively (since 2021 â€“ tripled our employees base â€“ we now have more than 8,000 engineers) and giving you the opportunity to grow with us.\nâ€¢ Work hard and play harder with our flexible and casual culture. Take a break from work and join an employee event, or\u202fenjoy the amenities and games provided from one of our Employees Centers.Save your earnings and prepare for your future by enrolling in our 401k plan where Qualitest will match your contributions accelerating your savings plan.\nâ€¢ Take care of health with enrollment into one of our competitive healthcare benefits. Qualitest will match towards your HSA if you choose to participate.\nâ€¢ Never stop experimenting and learning with\u202fQCraft â€“ our Learning & Development platform: 50,000+ courses, 300+ virtual labs, mentorship and leadership programs, professional tribes, sponsored certifications, and much more.\nâ€¢ Stay active and get rewarded with our Corporate Wellness Program. We pay your Gym membership and giving you opportunities to Earn additional vacation times for attendance the gym!\nâ€¢ Earn bonuses via our Client Referral and Employee Referral Programâ€™s. Refer and earn â€“ tap your network for net-worth.\nâ€¢ We recognize our employees work via our Qudos platform - You can earn bonuses and spot awards by celebrating your and your peersâ€™ achievements.\nâ€¢ Planning a vacation? Looking for car insurance? Get access to Qualitest Employee Perks for discounts on anything from travel to electronics. With so many offerings the savings are endless!\nâ€¢ A Competitive pay, the salary range for the role is $110,000 - $120,000.\nâ€¢ Intrigued to find more about us?\nâ€¢ Visit our website at https://www.qualitestgroup.com/\nâ€¢ If you like what you have read, send us your resume and letâ€™s start talking!', 'job_highlights': [{'title': 'Qualifications', 'items': ['Hands-on experience with SAP data tools: BODS, SLT, SDI, CPI-DS', 'Practical experience with Azure, AWS, or GCP data services', 'Familiarity with SAP Analytics Cloud (SAC), Power BI, or similar BI/reporting tools', 'Bachelorâ€™s degree in Computer Science, Information Systems, Data Engineering, or a related field (or equivalent work experience)', '5â€“8 years of experience in SAP Data Engineering, ETL Development, or Analytics Enablement', 'Strong expertise in SQL, data modeling, and integration of SAP HANA and BW/4HANA environments', 'Proven proficiency with SAP data integration tools (BODS, SLT, SDI, CPI-DS)', 'Experience with SAP Analytics Cloud (SAC), Power BI, or equivalent BI tools', 'Working knowledge of Python, Spark, or cloud data platforms (Azure, AWS, GCP)', 'Understanding of data governance, master data management (MDM), and data lifecycle management principles', 'Excellent analytical thinking, communication, and problem-solving skills with a strong sense of ownership and collaboration', 'SAP Data Integration Expertise', 'Hands-on experience with SAP data tools: BODS, SLT, SDI, CPI-DS', 'Integration of SAP platforms (S/4HANA, BW/4HANA, Ariba) with enterprise and cloud systems', 'Proven ability to design, optimize, and automate ETL/ELT workflows', 'Cloud & Analytics Integration', 'Practical experience with Azure, AWS, or GCP data services', 'Familiarity with SAP Analytics Cloud (SAC), Power BI, or similar BI/reporting tools', 'Bachelorâ€™s degree in Computer Science, Information Systems, Data Engineering, or a related field (or equivalent work experience)', '5â€“8 years of experience in SAP Data Engineering, ETL Development, or Analytics Enablement', 'Strong expertise in SQL, data modeling, and integration of SAP HANA and BW/4HANA environments', 'Proven proficiency with SAP data integration tools (BODS, SLT, SDI, CPI-DS)', 'Experience with SAP Analytics Cloud (SAC), Power BI, or equivalent BI tools', 'Working knowledge of Python, Spark, or cloud data platforms (Azure, AWS, GCP)', 'Understanding of data governance, master data management (MDM), and data lifecycle management principles', 'Excellent analytical thinking, communication, and problem-solving skills with a strong sense of ownership and collaboration']}, {'title': 'Benefits', 'items': ['Work hard and play harder with our flexible and casual culture', 'Take a break from work and join an employee event, or\u202fenjoy the amenities and games provided from one of our Employees Centers', 'Save your earnings and prepare for your future by enrolling in our 401k plan where Qualitest will match your contributions accelerating your savings plan', 'Take care of health with enrollment into one of our competitive healthcare benefits', 'Never stop experimenting and learning with\u202fQCraft â€“ our Learning & Development platform: 50,000+ courses, 300+ virtual labs, mentorship and leadership programs, professional tribes, sponsored certifications, and much more', 'Stay active and get rewarded with our Corporate Wellness Program', 'We pay your Gym membership and giving you opportunities to Earn additional vacation times for attendance the gym!', 'Earn bonuses via our Client Referral and Employee Referral Programâ€™s', 'Refer and earn â€“ tap your network for net-worth', 'We recognize our employees work via our Qudos platform - You can earn bonuses and spot awards by celebrating your and your peersâ€™ achievements', 'Planning a vacation?', 'Get access to Qualitest Employee Perks for discounts on anything from travel to electronics', 'With so many offerings the savings are endless!', 'A Competitive pay, the salary range for the role is $110,000 - $120,000']}, {'title': 'Responsibilities', 'items': ['In this role, you will design, develop, and maintain scalable data pipelines and integration frameworks that connect SAP and non-SAP ecosystems, enabling enterprise-wide insights and automation', 'As a key contributor to the enterprise data strategy, youâ€™ll ensure data quality, consistency, and accessibility across systems while working closely with AI/ML, analytics, and business teams', 'You will help shape the data foundation that powers predictive insights, intelligent automation, and data-driven decision-making', 'SAP Data Integration Expertise', 'Integration of SAP platforms (S/4HANA, BW/4HANA, Ariba) with enterprise and cloud systems', 'ETL & Data Modeling', 'Proven ability to design, optimize, and automate ETL/ELT workflows', 'Experience developing data models and architectures to support analytics and AI/ML initiatives', 'Cloud & Analytics Integration', 'Design, build, and maintain end-to-end data pipelines integrating SAP (S/4HANA, BW/4HANA, Ariba) with enterprise and cloud platforms', 'Develop and optimize ETL/ELT workflows using SAP BODS, SLT, SDI, CPI-DS, and related integration tools', 'Partner with AI/ML teams to enable data-driven automation, predictive analytics, and intelligent process optimization', 'Collaborate with business and analytics stakeholders to define efficient data models and semantic layers for reporting', 'Support SAP analytics initiatives using SAC, Power BI, and other visualization tools', 'Implement and enforce data governance, validation, and compliance standards (HIPAA, audit, security)', 'Design and optimize data architectures for scalability, performance, and cloud readiness', 'Ensure end-to-end data lineage, integrity, and traceability across SAP and non-SAP systems', 'Diagnose and resolve complex data integration and performance issues', 'Maintain technical documentation, reusable frameworks, and process standards for consistency and knowledge sharing', 'Stay current with emerging SAP, data engineering, and AI technologies to drive innovation and best practices', 'In this role, you will design, develop, and maintain scalable data pipelines and integration frameworks that connect SAP and non-SAP ecosystems, enabling enterprise-wide insights and automation', 'As a key contributor to the enterprise data strategy, youâ€™ll ensure data quality, consistency, and accessibility across systems while working closely with AI/ML, analytics, and business teams', 'You will help shape the data foundation that powers predictive insights, intelligent automation, and data-driven decision-making', 'ETL & Data Modeling', 'Experience developing data models and architectures to support analytics and AI/ML initiatives', 'Design, build, and maintain end-to-end data pipelines integrating SAP (S/4HANA, BW/4HANA, Ariba) with enterprise and cloud platforms', 'Develop and optimize ETL/ELT workflows using SAP BODS, SLT, SDI, CPI-DS, and related integration tools', 'Partner with AI/ML teams to enable data-driven automation, predictive analytics, and intelligent process optimization', 'Collaborate with business and analytics stakeholders to define efficient data models and semantic layers for reporting', 'Support SAP analytics initiatives using SAC, Power BI, and other visualization tools', 'Implement and enforce data governance, validation, and compliance standards (HIPAA, audit, security)', 'Design and optimize data architectures for scalability, performance, and cloud readiness', 'Ensure end-to-end data lineage, integrity, and traceability across SAP and non-SAP systems', 'Diagnose and resolve complex data integration and performance issues', 'Maintain technical documentation, reusable frameworks, and process standards for consistency and knowledge sharing', 'Stay current with emerging SAP, data engineering, and AI technologies to drive innovation and best practices']}], 'apply_options': [{'title': 'LinkedIn', 'link': 'https://www.linkedin.com/jobs/view/%2320499-sap-data-engineer-at-qualitest-4321822011?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'BeBee', 'link': 'https://us.bebee.com/job/d3841ad5c20fb9f5eb7e0fefe86a9d03?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiIjMjA0OTkgLSBTQVAgRGF0YSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IlF1YWxpdGVzdCIsImFkZHJlc3NfY2l0eSI6IkF0bGFudGEsIEdBIiwiaHRpZG9jaWQiOiJvRW5DSTkxRVdvNm05Y3RpQUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': 'Data Engineer III', 'company_name': 'eStaffing', 'location': 'Ohio', 'via': 'WhatJobs', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=16TqTNkl5l-c7nvtAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQoCMQwA0P02V6eMItiK4KKrInVx8AOO9Mi1lV5SmgyHXy--4Q3fYXdDQ7hzKkzUIYQAB3hKBCXsUwZheIikSttrNmt68V61uqSGViY3yeKFKcrqPxL136gZO7WKRuPpfFxd47Tf0NtwngsnKAyvXOQHpSv7iHwAAAA&shmds=v1_AdeF8KiVOwNNtQELf50SveSbDv22ONYsh2R6QxHHTgMpBI3Z_g&shem=damc&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=16TqTNkl5l-c7nvtAAAAAA%3D%3D', 'extensions': ['3 days ago', 'Full-time'], 'detected_extensions': {'posted_at': '3 days ago', 'schedule_type': 'Full-time'}, 'description': "Duration: 12/31/25, should extend out long term.\nVisa: USC/GC\nJOB DESCRIPTION\nWe are seeking an experienced Data Engineer III. The ideal candidate will be responsible for working with business analysts, data engineers and upstream teams to understand impacts to data sources. Take the requirements and update/build ETL data pipelines using Datastage and DBT for ingestion into Financial Crimes applications. Perform testing and ensure data quality of updated data sources.\nJob Summary: Handle the design and construction of scalable management systems, ensure that all data systems meet company requirements, and also research new uses for data acquisition. Required to know and understand the ins and outs of the industry such as data mining practices, algorithms, and how data can be used.\nPrimary Responsibilities:Design, construct, install, test and maintain data management systems. Build high-performance algorithms, predictive models, and prototypes. Ensure that all systems meet the business/company requirements as well as industry practices. Integrate up-and-coming data management and software engineering technologies into existing data structures. Develop set processes for data mining, data modeling, and data production. Create custom software components and analytics applications. Research new uses for existing data. Employ an array of technological languages and tools to connect systems together. Collaborate with members of your team (eg, data architects, the IT team, data scientists) on the project's goals. Install/update disaster recovery procedures. Recommend different ways to constantly improve data reliability and quality.\nQualifications Locals are highly preferred. Open to relocation possibility from within the state of Ohio with no assistance Technical Degree or related work experience Experience with non-relational & relational databases (SQL, MySQL, NoSQL, Hadoop, MongoDB, etc.) Experience programming and/or architecting a back-end language (Java, J2EE, etc) Business Intelligence - Data Engineering ETL DataStage Developer SQL Strong communication skills, ability to collaborate with members of your team", 'job_highlights': [{'title': 'Qualifications', 'items': ['Required to know and understand the ins and outs of the industry such as data mining practices, algorithms, and how data can be used', 'Install/update disaster recovery procedures', 'Recommend different ways to constantly improve data reliability and quality', 'Open to relocation possibility from within the state of Ohio with no assistance Technical Degree or related work experience Experience with non-relational & relational databases (SQL, MySQL, NoSQL, Hadoop, MongoDB, etc.)', 'Experience programming and/or architecting a back-end language (Java, J2EE, etc) Business Intelligence - Data Engineering ETL DataStage Developer SQL Strong communication skills, ability to collaborate with members of your team']}, {'title': 'Responsibilities', 'items': ['The ideal candidate will be responsible for working with business analysts, data engineers and upstream teams to understand impacts to data sources', 'Take the requirements and update/build ETL data pipelines using Datastage and DBT for ingestion into Financial Crimes applications', 'Perform testing and ensure data quality of updated data sources', 'Job Summary: Handle the design and construction of scalable management systems, ensure that all data systems meet company requirements, and also research new uses for data acquisition', 'Primary Responsibilities:Design, construct, install, test and maintain data management systems', 'Build high-performance algorithms, predictive models, and prototypes', 'Ensure that all systems meet the business/company requirements as well as industry practices', 'Integrate up-and-coming data management and software engineering technologies into existing data structures', 'Develop set processes for data mining, data modeling, and data production', 'Create custom software components and analytics applications', 'Research new uses for existing data', 'Employ an array of technological languages and tools to connect systems together', "Collaborate with members of your team (eg, data architects, the IT team, data scientists) on the project's goals"]}], 'apply_options': [{'title': 'WhatJobs', 'link': 'https://www.whatjobs.com/jobs/data-engineer-iii?id=2261048376&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIElJSSIsImNvbXBhbnlfbmFtZSI6ImVTdGFmZmluZyIsImFkZHJlc3NfY2l0eSI6Ik9oaW8iLCJodGlkb2NpZCI6IjE2VHFUTmtsNWwtYzdudnRBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Data Engineer II', 'company_name': 'Grainger Businesses', 'location': 'Lake Forest, IL', 'via': 'ZipRecruiter', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=3l5TZk0tAjsKlp3xAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNwQrCMAwAULzuEzzlpCC6iuBFb6KOyv5hpCN01ZqMJsK-wm92Xt71Vd9Ftb6iIdw4JiYq4D3s4CEBlLD0AwhDIxIzLc-D2agn51RzHdXQUl_38nbCFGRyTwn6p9MBC40ZjbrDcT_VI8fNqimYOM7B5aPzpEoKiaHFF8FdCqltwbc_312D5ZEAAAA&shmds=v1_AdeF8Kj_RFb8B9C7yMBA32GFK5rHiMwXEibTnXikpmVGoOhy5Q&shem=damc&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=3l5TZk0tAjsKlp3xAAAAAA%3D%3D', 'extensions': ['60.4Kâ€“101K a year', 'Full-time', 'Health insurance', 'Dental insurance', 'Paid time off'], 'detected_extensions': {'salary': '60.4Kâ€“101K a year', 'schedule_type': 'Full-time', 'health_insurance': True, 'dental_coverage': True, 'paid_time_off': True}, 'description': "Work Location Type: Hybrid\n\nReq Number 324219\n\nAbout Grainger:\n\nW.W. Grainger, Inc., is a leading broad line distributor with operations primarily in North America, Japan and the United Kingdom. At Grainger, We Keep the World Working by serving more than 4.5 million customers worldwide with products and solutions delivered through innovative technology and deep customer relationships. Known for its commitment to service and award-winning culture, the Company had 2024 revenue of $17.2 billion across its two business models. In the High-Touch Solutions segment, Grainger offers approximately 2 million maintenance, repair and operating (MRO) products and services, including technical support and inventory management. In the Endless Assortment segment,Zoro.comoffers customers access to more than 14 million products, andMonotaRO.comoffers more than 24 million products. For more information, visitwww.grainger.com.\n\nCompensation\n\nThe anticipated base pay compensation range for this position is $60,400.00 to $100,700.00.\n\nRewards and Benefits:\n\nWith benefits starting on day one, our programs provide choice and flexibility to meet team members' individual needs, including:\nâ€¢ Medical, dental, vision, and life insurance plans with coverage starting on day one of employment and 6 free sessions each year with a licensed therapist to support your emotional wellbeing.\nâ€¢ 18 paid time off (PTO) days annually for full-time employees (accrual prorated based on employment start date) and 6 company holidays per year.\nâ€¢ 6% company contribution to a 401(k) Retirement Savings Plan each pay period, no employee contribution required.\nâ€¢ Employee discounts, tuition reimbursement, student loan refinancing and free access to financial counseling, education, and tools.\nâ€¢ Maternity support programs, nursing benefits, and up to 14 weeks paid leave for birth parents and up to 4 weeks paid leave for non-birth parents.\n\nFor additional information and details regarding Grainger's benefits, please click on the link below:\n\nhttps://experience100.ehr.com/grainger/Home/Tools-Resources/Key-Resources/New-Hire\n\nThe pay range provided above is not a guarantee of compensation. The range reflects the potential base pay for this role at the time of this posting based on the job grade for this position. Individual base pay compensation will depend, in part, on factors such as geographic work location and relevant experience and skills.\n\nThe anticipated compensation range described above is subject to change and the compensation ultimately paid may be higher or lower than the range described above.\n\nGrainger reserves the right to amend, modify, or terminate its compensation and benefit programs in its sole discretion at any time, consistent with applicable law.\n\nPosition Details\n\nWe're hiring a Data Engineer II for the Supply Chain Optimization - Machine Learning Engineering team. The team's mission is to ensure the durability and scalability of machine learning and operations research models that drive impactful results across our supply chain.\n\nThis role centers on designing and developing data pipelines and products that support advanced analytics. It involves close collaboration with machine learning engineers and data scientists to set up pipelines for model development and deployment, automate workflows, and maintain the data infrastructure behind key data products.\n\nExpect to work closely with subject matter experts, architects, analysts, and other stakeholders to integrate data from a wide range of enterprise sources. The position reports to the Senior Manager, Machine Learning Engineering.\n\nThis position is not eligible for any form of sponsorship now or in the future. Individuals requiring sponsorship (e.g. OPT or H1B visa status) should not apply. Only individuals authorized to work in the United States now and for the foreseeable future will be considered for this position.\n\nYou Will\nâ€¢ Enable analytics and reporting by centralizing and integrating high quality, large, complex data sets in a performant and scalable cloud platform\nâ€¢ Identify, design, and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes\nâ€¢ Build required tools for extraction, transformation and loading of data from multiple data sources\nâ€¢ Build frameworks, standards & product features to enable self-service analytics\nâ€¢ Partner with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues\n\nYou Have\nâ€¢ Bachelor's Degree or equivalent experience in Engineering or Computer Science or Information Technology, or a related technical discipline required\nâ€¢ 1+ years of experience with Modern Data Engineering projects and practices required\nâ€¢ 1+ years of experience deploying cloud native solutions required\nâ€¢ 1+ years of experience with AWS, SQL, Python, Docker/Kubernetes, CI/CD, Git familiarity with: Snowflake, DBT, Airflow required\nâ€¢ Experience with advanced analytics and machine learning\nâ€¢ Familiarity with BI tools such as Tableau, PowerBI\n\nWe are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex (including pregnancy), national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or expression, protected veteran status or any other protected characteristic under federal, state, or local law. We are proud to be an equal opportunity workplace.\n\nWe are committed to fostering an inclusive, accessible work environment that includes both providing reasonable accommodations to individuals with disabilities during the application and hiring process as well as throughout the course of one's employment, should you need a reasonable accommodation during the application and selection process, including, but not limited to use of our website, any part of the application, interview or hiring process, please advise us so that we can provide appropriate assistance.", 'job_highlights': [{'title': 'Qualifications', 'items': ['Individuals requiring sponsorship (e.g', 'OPT or H1B visa status) should not apply', "Bachelor's Degree or equivalent experience in Engineering or Computer Science or Information Technology, or a related technical discipline required", '1+ years of experience with Modern Data Engineering projects and practices required', '1+ years of experience deploying cloud native solutions required', '1+ years of experience with AWS, SQL, Python, Docker/Kubernetes, CI/CD, Git familiarity with: Snowflake, DBT, Airflow required', 'Experience with advanced analytics and machine learning', 'Familiarity with BI tools such as Tableau, PowerBI']}, {'title': 'Benefits', 'items': ['The anticipated base pay compensation range for this position is $60,400.00 to $100,700.00', "With benefits starting on day one, our programs provide choice and flexibility to meet team members' individual needs, including:", 'Medical, dental, vision, and life insurance plans with coverage starting on day one of employment and 6 free sessions each year with a licensed therapist to support your emotional wellbeing', '18 paid time off (PTO) days annually for full-time employees (accrual prorated based on employment start date) and 6 company holidays per year', '6% company contribution to a 401(k) Retirement Savings Plan each pay period, no employee contribution required', 'Employee discounts, tuition reimbursement, student loan refinancing and free access to financial counseling, education, and tools', 'Maternity support programs, nursing benefits, and up to 14 weeks paid leave for birth parents and up to 4 weeks paid leave for non-birth parents', 'The pay range provided above is not a guarantee of compensation', 'The range reflects the potential base pay for this role at the time of this posting based on the job grade for this position', 'Individual base pay compensation will depend, in part, on factors such as geographic work location and relevant experience and skills', 'The anticipated compensation range described above is subject to change and the compensation ultimately paid may be higher or lower than the range described above', 'Grainger reserves the right to amend, modify, or terminate its compensation and benefit programs in its sole discretion at any time, consistent with applicable law']}, {'title': 'Responsibilities', 'items': ["The team's mission is to ensure the durability and scalability of machine learning and operations research models that drive impactful results across our supply chain", 'This role centers on designing and developing data pipelines and products that support advanced analytics', 'It involves close collaboration with machine learning engineers and data scientists to set up pipelines for model development and deployment, automate workflows, and maintain the data infrastructure behind key data products', 'Expect to work closely with subject matter experts, architects, analysts, and other stakeholders to integrate data from a wide range of enterprise sources', 'The position reports to the Senior Manager, Machine Learning Engineering', 'Enable analytics and reporting by centralizing and integrating high quality, large, complex data sets in a performant and scalable cloud platform', 'Identify, design, and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes', 'Build required tools for extraction, transformation and loading of data from multiple data sources', 'Build frameworks, standards & product features to enable self-service analytics', 'Partner with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues']}], 'apply_options': [{'title': 'ZipRecruiter', 'link': 'https://www.ziprecruiter.com/c/Grainger-Businesses/Job/Data-Engineer-II/-in-Lake-Forest,IL?jid=a9efc398784a014e&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'WhatJobs', 'link': 'https://www.whatjobs.com/jobs/data-engineer-ii?id=2262738059&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Talent.com', 'link': 'https://www.talent.com/view?id=d42aa25f8b3d&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'BeBee', 'link': 'https://us.bebee.com/job/9f663eef87b82a92293b2a8d21e372e2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobilize', 'link': 'https://www.jobilize.com/job/us-il-lake-forest-data-engineer-ii-grainger-businesses-hiring-now?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIElJIiwiY29tcGFueV9uYW1lIjoiR3JhaW5nZXIgQnVzaW5lc3NlcyIsImFkZHJlc3NfY2l0eSI6Ikxha2UgRm9yZXN0LCBJTCIsImh0aWRvY2lkIjoiM2w1VFprMHRBanNLbHAzeEFBQUFBQT09IiwiaGwiOiJlbiJ9'}, {'title': 'Data Engineer III', 'company_name': 'Insight Global', 'location': 'Atlanta, GA', 'via': 'Insight Global', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=8fMR8J3Q17toJ_nMAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEQQoCMQwAQLzuEwQhRxHdiuBFTwvKUj-xpCW0lZqUJod9ge8W5zDDdzPsH2gIT06FiTp47-EELwmghD1mEIZZJFXa3rNZ05tzqnVMamgljlE-TpiCrO4tQf8tmrFTq2i0XK7ndWycDjvPWlI2mKsErFAYJqvIhkeYpx8ChwFhiQAAAA&shmds=v1_AdeF8KijkE4YppudhynSjte4hiu4s5EkZo0lg3_zEzsowpThkA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=8fMR8J3Q17toJ_nMAAAAAA%3D%3D', 'extensions': ['Contractor', 'No degree mentioned'], 'detected_extensions': {'schedule_type': 'Contractor', 'qualifications': 'No degree mentioned'}, 'description': "The main function of the Data Engineer is to develop, evaluate, test and maintain architectures and data solutions within our organization. The typical Data Engineer executes plans, policies, and practices that control, protect, deliver, and enhance the value of the organizations data assets.\n\nWe are a company committed to creating diverse and inclusive environments where people can bring their full, authentic selves to work every day. We are an equal opportunity/affirmative action employer that believes everyone matters. Qualified candidates will receive consideration for employment regardless of their race, color, ethnicity, religion, sex (including pregnancy), sexual orientation, gender identity and expression, marital status, national origin, ancestry, genetic factors, age, disability, protected veteran status, military or uniformed service member status, or any other status or characteristic protected by applicable laws, regulations, and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send a request to HR@insightglobal.com.\n\nTo learn more about how we collect, keep, and process your private information, please review Insight Global's Workforce Privacy Policy: https://insightglobal.com/workforce-privacy-policy/ .", 'job_highlights': [{'title': 'Responsibilities', 'items': ['The main function of the Data Engineer is to develop, evaluate, test and maintain architectures and data solutions within our organization', 'The typical Data Engineer executes plans, policies, and practices that control, protect, deliver, and enhance the value of the organizations data assets']}], 'apply_options': [{'title': 'Insight Global', 'link': 'https://insightglobal.com/jobs/find_a_job/california/menlo-park/data-engineer-iii/job-390229/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Career.io', 'link': 'https://career.io/job/data-engineer-iii-new-york-sirius-xm-0eb144fa01a23fed5376a3c3d3a5c3a4?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Trabalha ES', 'link': 'https://www.trabalhaes.com.br/vaga/data-engineer-iii/remoto/89190/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Remote Rocketship', 'link': 'https://www.remoterocketship.com/company/esource/jobs/data-engineer-iii-united-states-remote/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'DailyRemote', 'link': 'https://dailyremote.com/remote-job/data-engineer-iii-4186158?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'MyNiceJob', 'link': 'https://mynicejob.com/jobs/data-engineer-iii/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Career Vault', 'link': 'https://www.careervault.io/remote/rackspace-21983/data/r-12711-data-engineer-iii-vietnam-remote-5221576?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Iitjobs', 'link': 'https://www.iitjobs.com/job/data-engineer-iii-farmington-hills-mi-usa-harman-square-networks-96664?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIElJSSIsImNvbXBhbnlfbmFtZSI6Ikluc2lnaHQgR2xvYmFsIiwiYWRkcmVzc19jaXR5IjoiQXRsYW50YSwgR0EiLCJodGlkb2NpZCI6IjhmTVI4SjNRMTd0b0pfbk1BQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Data Engineer - Healthcare', 'company_name': 'VirtualVocations', 'location': 'Carson, CA', 'via': 'Talent.com', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=4aAwf9tEA0jDpBXKAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWMQQrCMBAA8doneJA9iyYqeNGTVFF8QK9lG5YkEndLdoU-wydbLwNzmGm-i2Z_RUO4ccxMVGELD8JiKWClWZ4ygBLWkEAY7iKx0PKczEY9ea9aXFRDy8EFeXthGmTyLxn0j17TfBkLGvWH425yI8f1qsvVPlg6CXMnrJAZWqwqvIH28gPx754zkwAAAA&shmds=v1_AdeF8KiEpYjs0KqKEtD7m-8UreaWMFoQK5cecBxx-MklEId9aQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=4aAwf9tEA0jDpBXKAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965bc932b0e1e6fb1cc6f/images/6c5ca040eabb1b114302ba407b7fe258bfc942b591d5ee5d445d9f9e93af5468.jpeg', 'extensions': ['3 days ago', 'Full-time'], 'detected_extensions': {'posted_at': '3 days ago', 'schedule_type': 'Full-time'}, 'description': 'A company is looking for a Data Engineer - Healthcare.\n\nKey Responsibilities\n\nTranslate business requirements into specifications for data pipelines and reporting\n\nDevelop and implement data pipelines to support organizational initiatives\n\nEnsure data systems are efficient, scalable, and performant for data-intensive applications\n\nRequired Qualifications\n\n5+ years of experience building data ingestion processes and tools\n\n5+ years of experience with Data Bricks and Azure Data Factory (ADF)\n\n5+ years of database experience with MS SQL Server\n\n5+ years of experience in the healthcare industry with a strong understanding of data terminology and HIPAA protocols\n\nBS in Computer Science, IT, or equivalent experience; Microsoft Azure Certification is a plus', 'job_highlights': [{'title': 'Qualifications', 'items': ['5+ years of experience building data ingestion processes and tools', '5+ years of experience with Data Bricks and Azure Data Factory (ADF)', '5+ years of database experience with MS SQL Server', '5+ years of experience in the healthcare industry with a strong understanding of data terminology and HIPAA protocols']}, {'title': 'Responsibilities', 'items': ['Translate business requirements into specifications for data pipelines and reporting', 'Develop and implement data pipelines to support organizational initiatives', 'Ensure data systems are efficient, scalable, and performant for data-intensive applications']}], 'apply_options': [{'title': 'Talent.com', 'link': 'https://www.talent.com/view?id=5d7083971293&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIC0gSGVhbHRoY2FyZSIsImNvbXBhbnlfbmFtZSI6IlZpcnR1YWxWb2NhdGlvbnMiLCJhZGRyZXNzX2NpdHkiOiJDYXJzb24sIENBIiwiaHRpZG9jaWQiOiI0YUF3Zjl0RUEwakRwQlhLQUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': 'Consumer Data Engineering Lead', 'company_name': 'MondelÄ“z International', 'location': 'East Hanover, NJ', 'via': 'Indeed', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=U-ZcX9Rer7VnmfLOAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXLOw6CQBAA0NhyBKup_YAxodFSiUrUK5ABJsuaZYbsjIZ4DU_gXTyY2rzuJe9Jku-E9d5ThD0aQsHOM1H07OBM2MISSqlBCWPTgTAcRFyg6bYzG3STZaohdWpovkkb6TNhqmXMblLrn0o7jDQENKrW-WpMB3az-UW4pfB5PeHERpF_WxgDeIYC1eCILA-KC7iWXyLL5iykAAAA&shmds=v1_AdeF8KhEIx7JkCLuhu1i8uz5Xoyqe6EbcqZEg8dLin8a5Ol8rw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=U-ZcX9Rer7VnmfLOAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965bc932b0e1e6fb1cc6f/images/6c5ca040eabb1b1110582601ea353cc659cb805189d5036d506fc3df5de4e49a.jpeg', 'extensions': ['117,400â€“161,425 a year', 'Full-time', 'Paid time off', 'Dental insurance', 'Health insurance'], 'detected_extensions': {'salary': '117,400â€“161,425 a year', 'schedule_type': 'Full-time', 'paid_time_off': True, 'dental_coverage': True, 'health_insurance': True}, 'description': "Are You Ready to Make It Happen at MondelÄ“z International?\n\nJoin our Mission to Lead the Future of Snacking. Make It Uniquely Yours.\n\nWe are looking for a Regional Tech Engineering Lead to manage the technical operations and delivery of our digital marketing product stack across NA/LA Region. This includes websites, the regional Customer Data Platform (CDP), media solutions, and emerging Marketing GenAI solutions. This role ensures the technology landscape runs smoothly, integrations are executed effectively, and platform performance meets the needs of our marketing teams.As the go-to technical expert in-region, you will partner with regional & global IT teams, vendors, and local market stakeholders to ensure seamless execution and support of our digital marketing ecosystem.\n\nKey Responsibilities:\nâ€¢ Oversee the regional MarTech landscape from a tech standpoint, ensuring platform reliability, integration, and performance\nâ€¢ Act as the technical point of contact for global digital marketing product rollouts and support\nâ€¢ Collaborate with global product and engineering teams to align on architecture, deployment, and enhancements\nâ€¢ Oversee System Integration Testing (SIT) execution, including test plan approval, defect management, and quality gate enforcement\nâ€¢ Ensure IT Operations setup is fully implemented in partnership with Global Product Team, including monitoring, alerting, and support procedures\nâ€¢ Manage technical issue resolution, documentation, and user enablement for regional stakeholders\nâ€¢ Align on key data quality monitoring scope and process, implementing data quality metrics and automated validation checks\nâ€¢ Maintain comprehensive technical documentation and knowledge base for the platform\nâ€¢ Ensure platforms comply with security, privacy, and Responsible AI principles\nâ€¢ Support implementation of new features or tools by coordinating across business and IT teams\nâ€¢ Identify and manage technical debt with prioritized remediation plans\nâ€¢ Direct interactions with Architecture, Application/Platform, and Technical Product teams to guide technology selections, favoring scalable and cost-effective solutions.\n\nMore about this role\n\nRequired Skills:\nâ€¢ 5â€“7 years of experience in marketing technology, IT solution delivery, or digital platform support\nâ€¢ Technical knowledge of CDPs, CRM, CMS, DAM, media platforms, and marketing data flows\nâ€¢ Proven experience supporting or delivering regional or global digital products\nâ€¢ Strong troubleshooting, integration, and communication skills\nâ€¢ Experience working in cross-functional and multi-market environments\nâ€¢ Familiarity with cloud environments (e.g., GCP, Azure), APIs, and data privacy practices preferred\nâ€¢ Proven hands-on experience in architecting and deploying scalable cloud-based solutions for data processing, reporting, and analytics.\nâ€¢ Proven skills in data integration techniques, ETL processes, and familiarity with APIs and web services for connecting disparate data sources in a secure and efficient manner.\nâ€¢ Proven experience in leading technical teams and managing complex IT projects from conception through to implementation, ensuring alignment with strategic business objectives\nâ€¢ Eager to embrace new technologies, methodologies, and industry developments, with a commitment to lifelong learning and professional growth.\nâ€¢ Strategic and analytical thinker, setting technical direction, prioritizing projects, and mobilizing teams to deliver innovative solutions.\nâ€¢ Familiarity with GCP's analytics and machine learning services, including BigQuery, Dataflow, AI Platform\n\nCompensation:\n\nThe base salary range for this position is $117,400 to $161,425; the exact salary depends on several factors such as experience, skills, education, and budget. In addition to base salary, this position is eligible for participation in a highly competitive bonus program with possibility for overachievement based on performance and company results.\n\nIn addition, Mondelez International offers the following benefits: health insurance, wellness and family support programs, life and disability insurance, retirement savings plans, paid leave programs, education related programs, paid holidays, and vacation time. Some of these benefits have eligibility requirements. Many of these benefits are subsidized or fully paid for by the company.\n\nNo Relocation support available\n\nBusiness Unit Summary\n\nThe United States is the largest market in the MondelÄ“z International family with a significant employee and manufacturing footprint. Here, weproduce our well-loved household favorites to provide our consumers with the right snack, at the right moment, made the right way. We have corporate offices, sales, manufacturing and distribution locations throughout the U.S. to ensure our iconic brandsâ€”including Oreo and Chips Ahoy! cookies, Ritz, Wheat Thins and Triscuit crackers, and Swedish Fish and Sour Patch Kids confectionery products â€”are close at hand for our consumers across the country.\n\nMondelÄ“z Global LLC is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected Veteran status, sexual orientation, gender identity, gender expression, genetic information, or any other characteristic protected by law. Applicants who require accommodation to participate in the job application process may contact 847-943-5460 for assistance.\n\nFor more information about your Federal rights, please see eeopost.pdf; EEO is the Law Poster Supplement; Pay Transparency Nondiscrimination Provision; Know Your Rights: Workplace Discrimination is Illegal\n\nJob Type\n\nRegular\n\nSoftware & Applications\n\nTechnology & Digital", 'job_highlights': [{'title': 'Qualifications', 'items': ['5â€“7 years of experience in marketing technology, IT solution delivery, or digital platform support', 'Technical knowledge of CDPs, CRM, CMS, DAM, media platforms, and marketing data flows', 'Proven experience supporting or delivering regional or global digital products', 'Strong troubleshooting, integration, and communication skills', 'Experience working in cross-functional and multi-market environments', 'Proven hands-on experience in architecting and deploying scalable cloud-based solutions for data processing, reporting, and analytics', 'Proven skills in data integration techniques, ETL processes, and familiarity with APIs and web services for connecting disparate data sources in a secure and efficient manner', 'Proven experience in leading technical teams and managing complex IT projects from conception through to implementation, ensuring alignment with strategic business objectives', 'Eager to embrace new technologies, methodologies, and industry developments, with a commitment to lifelong learning and professional growth', 'Strategic and analytical thinker, setting technical direction, prioritizing projects, and mobilizing teams to deliver innovative solutions', "Familiarity with GCP's analytics and machine learning services, including BigQuery, Dataflow, AI Platform"]}, {'title': 'Benefits', 'items': ['The base salary range for this position is $117,400 to $161,425; the exact salary depends on several factors such as experience, skills, education, and budget', 'In addition to base salary, this position is eligible for participation in a highly competitive bonus program with possibility for overachievement based on performance and company results', 'In addition, Mondelez International offers the following benefits: health insurance, wellness and family support programs, life and disability insurance, retirement savings plans, paid leave programs, education related programs, paid holidays, and vacation time', 'Many of these benefits are subsidized or fully paid for by the company', 'No Relocation support available']}, {'title': 'Responsibilities', 'items': ['This role ensures the technology landscape runs smoothly, integrations are executed effectively, and platform performance meets the needs of our marketing teams.As the go-to technical expert in-region, you will partner with regional & global IT teams, vendors, and local market stakeholders to ensure seamless execution and support of our digital marketing ecosystem', 'Oversee the regional MarTech landscape from a tech standpoint, ensuring platform reliability, integration, and performance', 'Act as the technical point of contact for global digital marketing product rollouts and support', 'Collaborate with global product and engineering teams to align on architecture, deployment, and enhancements', 'Oversee System Integration Testing (SIT) execution, including test plan approval, defect management, and quality gate enforcement', 'Ensure IT Operations setup is fully implemented in partnership with Global Product Team, including monitoring, alerting, and support procedures', 'Manage technical issue resolution, documentation, and user enablement for regional stakeholders', 'Align on key data quality monitoring scope and process, implementing data quality metrics and automated validation checks', 'Maintain comprehensive technical documentation and knowledge base for the platform', 'Ensure platforms comply with security, privacy, and Responsible AI principles', 'Support implementation of new features or tools by coordinating across business and IT teams', 'Identify and manage technical debt with prioritized remediation plans', 'Direct interactions with Architecture, Application/Platform, and Technical Product teams to guide technology selections, favoring scalable and cost-effective solutions']}], 'apply_options': [{'title': 'Indeed', 'link': 'https://www.indeed.com/viewjob?jk=e26546d303d63261&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Glassdoor', 'link': 'https://www.glassdoor.com/job-listing/consumer-data-engineering-lead-mondel%C4%93z-international-JV_IC1126743_KO0,30_KE31,53.htm?jl=1009869471803&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'LinkedIn', 'link': 'https://www.linkedin.com/jobs/view/consumer-data-engineering-lead-at-mondel%C4%93z-international-4297628590?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'SimplyHired', 'link': 'https://www.simplyhired.com/job/jNGYWAHfG4oqBtTNHyNRlymZGZ5-x1q_DKZR2f1aljRWWX1PC9zpZw?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'BeBee', 'link': 'https://us.bebee.com/job/4d290764c8131b199b58bf2cfbefe384?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobilize', 'link': 'https://www.jobilize.com/job/us-nj-east-hanover-consumer-data-engineering-lead-mondelez-international?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobs Trabajo.org', 'link': 'https://us.trabajo.org/job-3310-d2761c0c0a43c6c242e1a1a460bc0060?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJDb25zdW1lciBEYXRhIEVuZ2luZWVyaW5nIExlYWQiLCJjb21wYW55X25hbWUiOiJNb25kZWzEk3ogSW50ZXJuYXRpb25hbCIsImFkZHJlc3NfY2l0eSI6IkVhc3QgSGFub3ZlciwgTkoiLCJodGlkb2NpZCI6IlUtWmNYOVJlcjdWbm1mTE9BQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Data Engineer [On-Site, See Cities in Posting] | Birmingham, AL, USA', 'company_name': 'Regions Financial Corporation', 'location': 'Birmingham, AL', 'via': 'EFinancialCareers', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=XpVXAP7hKmaQG-1lAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_1WOu2rDUBAFSetPSHVqI-sGQxqncpwHBEOCRSoTzEosVxukXXF3Cxf-w_yUldLNcKo5s_i7WxxfKAivmkWZC46fumokuELDjJ2EsEMUX-Yhmn9wwbOUcZ49jRW2-wrfzRYrfFgLZypdD1O8m-WB75_6iMk3KbkPdfagkK7ubEym3No5_Vrr_zh5T4WngYJP68eHcz1pXqYDZzF1vImSdkIDdlYmK7NlvpijbkuucnX2Ys4AAAA&shmds=v1_AdeF8Kh8nT6THsRMNM1I5gUO8fKhsBeW5rRtWz4RTHIjbZyjwg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=XpVXAP7hKmaQG-1lAAAAAA%3D%3D', 'extensions': ['21 days ago', 'Full-time', 'Health insurance', 'Dental insurance', 'Paid time off'], 'detected_extensions': {'posted_at': '21 days ago', 'schedule_type': 'Full-time', 'health_insurance': True, 'dental_coverage': True, 'paid_time_off': True}, 'description': 'Data Engineer [On-Site, See Cities in Posting]\n\nThank you for your interest in a career at Regions. At Regions, we believe associates deserve more than just a job. We believe in offering performance-driven individuals a place where they can build a career --- a place to expect more opportunities. If you are focused on results, dedicated to quality, strength and integrity, and possess the drive to succeed, then we are your employer of choice.\n\nRegions is dedicated to taking appropriate steps to safeguard and protect private and personally identifiable information you submit. The information that you submit will be collected and reviewed by associates, consultants, and vendors of Regions in order to evaluate your qualifications and experience for job opportunities and will not be used for marketing purposes, sold, or shared outside of Regions unless required by law. Such information will be stored in accordance with regulatory requirements and in conjunction with Regions\' Retention Schedule for a minimum of three years. You may review, modify, or update your information by visiting and logging into the careers section of the system.\n\nJob Description:\n\nAt Regions, the Data Engineer supports the Data and Analytics organization by designing, optimizing, and developing data pipelines to support machine learning and artificial intelligence (AI) models. The Data Engineer works with a cross functional team of Data Scientists and Software Developers on feature engineering and scoring pipelines. This role supports software developers, database architects, data analysts, and data scientists on data initiatives and ensures optimal data delivery architecture is consistent throughout ongoing projects.\n\nPrimary Responsibilities\n\nâ€¢ Partners with Regions Technology partners to Design, Build, and Maintain the data-based structures and systems in support of Data and Analytics and Data Product use cases\nâ€¢ Builds data pipelines to collect and arrange data and manage data storage in Regions\' big data environment\nâ€¢ Builds robust, testable programs for moving, transforming, and loading data using big data tools such as Spark\nâ€¢ Ensures data is prepared, arranged and ready for each defined business use case\nâ€¢ Provides consultation to all areas of the organization that plan to use data to make decisions\nâ€¢ Supports any team members in the development of such information delivery and aid in the automation of data products\nThis position is exempt from timekeeping requirements under the Fair Labor Standards Act and is not eligible for overtime pay.\n\nRequirements\n\nâ€¢ Bachelor\'s degree and four (4) years of experience in a quantitative/analytical/STEM field or technical related field\nâ€¢ Or Master\'s degree and two (2) years of experience in in a quantitative/analytical/STEM field\nâ€¢ Or Ph.D. in a quantitative/analytical/STEM field\nâ€¢ One (1) year of working programming experience in Python/PySpark, Scala, SQL\nâ€¢ One (1) year of working experience in Big Data Technology in Hadoop, Hive, Impala, Spark, or Kafka\nPreferences\n\nâ€¢ Background in Big Data Engineering and Advanced Data Analytics\nâ€¢ Experience developing solutions for the financial services industry\nâ€¢ Experience in Agile Software Development\nâ€¢ Experience or exposure to cloud technologies and migrations\nâ€¢ Prior banking or financial services experience\nSkills and Competencies\n\nâ€¢ Experience building data solutions at scale\nâ€¢ Experience designing and building relational data structures in multiple environments\nâ€¢ Experience with DevOps principals, CI/CD, and Software Development Lifecycle\nâ€¢ Experience with No-SQL databases\nâ€¢ Experience working with large-scale data Lakehouses\nâ€¢ Proven record of accomplishment of delivering operational Data solutions including Report and Model Ready Data Assets\nâ€¢ Significant experience working with senior executives in the use of data, reporting and visualizations to support strategic and operational decision making\nâ€¢ Strong ability to transform and integrate complex data from multiple sources into accessible, understandable, and usable data assets and frameworks\nâ€¢ Strong background in synthesizing data and analytics in a large (Fortune 500), complex, and highly regulated environment\nâ€¢ Strong communication skills through written and oral presentations\nâ€¢ Strong technical background including database and business intelligence skills\nThis position is intended to be onsite, now or in the near future . Associates will have regular work hours, including full days in the office three or more days a week. The manager will set the work schedule for this position, including in-office expectations. Regions will not provide relocation assistance for this position, and relocation would be at your expense. The locations available for this role are Birmingham, AL, Atlanta, GA or Charlotte, NC.\n\nRegions will not sponsor applicants for work visas for this position at this time. Applicants for this position must currently be authorized to work in the United States on a full-time basis.\n\nPosition Type\nFull time\n\nCompensation Details\n\nPay ranges are job specific and are provided as a point-of-market reference for compensation decisions. Other factors which directly impact pay for individual associates include: experience, skills, knowledge, contribution, job location and, most importantly, performance in the job role. As these factors vary by individuals, pay will also vary among individual associates within the same job.\n\nThe target information listed below is based on the Metropolitan Statistical Area Market Range for where the position is located and level of the position.\n\nJob Range Target:\n\nMinimum:\n$88,553.85 USD\nMedian:\n$119,210.00 USD\n\nIncentive Pay Plans:\nThis job is not incentive eligible.\n\nBenefits Information\n\nRegions offers a benefits package that is flexible, comprehensive and recognizes that "one size does not fit all" for benefits-eligible associates. Listed below is a synopsis of the benefits offered by Regions for informational purposes, which is not intended to be a complete summary of plan terms and conditions.\n\nâ€¢ Paid Vacation/Sick Time\nâ€¢ 401K with Company Match\nâ€¢ Medical, Dental and Vision Benefits\nâ€¢ Disability Benefits\nâ€¢ Health Savings Account\nâ€¢ Flexible Spending Account\nâ€¢ Life Insurance\nâ€¢ Parental Leave\nâ€¢ Employee Assistance Program\nâ€¢ Associate Volunteer Program\nPlease note, benefits and plans may be changed, amended, or terminated with respect to all or any class of associate at any time. To learn more about Regions\' benefits, please click or copy the link below to your browser.\n\nhttps://www.regions.com/about-regions/welcome-portal/benefits\n\nLocation Details\nRiverchase Operations Center\n\nLocation:\nHoover, Alabama\n\nEqual Opportunity Employer/including Disabled/Veterans\n\nJob applications at Regions are accepted electronically through our career site for a minimum of five business days from the date of posting. Job postings for higher-volume positions may remain active for longer than the minimum period due to business need and may be closed at any time thereafter at the discretion of the company.', 'job_highlights': [{'title': 'Qualifications', 'items': ["Bachelor's degree and four (4) years of experience in a quantitative/analytical/STEM field or technical related field", "Or Master's degree and two (2) years of experience in in a quantitative/analytical/STEM field", 'Or Ph.D. in a quantitative/analytical/STEM field', 'One (1) year of working programming experience in Python/PySpark, Scala, SQL', 'One (1) year of working experience in Big Data Technology in Hadoop, Hive, Impala, Spark, or Kafka', 'Background in Big Data Engineering and Advanced Data Analytics', 'Experience developing solutions for the financial services industry', 'Experience in Agile Software Development', 'Experience or exposure to cloud technologies and migrations', 'Prior banking or financial services experience', 'Experience building data solutions at scale', 'Experience designing and building relational data structures in multiple environments', 'Experience with DevOps principals, CI/CD, and Software Development Lifecycle', 'Experience with No-SQL databases', 'Experience working with large-scale data Lakehouses', 'Proven record of accomplishment of delivering operational Data solutions including Report and Model Ready Data Assets', 'Significant experience working with senior executives in the use of data, reporting and visualizations to support strategic and operational decision making', 'Strong ability to transform and integrate complex data from multiple sources into accessible, understandable, and usable data assets and frameworks', 'Strong background in synthesizing data and analytics in a large (Fortune 500), complex, and highly regulated environment', 'Strong communication skills through written and oral presentations', 'Strong technical background including database and business intelligence skills', 'Applicants for this position must currently be authorized to work in the United States on a full-time basis']}, {'title': 'Benefits', 'items': ['Pay ranges are job specific and are provided as a point-of-market reference for compensation decisions', 'Other factors which directly impact pay for individual associates include: experience, skills, knowledge, contribution, job location and, most importantly, performance in the job role', 'As these factors vary by individuals, pay will also vary among individual associates within the same job', '$88,553.85 USD', '$119,210.00 USD', 'Incentive Pay Plans:', 'This job is not incentive eligible', 'Regions offers a benefits package that is flexible, comprehensive and recognizes that "one size does not fit all" for benefits-eligible associates', 'Paid Vacation/Sick Time', '401K with Company Match', 'Medical, Dental and Vision Benefits', 'Disability Benefits', 'Health Savings Account', 'Flexible Spending Account', 'Life Insurance', 'Parental Leave', 'Employee Assistance Program', 'Associate Volunteer Program']}, {'title': 'Responsibilities', 'items': ['The information that you submit will be collected and reviewed by associates, consultants, and vendors of Regions in order to evaluate your qualifications and experience for job opportunities and will not be used for marketing purposes, sold, or shared outside of Regions unless required by law', "Such information will be stored in accordance with regulatory requirements and in conjunction with Regions' Retention Schedule for a minimum of three years", 'At Regions, the Data Engineer supports the Data and Analytics organization by designing, optimizing, and developing data pipelines to support machine learning and artificial intelligence (AI) models', 'The Data Engineer works with a cross functional team of Data Scientists and Software Developers on feature engineering and scoring pipelines', 'This role supports software developers, database architects, data analysts, and data scientists on data initiatives and ensures optimal data delivery architecture is consistent throughout ongoing projects', 'Partners with Regions Technology partners to Design, Build, and Maintain the data-based structures and systems in support of Data and Analytics and Data Product use cases', "Builds data pipelines to collect and arrange data and manage data storage in Regions' big data environment", 'Builds robust, testable programs for moving, transforming, and loading data using big data tools such as Spark', 'Ensures data is prepared, arranged and ready for each defined business use case', 'Provides consultation to all areas of the organization that plan to use data to make decisions', 'Supports any team members in the development of such information delivery and aid in the automation of data products', 'Associates will have regular work hours, including full days in the office three or more days a week', 'The manager will set the work schedule for this position, including in-office expectations']}], 'apply_options': [{'title': 'EFinancialCareers', 'link': 'https://www.efinancialcareers.com/jobs-USA-AL-Birmingham-Data_Engineer_On-Site_See_Cities_in_Posting.id23357843?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIFtPbi1TaXRlLCBTZWUgQ2l0aWVzIGluIFBvc3RpbmddIHwgQmlybWluZ2hhbSwgQUwsIFVTQSIsImNvbXBhbnlfbmFtZSI6IlJlZ2lvbnMgRmluYW5jaWFsIENvcnBvcmF0aW9uIiwiYWRkcmVzc19jaXR5IjoiQmlybWluZ2hhbSwgQUwiLCJodGlkb2NpZCI6IlhwVlhBUDdoS21hUUctMWxBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Data Engineer at GovServicesHub North Smithfield, RI', 'company_name': 'GovServicesHub', 'location': 'North Smithfield, RI', 'via': 'Sg-Host.com', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=p3c5eJQ8akJoK3znAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_23NPQoCMRBAYWz3CFZTWYgmIthoq_gHFu4BliSOSSSbWTLjsufyhGorNq97fNVrVJ23Rgzsso8ZsYAR2FNfY-mjQz48LVyoSIC6jRLuEdNtBtcjzOFEFhhNcQEofx7yCcebINLxWmvmpDyLkeiUo1ZTRkuDfpDlbxoOpmCXjGCzXC0G1WU_nfzAMf-1302X5BG1AAAA&shmds=v1_AdeF8KiOi10EMb2PtBN6eeF3kCO5QnqGXlAhjhMs43srbBq_sA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=p3c5eJQ8akJoK3znAAAAAA%3D%3D', 'extensions': ['23 days ago', 'Full-time'], 'detected_extensions': {'posted_at': '23 days ago', 'schedule_type': 'Full-time'}, 'description': 'Data Engineer job at GovServicesHub. North Smithfield, RI. Requirements\n\nOverview\n\nWe are seeking a highly motivated Data Engineer responsible for designing, building, and maintaining operational and analytical capabilities within enterprise data platforms. The data infrastructure is undergoing significant growth, offering an opportunity to play a leading role in shaping data capabilities across the organization.\n\nThe role involves solution design, data analysis, and production rollout project activities on data lakes using Snowflake, AWS, and Python.\n\nResponsibilities\n\nDesign, develop, and maintain data pipelines and solutions for large-scale data platforms.\n\nPerform solution design, data analysis, and production rollout of data projects.\n\nCollaborate with multiple teams to integrate data solutions for enhanced security and fraud detection.\n\nOptimize SQL queries for performance and efficiency.\n\nParticipate in Agile development processes (Kanban/SCRUM).\n\nPreferred Skills\n\nExperience with CockroachDB (distributed database).\n\nAPI development for real-time data solutions.\n\nExperience with job scheduling tools (Control-M preferred).\n\nFamiliarity with DevOps and CI/CD tools such as Maven, Jenkins, Ansible, Docker.\n\nExposure to containerization and Lambda development.\n\nQualifications\n\nBachelorâ€™s or Masterâ€™s degree in Computer Science, Engineering, or a related field.\n\n10+ years of relevant experience in data engineering.\n\nProven ability to work in a fast-paced, dynamic environment with multiple teams.\n\nExcellent communication and problem-solving skills.', 'job_highlights': [{'title': 'Qualifications', 'items': ['Experience with CockroachDB (distributed database)', 'API development for real-time data solutions', 'Familiarity with DevOps and CI/CD tools such as Maven, Jenkins, Ansible, Docker', 'Exposure to containerization and Lambda development', 'Bachelorâ€™s or Masterâ€™s degree in Computer Science, Engineering, or a related field', '10+ years of relevant experience in data engineering', 'Proven ability to work in a fast-paced, dynamic environment with multiple teams', 'Excellent communication and problem-solving skills']}, {'title': 'Responsibilities', 'items': ['We are seeking a highly motivated Data Engineer responsible for designing, building, and maintaining operational and analytical capabilities within enterprise data platforms', 'The data infrastructure is undergoing significant growth, offering an opportunity to play a leading role in shaping data capabilities across the organization', 'The role involves solution design, data analysis, and production rollout project activities on data lakes using Snowflake, AWS, and Python', 'Design, develop, and maintain data pipelines and solutions for large-scale data platforms', 'Perform solution design, data analysis, and production rollout of data projects', 'Collaborate with multiple teams to integrate data solutions for enhanced security and fraud detection', 'Optimize SQL queries for performance and efficiency', 'Participate in Agile development processes (Kanban/SCRUM)']}], 'apply_options': [{'title': 'Sg-Host.com', 'link': 'http://austinb36.sg-host.com/wp-jobs/job/data-engineer-at-govserviceshub-north-smithfield-ri-ZmdnTjBhU3duWHEvMEhBek5US0kwbUJ6Q1E9PQ==?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Www.gdsolaire-Inter.com', 'link': 'https://www.gdsolaire-inter.com/library/job/data-engineer-at-govserviceshub-north-smithfield-ri-U3FySXBGS3l1MVZ4Z2lYbm9qaGtFU0RPSmc9PQ==?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Svady', 'link': 'https://svady.cl/list/job/data-engineer-at-govserviceshub-north-smithfield-ri-WjZkMFhIY0V4dklaVHJ6QkIzeTN2V25pZWc9PQ==?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIGF0IEdvdlNlcnZpY2VzSHViIE5vcnRoIFNtaXRoZmllbGQsIFJJIiwiY29tcGFueV9uYW1lIjoiR292U2VydmljZXNIdWIiLCJhZGRyZXNzX2NpdHkiOiJOb3J0aCBTbWl0aGZpZWxkLCBSSSIsImh0aWRvY2lkIjoicDNjNWVKUThha0pvSzN6bkFBQUFBQT09IiwiaGwiOiJlbiJ9'}, {'title': 'Data Engineering Manager (Azure Data Factory)', 'company_name': 'Motion Recruitment', 'location': 'Chevy Chase, MD', 'via': 'Motion Recruitment', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=fhxPyCVCZqSFZhsgAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_x3MMQrCQBBAUWxzBKtpBBXNimCjlRgVhDReQCbLsLuSzITdiSQeyVMabV7z4WefSVYUqAhndoGJYmAHJTI6ijA_vrtI8O8XtCpxWMAablJBIozWgzBcRVxN04NXbdPemJTq3CVFDTa30hhhqqQ3T6nSj0fyGKmtUemx3W36vGW3nJWiYXzdycYuaEOsEBhOnl7DKCZaQVl8AWw0LqWtAAAA&shmds=v1_AdeF8KiQLnJKsB_kZqRTTWjKVasXSGh7PaEqj4Px-CRIt1h2DA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=fhxPyCVCZqSFZhsgAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965bc932b0e1e6fb1cc6f/images/6c5ca040eabb1b111abd88dd5f79b0d7bd67ab53febbcca14607260dc0589302.jpeg', 'extensions': ['158Kâ€“168K a year', 'Full-time', 'No degree mentioned', 'Paid time off', 'Health insurance'], 'detected_extensions': {'salary': '158Kâ€“168K a year', 'schedule_type': 'Full-time', 'qualifications': 'No degree mentioned', 'paid_time_off': True, 'health_insurance': True}, 'description': 'A Maryland based insurance firm is looking for a Data Engineering Manager. Ideal candidates will have at least 2+ years of professional management experience, and preferably 10+ years in the IT field. Candidate for this position should be able to remain and be hands-on when needed and have experience working within Azure (and using Azure Data Factory). Any former experience with SSIS is a plus. Required Skills & Experience\nâ€¢ 2+ years of management experience\nâ€¢ 10+ years of total experience\nâ€¢ Azure / Azure Data Factory\n\nThe Offer\nâ€¢ Competitive Salary\nâ€¢ Full-Health Benefits\n\nYou will receive the following benefits:\nâ€¢ Medical Insurance\nâ€¢ Dental Benefits\nâ€¢ Vision Benefits\nâ€¢ Paid Time Off (PTO)\nâ€¢ 401(k)\n\nApplicants must be currently authorized to work in the US on a full-time basis now and in the future.', 'job_highlights': [{'title': 'Qualifications', 'items': ['Ideal candidates will have at least 2+ years of professional management experience, and preferably 10+ years in the IT field', 'Candidate for this position should be able to remain and be hands-on when needed and have experience working within Azure (and using Azure Data Factory)', '2+ years of management experience', '10+ years of total experience', 'Azure / Azure Data Factory', 'Applicants must be currently authorized to work in the US on a full-time basis now and in the future']}, {'title': 'Benefits', 'items': ['Competitive Salary', 'Full-Health Benefits', 'You will receive the following benefits:', 'Medical Insurance', 'Dental Benefits', 'Vision Benefits', 'Paid Time Off (PTO)', '401(k)']}], 'apply_options': [{'title': 'Motion Recruitment', 'link': 'https://motionrecruitment.com/tech-jobs/chevy-chase/direct-hire/data-engineering-manager-azure-data-factory/796041?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'LinkedIn', 'link': 'https://www.linkedin.com/jobs/view/data-engineering-manager-azure-data-factory-at-motion-recruitment-4314197196?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jooble', 'link': 'https://jooble.org/jdp/4751873942821297956?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Ladders', 'link': 'https://www.theladders.com/job/data-engineering-manager-azure-data-factory-motion-recruitment-chevy-chase-md_83871248?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyaW5nIE1hbmFnZXIgKEF6dXJlIERhdGEgRmFjdG9yeSkiLCJjb21wYW55X25hbWUiOiJNb3Rpb24gUmVjcnVpdG1lbnQiLCJhZGRyZXNzX2NpdHkiOiJDaGV2eSBDaGFzZSwgTUQiLCJodGlkb2NpZCI6ImZoeFB5Q1ZDWnFTRlpoc2dBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Senior Data Engineer', 'company_name': 'TekSynap', 'location': 'Ashburn, VA', 'via': 'TekSynap Job Openings - ICIMS', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=DI_gmnxno-NVwm8wAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEvQoCMQwAYFzvBQSnzP60IrjodKAIrieuR1pCW61JaSqcsy8ufsPXfWfdaiBOUuGEDeHMITFRhQ1cxYESVh9BGC4iIdPiGFsrerBWNZugDVvyxsvLCpOTyT7E6b9RI1YqGRuNu_12MoXDcn6j5_BhLJAYeo3uXXkN9_4HoMewSoYAAAA&shmds=v1_AdeF8KhiPrLGWPkowvs2k_tEz1kpzZhuRmh53PGntB0-phne2g&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=DI_gmnxno-NVwm8wAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965bc932b0e1e6fb1cc6f/images/6c5ca040eabb1b11e56e269e96d6b00df664d6ce93be05712402b3f6e89c5ef8.jpeg', 'extensions': ['Full-time', 'Paid time off', 'Health insurance', 'Dental insurance'], 'detected_extensions': {'schedule_type': 'Full-time', 'paid_time_off': True, 'health_insurance': True, 'dental_coverage': True}, 'description': 'Responsibilities & Qualifications\n\nACTIVITIES & RESPONSIBILITIES\n\nThe Senior Data Engineer will be responsible for developing and integrating advanced data analytics for both desktop and web-based visual analytic platforms. This includes enabling large-scale analysis of relational data through innovative graphics and visualization techniques, utilizing high-performance computing resources. The engineer will work with open source, commercial off-the-shelf (COTS), and government off-the-shelf (GOTS) technologies to transform structured and semi-structured data into actionable intelligence.\n\nThe individual will also:\nâ€¢ Design enterprise database strategies and set standards for operational performance, programming practices, and security protocols.\nâ€¢ Build and manage large-scale relational databases.\nâ€¢ Ensure seamless integration of new systems with existing data warehouse infrastructure.\nâ€¢ Optimize system performance and enhance overall functionality.\n\nSKILLS\n\nA senior-level data engineer must possess comprehensive expertise across key technology domains and high-impact assignments. This includes:\nâ€¢ Leading complex engineering efforts and shaping strategic technology directions.\nâ€¢ Conducting performance evaluations and recommending significant improvements that drive short-term project outcomes and long-term success.\nâ€¢ Acting as a technical authority across multiple concurrent projects.\nâ€¢ Providing guidance and potentially supervising other team members.\nâ€¢ This is a high-impact, mission-critical role for a candidate with proven experience in data engineering and secure, scalable analytics environments.\n\nREQUIRED QUALIFICATIONS\nâ€¢ Masterâ€™s degree\nâ€¢ Minimum of 8 years of relevant professional experience\nâ€¢ Applicants must:\nâ€¢ Possess a Top Secret (TS) Security Clearance with the ability to obtain Sensitive Compartmented Information (SCI) clearance.\nâ€¢ Be a Citizen of the United States of America only â€“ no dual citizenship shall be allowed.\nâ€¢ Possess a valid U.S. driverâ€™s license.\nâ€¢ Pass a criminal background check.\n\nOverview\n\nWe are seeking a highly skilled Senior Data Engineer to lead and execute critical engineering tasks in support of an advanced visual analytic application. This role requires a motivated and experienced individual with a deep understanding of data analytics, database architecture, and high-performance computing to deliver powerful analytic capabilities to our government client.\n\nThe Systems and Data Engineering Services effort requires highly skilled systems support staff to assist in engagement efforts, research support, and office management. Staff will be integral members of a team that provides support that is critical to the execution of the mission. This position supports a government customer who devises policy matters and techniques to be used in complicated investigations, as well as for making a continuous review of investigative procedures and programs. In addition, the customer maintains top-level liaison with officials in the Department of Justice, as well as other Government agencies and foreign law enforcement officials, on matters under the jurisdiction of the FBI for the purpose of coordinating and resolving major policy matters concerning both criminal cases and civil litigations.\n\nTekSynap is a fast-growing high-tech company that understands both the pace of technology today and the need to have a comprehensive well planned information management environment. â€œTechnology moving at the speed of thoughtâ€ embodies these principles â€“ the need to nimbly utilize the best that information technology offers to meet the business needs of our Federal Government customers.\n\nWe offer our full-time employees a competitive benefits package to include health, dental, vision, 401K, life insurance, short-term and long-term disability plans, vacation time and holidays.\n\nVisit us at www.TekSynap.com.\n\nApply now to explore jobs with us!\n\nThe safety and health of our employees is of the utmost importance. Employees are required to comply with any contractually mandated Federal COVID-19 requirements. More information can be found here.\n\n"As part of the application process, you agree that TekSynap Corporation may retain and use your name, e-mail, and contact information for purposes related to employment consideration".\n\nAdditional Job Information\n\nWORK ENVIRONMENT AND PHYSICAL DEMANDS\n\nThe work environment characteristics described here are representative of those an employee encounters while performing the essential functions of the job. Reasonable accommodation may be made to enable individuals with disabilities to perform the essential functions.\nâ€¢ Location: Chantilly VA / Washington DC area\nâ€¢ Remote or In-Person: 100% On site. Remote/Telework not available.\nâ€¢ Type of environment: Office\nâ€¢ Noise level: Medium\nâ€¢ Work schedule: Schedule is day shift Monday â€“ Friday.\nâ€¢ Amount of Travel: Limited travel within the US may be required.\n\nPHYSICAL DEMANDS\n\nThe physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.\n\nWORK AUTHORIZATION/SECURITY CLEARANCE\nâ€¢ Active Top Secret Clearance Required\n\nOTHER DUTIES\n\nPlease note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nEQUAL EMPLOYMENT OPPORTUNITY\n\nIn order to provide equal employment and advancement opportunities to all individuals, employment decisions will be based on merit, qualifications, and abilities. TekSynap does not discriminate against any person because of race, color, creed, religion, sex, national origin, disability, age, genetic information, or any other characteristic protected by law (referred to as â€œprotected statusâ€). This nondiscrimination policy extends to all terms, conditions, and privileges of employment as well as the use of all company facilities, participation in all company-sponsored activities, and all employment actions such as promotions, compensation, benefits, and termination of employment.', 'job_highlights': [{'title': 'Qualifications', 'items': ['A senior-level data engineer must possess comprehensive expertise across key technology domains and high-impact assignments', 'Masterâ€™s degree', 'Minimum of 8 years of relevant professional experience', 'Possess a Top Secret (TS) Security Clearance with the ability to obtain Sensitive Compartmented Information (SCI) clearance', 'Be a Citizen of the United States of America only â€“ no dual citizenship shall be allowed', 'Possess a valid U.S. driverâ€™s license', 'Pass a criminal background check', 'We are seeking a highly skilled Senior Data Engineer to lead and execute critical engineering tasks in support of an advanced visual analytic application', 'This role requires a motivated and experienced individual with a deep understanding of data analytics, database architecture, and high-performance computing to deliver powerful analytic capabilities to our government client', 'The Systems and Data Engineering Services effort requires highly skilled systems support staff to assist in engagement efforts, research support, and office management', 'Amount of Travel: Limited travel within the US may be required', 'The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job', 'Active Top Secret Clearance Required']}, {'title': 'Benefits', 'items': ['We offer our full-time employees a competitive benefits package to include health, dental, vision, 401K, life insurance, short-term and long-term disability plans, vacation time and holidays', 'Remote or In-Person: 100% On site']}, {'title': 'Responsibilities', 'items': ['The Senior Data Engineer will be responsible for developing and integrating advanced data analytics for both desktop and web-based visual analytic platforms', 'This includes enabling large-scale analysis of relational data through innovative graphics and visualization techniques, utilizing high-performance computing resources', 'The engineer will work with open source, commercial off-the-shelf (COTS), and government off-the-shelf (GOTS) technologies to transform structured and semi-structured data into actionable intelligence', 'Design enterprise database strategies and set standards for operational performance, programming practices, and security protocols', 'Build and manage large-scale relational databases', 'Ensure seamless integration of new systems with existing data warehouse infrastructure', 'Optimize system performance and enhance overall functionality', 'Leading complex engineering efforts and shaping strategic technology directions', 'Conducting performance evaluations and recommending significant improvements that drive short-term project outcomes and long-term success', 'Acting as a technical authority across multiple concurrent projects', 'Providing guidance and potentially supervising other team members', 'This is a high-impact, mission-critical role for a candidate with proven experience in data engineering and secure, scalable analytics environments', 'Staff will be integral members of a team that provides support that is critical to the execution of the mission', 'This position supports a government customer who devises policy matters and techniques to be used in complicated investigations, as well as for making a continuous review of investigative procedures and programs', 'In addition, the customer maintains top-level liaison with officials in the Department of Justice, as well as other Government agencies and foreign law enforcement officials, on matters under the jurisdiction of the FBI for the purpose of coordinating and resolving major policy matters concerning both criminal cases and civil litigations', 'The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of the job', 'Work schedule: Schedule is day shift Monday â€“ Friday', 'Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions']}], 'apply_options': [{'title': 'TekSynap Job Openings - ICIMS', 'link': 'https://careers-teksynap.icims.com/jobs/8146/senior-data-engineer/job?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'ZipRecruiter', 'link': 'https://www.ziprecruiter.com/c/Infinitive-Inc/Job/Senior-Data-Engineer-(Databricks)/-in-Ashburn,VA?jid=501afe7bf9a3cf5f&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Built In', 'link': 'https://builtin.com/job/senior-data-engineer-databricks/3188289?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'LinkedIn', 'link': 'https://www.linkedin.com/jobs/view/senior-data-engineer-at-teksynap-4264181311?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Indeed', 'link': 'https://www.indeed.com/viewjob?jk=0627a2078b089985&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Iitjobs', 'link': 'https://www.iitjobs.com/job/senior-data-engineer-ashburn-va-usa-largeton-126956?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobs - JazzHR', 'link': 'https://infinitive.applytojob.com/apply/B6XnkdgurR/Senior-Data-Engineer-PythonPySparkAWS?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'SimplyHired', 'link': 'https://www.simplyhired.com/job/Td8D4KJW1LkVeomUo0wCPBrUHOBbbCxhVUtKWNHvEesCKk33ceycdw?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IlRla1N5bmFwIiwiYWRkcmVzc19jaXR5IjoiQXNoYnVybiwgVkEiLCJodGlkb2NpZCI6IkRJX2dtbnhuby1OVndtOHdBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Senior Database Engineer', 'company_name': 'SkySlope', 'location': 'Sacramento, CA', 'via': 'Lever', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=abnpTIrP6vZE9z-dAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCMBAAUFz7CZ1uFm1EcNFJqgiu-YByCUcaTe9C7ob6C361-IbXfTed88RZGtzQMKAS3DllJmqwh6cEUMIWZxCGh0gq1F9ms6pn51TLkNTQchyiLE6YgqzuJUH_TTpjo1rQaDqeDutQOW17__74IpUgM3iMDRdikx2M1x_d_aQEjQAAAA&shmds=v1_AdeF8Kj5mUYEdosuDDTQuVwnhEl2Av2b7SjmEfsdoU3AjNxUeQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=abnpTIrP6vZE9z-dAAAAAA%3D%3D', 'extensions': ['19 days ago', 'Full-time', 'No degree mentioned', 'Health insurance', 'Paid time off', 'Dental insurance'], 'detected_extensions': {'posted_at': '19 days ago', 'schedule_type': 'Full-time', 'qualifications': 'No degree mentioned', 'health_insurance': True, 'paid_time_off': True, 'dental_coverage': True}, 'description': "OUR ORIGIN STORY ðŸŽ‚\n\nIn 2011 SkySlope started as an idea born at the kitchen table of our CEO, with just him and two others. Headquartered in Sacramento, California, we have since grown out of our previous 3 offices and many of our close to 150 employees are spread all across the United States. Those 150 employees support close to 300,000 users across 5,000 offices nationwide and now in Canada as well. Included in that is 8 out of the 15 largest Real Estate Brokerages in the nation.\n\nBut, despite being happy with what weâ€™ve achieved we know that as industry leaders in our space thereâ€™s a lot of work left to be done. All of the growth and success that has happened is a result of us obsessing over building cutting edge software that makes the Real Estate world a better place. We know this only happens by hiring people who donâ€™t just come up with out of the box ideas but hiring people who actually see those ideas through and bring them to life. As weâ€™ve grown, weâ€™ve been fortunate enough to hire plenty of people who possess that quality and realize itâ€™s equally important to hire people who can pair that skill with empathy, collaboration, and a keen sense of urgency. If youâ€™re looking to join a company where you can have real impact and surround yourself with an incredible team of people then look no further.\n\nSKYSLOPEâ€™S CORE VALUES ðŸ’ªðŸ»\n\nThese are the principles that helped us get to where we are and they are the principles that will guide us to where we want to go in the future. You can apply them to your professional life, your personal life, to any business and any situation. In no specific hierarchy, our core values are:\n\nAwareness | Execution | Obsession | Ownership | Humility | Radical Candor | Urgency | Greatness | Inches I Fun\n\nLearn more about our core values from our CEO, Tyler Smith here!\n\nThe purpose of the Senior Data Engineer at SkySlope is to develop, test, and deploy new and improved software, while managing deliverables in a fast-paced, agile work environment. They are expected to continue promoting their talent via self-learning and collaboration with our senior engineers to pursue advancing their own seniority in the company.\n\nJob Responsibilities\nâ€¢ Design and implement database schemas for both relational and NoSQL systems\nâ€¢ Optimize query performance and troubleshoot complex database issues\nâ€¢ Design and maintain ETL pipelines for data integration and transformation\nâ€¢ Work with multiple APIs for data extraction and system integration\nâ€¢ Lead database migration projects and version upgrades\nâ€¢ Establish backup, recovery, and disaster recovery procedures\nâ€¢ Monitor database health, capacity planning, and performance tuning\nâ€¢ Support data infrastructure for AI/ML workloads and analytics\nâ€¢ Mentor engineers and provide technical guidance on data architecture\nâ€¢ Collaborate with DevOps on database automation and CI/CD pipelines\n\nJob Requirements\nâ€¢ 5+ years of database engineering experience\nâ€¢ Expert-level SQL proficiency with complex query optimization\nâ€¢ Proficiency with major database platforms (SQL Server, MySQL, or PostgreSQL)\nâ€¢ Strong experience with NoSQL databases (MongoDB, DynamoDB or Redis)\nâ€¢ Experience with cloud database services (AWS RDS, Azure SQL, or Google Cloud SQL)\nâ€¢ Experience working with REST APIs and data integration from multiple sources\nâ€¢ Proficiency with Git and GitHub for version control and collaboration\nâ€¢ Knowledge of database security best practices and compliance requirements\nâ€¢ Familiarity with database monitoring tools (Grafana, DataDog, or similar)\nâ€¢ Experience with infrastructure as code (Terraform, CloudFormation)\nâ€¢ Experience with AI-powered development tools (Cursor, GitHub Copilot, or similar)\nâ€¢ Experience in an Agile environment\n\nPreferred Qualifications\nâ€¢ Experience with data warehousing solutions (Snowflake, BigQuery, Redshift)\nâ€¢ Experience with ETL tools (Apache Airflow, Talend, Pentaho, or custom solutions)\nâ€¢ Python or scripting experience for database automation\nâ€¢ C# (.NET Core and Framework)\nâ€¢ Experience with microservices database patterns\nâ€¢ Familiarity with vector databases or AI data storage patterns\nâ€¢ Cloud services (AWS DMS, RDS, S3, EC2, CloudFormation, etc.)\n\n$160,000 - $190,000 a year\n\nPerks & PTO ðŸŒ´\n\n- $1000 referral bonuses\n\n- 15 PTO days per year\n\n- 16 paid holidays per year (5 floating to be used at any time)\n\n- Paid day off on your birthday\n\n- 5 Days Paid Bereavement Leave\n\n- 6 Weeks Paid Parental Leave\n\n- Waldorf University discounts and perks\n\n- Child literacy assistance program discounts\n\n- Discounts on wireless, car, rentals, hotels, and more...\n\nInsurance Offerings ðŸ‘©\u200dâš•ï¸\n\n- Medical, Dental, and Vision Insurance\n\n- Short and Long Term Disability Insurance\n\n- Accident insurance, Critical Illness, Hospital indemnity\n\n- Company-paid Life Insurance\n\n- Flexible Spending Account (FSA)\n\n- Health Spending Account (HSA)\n\n- Pet Insurance\n\nRetirement and Investment ðŸ’¸\n\n- 401k + match\n\n- Employee Stock Purchase Plan opportunities\n\nSkySlope, is an Equal Opportunity employer. All qualified applicants will receive\n\nconsideration for employment without regard to race, color, religion, sex, age, disability, protected veteran status,\n\nnational origin, sexual orientation, gender identity or expression (including transgender status), genetic\n\ninformation or any other characteristic protected by applicable law.\n\nWe sincerely thank you for taking the time to review our open positions and hope you'll take the time to submit a concise and thoughtful application.\n\nStill thinking about applying? Waiting to hear back from us? Check out our social media in the meantime!\n\nSkySlope | Facebook | Instagram | YouTube | LinkedIn | Twitter\n\nYour privacy is important to us. Learn more about what data is collected and how we use it here.", 'job_highlights': [{'title': 'Qualifications', 'items': ['5+ years of database engineering experience', 'Expert-level SQL proficiency with complex query optimization', 'Proficiency with major database platforms (SQL Server, MySQL, or PostgreSQL)', 'Strong experience with NoSQL databases (MongoDB, DynamoDB or Redis)', 'Experience with cloud database services (AWS RDS, Azure SQL, or Google Cloud SQL)', 'Experience working with REST APIs and data integration from multiple sources', 'Proficiency with Git and GitHub for version control and collaboration', 'Knowledge of database security best practices and compliance requirements', 'Familiarity with database monitoring tools (Grafana, DataDog, or similar)', 'Experience with infrastructure as code (Terraform, CloudFormation)', 'Experience with AI-powered development tools (Cursor, GitHub Copilot, or similar)', 'Experience in an Agile environment']}, {'title': 'Benefits', 'items': ['$160,000 - $190,000 a year', 'Perks & PTO ðŸŒ´', '$1000 referral bonuses', '15 PTO days per year', '16 paid holidays per year (5 floating to be used at any time)', 'Paid day off on your birthday', '5 Days Paid Bereavement Leave', '6 Weeks Paid Parental Leave', 'Waldorf University discounts and perks', 'Child literacy assistance program discounts', 'Discounts on wireless, car, rentals, hotels, and more..', 'Insurance Offerings ðŸ‘©\u200dâš•ï¸', 'Medical, Dental, and Vision Insurance', 'Short and Long Term Disability Insurance', 'Accident insurance, Critical Illness, Hospital indemnity', 'Company-paid Life Insurance', 'Flexible Spending Account (FSA)', 'Health Spending Account (HSA)', 'Pet Insurance', 'Retirement and Investment ðŸ’¸', '401k + match']}, {'title': 'Responsibilities', 'items': ['Design and implement database schemas for both relational and NoSQL systems', 'Optimize query performance and troubleshoot complex database issues', 'Design and maintain ETL pipelines for data integration and transformation', 'Work with multiple APIs for data extraction and system integration', 'Lead database migration projects and version upgrades', 'Establish backup, recovery, and disaster recovery procedures', 'Monitor database health, capacity planning, and performance tuning', 'Support data infrastructure for AI/ML workloads and analytics', 'Mentor engineers and provide technical guidance on data architecture', 'Collaborate with DevOps on database automation and CI/CD pipelines']}], 'apply_options': [{'title': 'Lever', 'link': 'https://jobs.lever.co/skyslope/cbb6448b-acb3-4bcc-b8a0-abcc8c0a8a3b?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobs.weekday.works', 'link': 'https://jobs.weekday.works/skyslope-senior-database-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Talent.com', 'link': 'https://www.talent.com/view?id=a392c1a7b4a3&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'BeBee', 'link': 'https://us.bebee.com/job/0d415af8d8244281aac4035652a223c4?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobilize', 'link': 'https://www.jobilize.com/job/us-ca-sacramento-senior-database-engineer-energy-jobline-zr-hiring?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobs Trabajo.org', 'link': 'https://us.trabajo.org/job-3624-3a0347edbeffc186582669aa45b82318?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YWJhc2UgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJTa3lTbG9wZSIsImFkZHJlc3NfY2l0eSI6IlNhY3JhbWVudG8sIENBIiwiaHRpZG9jaWQiOiJhYm5wVElyUDZ2WkU5ei1kQUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': 'Data Engineer II - Enterprise Data & Analytics - Digital and Technology Partners - Remote', 'company_name': 'Mount Sinai Hospital', 'location': 'New York, NY', 'via': 'Ladders', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=q2wBr5qMZtI2lsoFAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_x2OvQoCQQyEsfURrFKIheidCDZaCYo_oIjaWEnuDHura7JsIuqD-j6eNsMM3zBM89NoxhkawpydZ6IEqxX062SUYvJK8KcdmDKGt_lSazrzzhsGQL7AkcqKJYh7ww6TMaVfY093MarNWgpQwlRWIAwLEReoNanMoo7zXDVkTg3r3ayUey5MhbzyqxT6k7NWmCgGNDoPR4NXFtl12xt5sMHBM3pYisb_Fc-wpSecJN16sD19Afm0FzPYAAAA&shmds=v1_AdeF8KhTBMNaD3MwnDvIfWQ6VL5Svk_AcBEmg7QzJPgH2poMkQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=q2wBr5qMZtI2lsoFAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965bd73d224b1cfd371f5/images/10ad8df2269535f2b44c1622d15658d33a3dd157add2fa202c303de243765313.jpeg', 'extensions': ['90,000â€“135,285 a year', 'Full-time', 'Health insurance'], 'detected_extensions': {'salary': '90,000â€“135,285 a year', 'schedule_type': 'Full-time', 'health_insurance': True}, 'description': 'Job Description\n\nData Engineer II - Enterprise Data & Analytics - Digital and Technology Partners - Remote\n\nThe Data Engineer II is responsible for designing, developing, and maintaining robust, scalable, and secure data architectures that facilitate efficient data collection, transformation, storage, analysis, and self-service access. Leveraging modern tools and methodologies, the Data Engineer II manages transformation logic, data models, and metric definitions, develops automated pipelines, and supports AI-driven workflows. By ensuring data reliability and quality, this role directly contributes to enhancing patient care delivery, clinical pathways, translational research, and overall clinical care. The Data Engineer II collaborates closely with business teams, data scientists, machine learning engineers, and backend developers to deliver impactful data solutions.\n\nResponsibilities\nâ€¢ Design, develop, and maintain scalable and reliable data pipelines using orchestration engines such as Airflow and Dataiku, ensuring seamless automation of data ingestion, transformation, and delivery processes.\nâ€¢ Deploy and maintain containerized applications and pipelines, employing technologies like Docker and Kubernetes to achieve resilient and maintainable data workflows.\nâ€¢ Develop, deploy, and operationalize AI workflows, including image processing, data categorization, and natural language processing (NLP) models, ensuring production-level reliability and performance.\nâ€¢ Implement DevOps best practices, including version control with Git, continuous integration and continuous deployment (CI/CD) pipelines, automated testing frameworks, and unit testing to facilitate rapid, reliable, and high-quality software deployments.\nâ€¢ Create and manage a scalable and maintainable data architecture, including designing reference tables and deploying AI-driven mechanisms to ensure reference tables remain accurate and current.\nâ€¢ Develop and maintain comprehensive data dictionaries, enforce data quality metrics, implement anomaly detection solutions, and establish atomic rollback processes to effectively manage and rectify data errors.\nâ€¢ Build robust data ingestion pipelines capable of handling diverse data sources, including AI-generated data, flat files, and RESTful APIs (both reading from and writing to endpoints).\nâ€¢ Collaborate closely with agile teams consisting of Application Developers, Database Developers, and Data Scientists, actively participating in sprint planning, stand-ups, and retrospectives.\nâ€¢ Create centralized documentation, diagrams, and metadata catalogs that clearly describe data solutions, facilitating knowledge sharing and ease of use.\nâ€¢ Design, implement, and manage data system monitoring, backups, and disaster recovery plans, safeguarding the integrity, availability, and security of data.\nâ€¢ Engage with stakeholders with a customer-focused approach, delivering solutions that align with scientific, research, and clinical objectives.\nâ€¢ Ensure adherence to industry best practices, HIPAA compliance, and institutional data governance policies and procedures.\nâ€¢ Maintain current knowledge of industry trends and emerging technologies, demonstrating flexibility and continuous learning to adapt and enhance skillsets relevant to data engineering.\nâ€¢ Develop standards and best practices documentation related to data management, architecture, and maintenance, and provide training and presentations to team members as required.\nâ€¢ Maintains strong desire to understand the "why" behind data.\n\nQualifications\n\nEducation Requirements\n\nBachelor\'s degree in Computer Science or a related discipline; Advanced degree preferred.\n\nExperience Requirements\nâ€¢ 4+ years of relevant professional experience, preferably in data engineering, data pipeline development, and data science workflows, in a Linux environment.\nâ€¢ Strong knowledge of SQL and NoSQL databases, including Azure SQL, PostgreSQL/MySQL, and MongoDB or similar.\nâ€¢ Proficiency with at least two programming languages among Python, Scala, Java, or Go, with the flexibility to quickly learn additional languages.\nâ€¢ Hands-on experience working with RESTful APIs and services, preferably using Node.js, Django, or similar frameworks.\nâ€¢ Demonstrated expertise in containerization technologies such as Docker and Kubernetes.\nâ€¢ Solid understanding and practical experience with orchestration and automation tools like Airflow or Dataiku.\nâ€¢ Experience in developing, deploying, and maintaining AI workflows, including computer vision/image processing, NLP, and data categorization models.\nâ€¢ Proficiency in using DevOps tools and practices, including Git, CI/CD tools, automated unit testing, and GitHub Copilot or similar AI-assisted coding tools (e.g., ChatGPT).\nâ€¢ Experience building scalable data architectures with reference tables and automated AI-driven updating mechanisms.\nâ€¢ Familiarity with Hadoop, Spark, Kafka, and streaming data platforms and technologies.\nâ€¢ Knowledge of healthcare data standards such as HL7, and familiarity with integration tools like Mirth is a significant plus.\nâ€¢ Strong knowledge of Azure cloud data services, including Azure Databricks, Azure Fabric, Azure Data Factory, serverless computing, virtual machines, and cloud security.\nâ€¢ Strong knowledge of data visualization tools such as Power BI or Tableau.\nâ€¢ Strong knowledge of data structures, data formats, algorithms, and object-oriented design and practical experience with data serialization and storage formats such as Parquet, Avro, JSON, or ORC.\nâ€¢ Experience working within Agile methodologies and tools, specifically JIRA, is highly desirable.\n\nCompensation Statement\n\nThe Mount Sinai Health System (MSHS) provides salary ranges that comply with the New York City Law on Salary Transparency in Job Advertisements. The salary range for the role is $90,000.00 - $135,285.00 Annually. Actual salaries depend on a variety of factors, including experience, education, and operational need. The salary range or contractual rate listed does not include bonuses/incentive, differential pay or other forms of compensation or benefits.\n\nNon-Bargaining Unit, 223 - DTP Enterprise Data & Analytics - MSH, Mount Sinai Hospital\n\nAbout Us\n\nStrength through Unity and Inclusion\n\nThe Mount Sinai Health System is committed to fostering an environment where everyone can contribute to excellence. We share a common dedication to delivering outstanding patient care. When you join us, you become part of Mount Sinai\'s unparalleled legacy of achievement, education, and innovation as we work together to transform healthcare. We encourage all team members to actively participate in creating a culture that ensures fair access to opportunities, promotes inclusive practices, and supports the success of every individual.\n\nAt Mount Sinai, our leaders are committed to fostering a workplace where all employees feel valued, respected, and empowered to grow. We strive to create an environment where collaboration, fairness, and continuous learning drive positive change, improving the well-being of our staff, patients, and organization. Our leaders are expected to challenge outdated practices, promote a culture of respect, and work toward meaningful improvements that enhance patient care and workplace experiences. We are dedicated to building a supportive and welcoming environment where everyone has the opportunity to thrive and advance professionally. Explore this opportunity and be part of the next chapter in our history.\n\nAbout the Mount Sinai Health System:\n\nMount Sinai Health System is one of the largest academic medical systems in the New York metro area, with more than 48,000 employees working across eight hospitals, more than 400 outpatient practices, more than 300 labs, a school of nursing, and a leading school of medicine and graduate education. Mount Sinai advances health for all people, everywhere, by taking on the most complex health care challenges of our time - discovering and applying new scientific learning and knowledge; developing safer, more effective treatments; educating the next generation of medical leaders and innovators; and supporting local communities by delivering high-quality care to all who need it. Through the integration of its hospitals, labs, and schools, Mount Sinai offers comprehensive health care solutions from birth through geriatrics, leveraging innovative approaches such as artificial intelligence and informatics while keeping patients\' medical and emotional needs at the center of all treatment. The Health System includes more than 9,000 primary and specialty care physicians; 13 joint-venture outpatient surgery centers throughout the five boroughs of New York City, Westchester, Long Island, and Florida; and more than 30 affiliated community health centers. We are consistently ranked by U.S. News & World Report\'s Best Hospitals, receiving high "Honor Roll" status, and are highly ranked: No. 1 in Geriatrics, top 5 in Cardiology/Heart Surgery, and top 20 in Diabetes/Endocrinology, Gastroenterology/GI Surgery, Neurology/Neurosurgery, Orthopedics, Pulmonology/Lung Surgery, Rehabilitation, and Urology. New York Eye and Ear Infirmary of Mount Sinai is ranked No. 12 in Ophthalmology. U.S. News & World Report\'s "Best Children\'s Hospitals" ranks Mount Sinai Kravis Children\'s Hospital among the country\'s best in several pediatric specialties. The Icahn School of Medicine at Mount Sinai is ranked No. 11 nationwide in National Institutes of Health funding and in the 99th percentile in research dollars per investigator according to the Association of American Medical Colleges. Newsweek\'s "The World\'s Best Smart Hospitals" ranks The Mount Sinai Hospital as No. 1 in New York and in the top five globally, and Mount Sinai Morningside in the top 20 globally.\n\nEqual Opportunity Employer\n\nThe Mount Sinai Health System is an equal opportunity employer, complying with all applicable federal civil rights laws. We do not discriminate, exclude, or treat individuals differently based on race, color, national origin, age, religion, disability, sex, sexual orientation, gender, veteran status, or any other characteristic protected by law. We are deeply committed to fostering an environment where all faculty, staff, students, trainees, patients, visitors, and the communities we serve feel respected and supported. Our goal is to create a healthcare and learning institution that actively works to remove barriers, address challenges, and promote fairness in all aspects of our organization.', 'job_highlights': [{'title': 'Qualifications', 'items': ['4+ years of relevant professional experience, preferably in data engineering, data pipeline development, and data science workflows, in a Linux environment', 'Strong knowledge of SQL and NoSQL databases, including Azure SQL, PostgreSQL/MySQL, and MongoDB or similar', 'Proficiency with at least two programming languages among Python, Scala, Java, or Go, with the flexibility to quickly learn additional languages', 'Hands-on experience working with RESTful APIs and services, preferably using Node.js, Django, or similar frameworks', 'Demonstrated expertise in containerization technologies such as Docker and Kubernetes', 'Solid understanding and practical experience with orchestration and automation tools like Airflow or Dataiku', 'Experience in developing, deploying, and maintaining AI workflows, including computer vision/image processing, NLP, and data categorization models', 'Proficiency in using DevOps tools and practices, including Git, CI/CD tools, automated unit testing, and GitHub Copilot or similar AI-assisted coding tools (e.g., ChatGPT)', 'Experience building scalable data architectures with reference tables and automated AI-driven updating mechanisms', 'Familiarity with Hadoop, Spark, Kafka, and streaming data platforms and technologies', 'Knowledge of healthcare data standards such as HL7, and familiarity with integration tools like Mirth is a significant plus', 'Strong knowledge of Azure cloud data services, including Azure Databricks, Azure Fabric, Azure Data Factory, serverless computing, virtual machines, and cloud security', 'Strong knowledge of data visualization tools such as Power BI or Tableau', 'Strong knowledge of data structures, data formats, algorithms, and object-oriented design and practical experience with data serialization and storage formats such as Parquet, Avro, JSON, or ORC']}, {'title': 'Benefits', 'items': ['The Mount Sinai Health System (MSHS) provides salary ranges that comply with the New York City Law on Salary Transparency in Job Advertisements', 'The salary range for the role is $90,000.00 - $135,285.00 Annually', 'Actual salaries depend on a variety of factors, including experience, education, and operational need', 'The salary range or contractual rate listed does not include bonuses/incentive, differential pay or other forms of compensation or benefits']}, {'title': 'Responsibilities', 'items': ['The Data Engineer II is responsible for designing, developing, and maintaining robust, scalable, and secure data architectures that facilitate efficient data collection, transformation, storage, analysis, and self-service access', 'Leveraging modern tools and methodologies, the Data Engineer II manages transformation logic, data models, and metric definitions, develops automated pipelines, and supports AI-driven workflows', 'By ensuring data reliability and quality, this role directly contributes to enhancing patient care delivery, clinical pathways, translational research, and overall clinical care', 'The Data Engineer II collaborates closely with business teams, data scientists, machine learning engineers, and backend developers to deliver impactful data solutions', 'Design, develop, and maintain scalable and reliable data pipelines using orchestration engines such as Airflow and Dataiku, ensuring seamless automation of data ingestion, transformation, and delivery processes', 'Deploy and maintain containerized applications and pipelines, employing technologies like Docker and Kubernetes to achieve resilient and maintainable data workflows', 'Develop, deploy, and operationalize AI workflows, including image processing, data categorization, and natural language processing (NLP) models, ensuring production-level reliability and performance', 'Implement DevOps best practices, including version control with Git, continuous integration and continuous deployment (CI/CD) pipelines, automated testing frameworks, and unit testing to facilitate rapid, reliable, and high-quality software deployments', 'Create and manage a scalable and maintainable data architecture, including designing reference tables and deploying AI-driven mechanisms to ensure reference tables remain accurate and current', 'Develop and maintain comprehensive data dictionaries, enforce data quality metrics, implement anomaly detection solutions, and establish atomic rollback processes to effectively manage and rectify data errors', 'Build robust data ingestion pipelines capable of handling diverse data sources, including AI-generated data, flat files, and RESTful APIs (both reading from and writing to endpoints)', 'Collaborate closely with agile teams consisting of Application Developers, Database Developers, and Data Scientists, actively participating in sprint planning, stand-ups, and retrospectives', 'Create centralized documentation, diagrams, and metadata catalogs that clearly describe data solutions, facilitating knowledge sharing and ease of use', 'Design, implement, and manage data system monitoring, backups, and disaster recovery plans, safeguarding the integrity, availability, and security of data', 'Engage with stakeholders with a customer-focused approach, delivering solutions that align with scientific, research, and clinical objectives', 'Ensure adherence to industry best practices, HIPAA compliance, and institutional data governance policies and procedures', 'Maintain current knowledge of industry trends and emerging technologies, demonstrating flexibility and continuous learning to adapt and enhance skillsets relevant to data engineering', 'Develop standards and best practices documentation related to data management, architecture, and maintenance, and provide training and presentations to team members as required', 'Maintains strong desire to understand the "why" behind data']}], 'apply_options': [{'title': 'Ladders', 'link': 'https://www.theladders.com/job/data-engineer-ii-enterprise-data-analytics-digital-and-technology-partners-remote-mountsinaihospital-new-york-ny_80710145?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIElJIC0gRW50ZXJwcmlzZSBEYXRhIFx1MDAyNiBBbmFseXRpY3MgLSBEaWdpdGFsIGFuZCBUZWNobm9sb2d5IFBhcnRuZXJzIC0gUmVtb3RlIiwiY29tcGFueV9uYW1lIjoiTW91bnQgU2luYWkgSG9zcGl0YWwiLCJhZGRyZXNzX2NpdHkiOiJOZXcgWW9yaywgTlkiLCJodGlkb2NpZCI6InEyd0JyNXFNWnRJMmxzb0ZBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Senior Data Engineer - Next Generation Big Data', 'company_name': 'American Express', 'location': 'Phoenix, AZ', 'via': 'Eightfold - Eightfold AI', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=_hv1gMkjqB7l3NubAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWOsQrCMBCGce0juHiz2EYEF50q1oKDCG4ucg1HEmnvQi5DnsjntOLyL9_3wV99FtXlQRwkwRkzQscuMFGCGm5UMvTElDAHYTgF93dquMoASpishxn0Im6k5dHnHPVgjOrYOM1zZRsrkxGmQYp5y6C_eanHRHHETK_dfluayG69aidKwSJDV2IiVQgMdy_ztbKB9vkF24KDiKkAAAA&shmds=v1_AdeF8KhIaWgMPz7RQUXh5knkdzQ15dmL0jBuq4MC2NqT1uHX2w&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=_hv1gMkjqB7l3NubAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965bd73d224b1cfd371f5/images/10ad8df2269535f2af61d011426a1e2d26236d1d8ca627f1cc7c1b1b447021b4.png', 'extensions': ['Full-time', 'No degree mentioned', 'Health insurance', 'Dental insurance'], 'detected_extensions': {'schedule_type': 'Full-time', 'qualifications': 'No degree mentioned', 'health_insurance': True, 'dental_coverage': True}, 'description': "At American Express, our culture is built on a 175-year history of innovation, shared values and Leadership Behaviors, and an unwavering commitment to back our customers, communities, and colleagues. As part of Team Amex, you'll experience this powerful backing with comprehensive support for your holistic well-being and many opportunities to learn new skills, develop as a leader, and grow your career.\n\nHere, your voice and ideas matter, your work makes an impact, and together, you will help us define the future of American Express.\n\nJoining Amex Tech means discovering and shaping your contribution to something big. Here, you can work alongside talented tech teams and build a unique career with the Powerful Backing of American Express. With a range of opportunities to work with the latest technologies, and a commitment to back the broader engineering community through open source, our mission is to power your success. Because Amex Tech is powered by our technology, our culture, and our colleagues.\n\nLUMI is companyâ€™s largest Big Data Platform, ideally suited for computationally and/or data intensive processing applications. Whether the data needs to be processed in batch, online, or streaming manner, Lumi provides robust capabilities to handle such workloads effectively, in a cost-efficient manner.\n\nA hub of very hardworking Big Data engineers and most exciting & upcoming technologies. Cornerstone platform offers an environment where Engineers are challenged every day to build world class products.\n\nAs we embark on the journey to move to public cloud - GCP you will be part of a fast-paced Agile team, design, develop, test, troubleshoot & optimize solutions created to simplify access to the Amexâ€™s Big Data Platform.\n\nFocus:\n\nDesigns, develops, solves problems, debugs, evaluates, modifies, deploys, and documents software and systems that meet the needs of customer-facing applications, business applications, and/or internal end user applications.\n\nOrganizational Context:\n\nMember of an engineering or delivery and integration team reporting to an Engineering manager or Engineering Director.\n\nResponsibilities:\nâ€¢ Implement scalable and efficient data architectures on GCP\nâ€¢ Collaborate with cross-functional teams to understand data requirements and develop solutions that meet business needs\nâ€¢ Data Pipeline Development:\nâ€¢ Build, test, and deploy data pipelines to move, transform, and process data from various sources to GCP\nâ€¢ Ensure the reliability, scalability, and performance of data pipelines\nâ€¢ Utilize GCP's big data technologies such as BigQuery, Dataflow, Dataprep, and Pub/Sub to implement effective data processing solutions\nâ€¢ Monitor system performance and proactively optimize data pipelines for efficiency\nâ€¢ Troubleshoot and resolve issues\nâ€¢ Create and maintain comprehensive documentation for tools, architecture, processes, and solutions\n\nQualifications:\nâ€¢ Understanding of GCP services Cloud dataflow, Cloud Pub-Sub, Big Query, Cloud Storage, Cloud Dataflow, Google Composer etc\nâ€¢ Strong SQL knowledge\nâ€¢ Understanding of fundamentals of Git and Git workflows\nâ€¢ Experience of working in agile application development environment\nâ€¢ Technical support to applications on trouble shooting Environment, software and application-level issues Write, test programs using Unix Shell scripting, oracle PL/SQL programming\nâ€¢ Experience of supporting platform Engineering Activities, Network, firewall\n\nSalary Range: $120,000.00 to $210,000.00 annually + bonus + benefits\n\nThe above represents the expected salary range for this job requisition. Ultimately, in determining your pay, weâ€™ll consider your location, experience, and other job-related factors.\n\nWe back you with benefits that support your holistic well-being so you can be and deliver your best. This means caring for you and your loved ones' physical, financial, and mental health, as well as providing the flexibility you need to thrive personally and professionally:\nâ€¢ Competitive base salaries\nâ€¢ Bonus incentives\nâ€¢ 6% Company Match on retirement savings plan\nâ€¢ Free financial coaching and financial well-being support\nâ€¢ Comprehensive medical, dental, vision, life insurance, and disability benefits\nâ€¢ Flexible working model with hybrid, onsite or virtual arrangements depending on role and business need\nâ€¢ 20+ weeks paid parental leave for all parents, regardless of gender, offered for pregnancy, adoption or surrogacy\nâ€¢ Free access to global on-site wellness centers staffed with nurses and doctors (depending on location)\nâ€¢ Free and confidential counseling support through our Healthy Minds program\nâ€¢ Career development and training opportunities\n\nFor a full list of Team Amex benefits, visit our Colleague Benefits Site.\n\nAmerican Express is an equal opportunity employer and makes employment decisions without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran status, disability status, age, or any other status protected by law. American Express will consider for employment all qualified applicants, including those with arrest or conviction records, in accordance with the requirements of applicable state and local laws, including, but not limited to, the California Fair Chance Act, the Los Angeles County Fair Chance Ordinance for Employers, and the City of Los Angelesâ€™ Fair Chance Initiative for Hiring Ordinance. For positions covered by federal and/or state banking regulations, American Express will comply with such regulations as it relates to the consideration of applicants with criminal convictions.\n\nWe back our colleagues with the support they need to thrive, professionally and personally. That's why we have Amex Flex, our enterprise working model that provides greater flexibility to colleagues while ensuring we preserve the important aspects of our unique in-person culture. Depending on role and business needs, colleagues will either work onsite, in a hybrid model (combination of in-office and virtual days) or fully virtually.\n\nUS Job Seekers - Click to view the â€œKnow Your Rightsâ€ poster. If the link does not work, you may access the poster by copying and pasting the following URL in a new browser window: https://www.eeoc.gov/poster\n\nDepending on factors such as business unit requirements, the nature of the position, cost and applicable laws, American Express may provide visa sponsorship for certain positions", 'job_highlights': [{'title': 'Qualifications', 'items': ['Understanding of GCP services Cloud dataflow, Cloud Pub-Sub, Big Query, Cloud Storage, Cloud Dataflow, Google Composer etc', 'Strong SQL knowledge', 'Understanding of fundamentals of Git and Git workflows', 'Experience of working in agile application development environment', 'Technical support to applications on trouble shooting Environment, software and application-level issues Write, test programs using Unix Shell scripting, oracle PL/SQL programming', 'Experience of supporting platform Engineering Activities, Network, firewall']}, {'title': 'Benefits', 'items': ['Salary Range: $120,000.00 to $210,000.00 annually + bonus + benefits', 'The above represents the expected salary range for this job requisition', 'Ultimately, in determining your pay, weâ€™ll consider your location, experience, and other job-related factors', 'We back you with benefits that support your holistic well-being so you can be and deliver your best', 'Competitive base salaries', 'Bonus incentives', '6% Company Match on retirement savings plan', 'Free financial coaching and financial well-being support', 'Comprehensive medical, dental, vision, life insurance, and disability benefits', 'Flexible working model with hybrid, onsite or virtual arrangements depending on role and business need', '20+ weeks paid parental leave for all parents, regardless of gender, offered for pregnancy, adoption or surrogacy', 'Free access to global on-site wellness centers staffed with nurses and doctors (depending on location)', 'Free and confidential counseling support through our Healthy Minds program', 'Career development and training opportunities']}, {'title': 'Responsibilities', 'items': ['Designs, develops, solves problems, debugs, evaluates, modifies, deploys, and documents software and systems that meet the needs of customer-facing applications, business applications, and/or internal end user applications', 'Member of an engineering or delivery and integration team reporting to an Engineering manager or Engineering Director', 'Implement scalable and efficient data architectures on GCP', 'Collaborate with cross-functional teams to understand data requirements and develop solutions that meet business needs', 'Data Pipeline Development:', 'Build, test, and deploy data pipelines to move, transform, and process data from various sources to GCP', 'Ensure the reliability, scalability, and performance of data pipelines', "Utilize GCP's big data technologies such as BigQuery, Dataflow, Dataprep, and Pub/Sub to implement effective data processing solutions", 'Monitor system performance and proactively optimize data pipelines for efficiency', 'Troubleshoot and resolve issues', 'Create and maintain comprehensive documentation for tools, architecture, processes, and solutions']}], 'apply_options': [{'title': 'Eightfold - Eightfold AI', 'link': 'https://aexp.eightfold.ai/careers/job/37818384-senior-data-engineer-next-generation-big-data-phoenix-arizona-united-states?domain=aexp.com&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'ZipRecruiter', 'link': 'https://www.ziprecruiter.com/c/American-Express/Job/Senior-Data-Engineer-Next-Generation-Big-Data/-in-Phoenix,AZ?jid=6da26f56b805231d&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'LazyApply', 'link': 'https://lazyapply.com/jobpreview/senior-data-engineer-next-generation-big-data-americanexpress-phoenix-az_83233214?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobs.weekday.works', 'link': 'https://jobs.weekday.works/american-express-senior-data-engineer---next-generation-big-data?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'BeBee', 'link': 'https://us.bebee.com/job/884d95d9f5ef7703a1db080e6729677b?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobs And Careers', 'link': 'https://www.career.com/job/american-express/senior-data-engineer-next-generation-big-data/j202509081823379424854?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Sift Jobs', 'link': 'https://sift10x.com/jobs/54827-senior-data-engineer-next-generation-big-american-express-phoenix?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobs Trabajo.org', 'link': 'https://us.trabajo.org/job-1898-0f1e80530913de5a17c8e1624c29927c?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBFbmdpbmVlciAtIE5leHQgR2VuZXJhdGlvbiBCaWcgRGF0YSIsImNvbXBhbnlfbmFtZSI6IkFtZXJpY2FuIEV4cHJlc3MiLCJhZGRyZXNzX2NpdHkiOiJQaG9lbml4LCBBWiIsImh0aWRvY2lkIjoiX2h2MWdNa2pxQjdsM051YkFBQUFBQT09IiwiaGwiOiJlbiJ9'}, {'title': 'Data Engineer (Actuarial Team)', 'company_name': 'Richmond National', 'location': 'Glen Allen, VA', 'via': 'Acturhire', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=ZiNWZIIzas4hWbCPAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWNsQrCMBBAce0nOB24qGgjQhedCorg4CDiWq7xSCLpXUlO6K_4t8blbe-96jurmhMqwpldYKIEy9bqB1PACA_CYQVbuEoPmTBZD8JwEXGR5kevOuaDMTnH2mVFDba2Mhhh6mUyb-nzH132mGiMqNTtm91Uj-zWi3uwfhB-wa14wmUWSjkSQxsLN_BsfyOm9SucAAAA&shmds=v1_AdeF8Kh_UJPqkfGGYwL0pL8JomjNfXOcBEPbmywAKp3Ti_0o5w&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=ZiNWZIIzas4hWbCPAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965bd73d224b1cfd371f5/images/10ad8df2269535f2a174a5ea2605c81823f8a1d2e728c52feebd5bc4ca8da593.png', 'extensions': ['Full-time', 'No degree mentioned', 'Health insurance', 'Dental insurance', 'Paid time off'], 'detected_extensions': {'schedule_type': 'Full-time', 'qualifications': 'No degree mentioned', 'health_insurance': True, 'dental_coverage': True, 'paid_time_off': True}, 'description': "We are seeking a skilled Data Engineer to join Richmond National in our Actuarial Department. As a Data Engineer, you will work closely with the Chief Actuary and collaborate with cross-functional teams to develop solutions that enable data-driven decision-making across the organization.\n\nResponsibilities:\nâ€¢ Data Infrastructure: Support and maintain the organization's cloud-based data infrastructure, including data pipelines, databases, and data lakes, to support the organization's data needs.\nâ€¢ ETL Development: Develop and optimize Extract, Transform, Load (ETL) processes to extract data from various sources, transform it into a consistent format, and load it into the data ecosystem.\nâ€¢ Data Quality: Adhere to data quality standards and perform regular data quality checks to ensure the accuracy, consistency, and reliability of data.\nâ€¢ Performance Optimization: Identify and implement performance tuning strategies to enhance the efficiency and speed of data processing and analysis.\nâ€¢ Data Security: Collaborate with the data and IT leadership team to implement data privacy and security measures, ensuring compliance with regulations and standards.\nâ€¢ Communication: Collaborate with cross-functional teams to understand the data needs and deliver data solutions that support their requirements.\nâ€¢ Continuous Improvement: Stay up to date with industry trends, emerging technologies, and best practices in data engineering, and proactively introduce new tools and techniques to improve data engineering processes and efficiency.\nâ€¢ Documentation: Maintain comprehensive documentation of data pipelines, workflows, and data structures, ensuring clear documentation for both technical and non-technical stakeholders.\n\nQualifications:\nâ€¢ Proven experience (3+ years) as a Data Engineer, preferably in the P&C insurance industry.\nâ€¢ Strong expertise in data modeling, ETL development (ideally DBT), and data integration techniques.\nâ€¢ Proficiency in programming languages such as Python, Java, or Scala, and expertise with SQL and database technologies.\nâ€¢ Experience with cloud-based data platforms and services, such as AWS, Azure, or Google Cloud Platform.\nâ€¢ Familiarity with data visualization tools and techniques.\nâ€¢ Excellent problem-solving skills and a strong attention to detail.\nâ€¢ Effective communication and collaboration skills to work with cross-functional teams and stakeholders.\n\nBenefits Overview\nâ€¢ Medical, Dental, and Vision insurance plans. FSA/HSA plans.\nâ€¢ Basic Life/AD&D/Short Term/Long Term Disability coverage.\nâ€¢ Matching 401k: 100% match on first 3%, 50% match on next 3%.\nâ€¢ Flexible PTO plan, 12 paid company-wide holidays, plus your birthday off.\nâ€¢ Recognized as a Top Workplace by Richmond Times-Dispatch\n\nEqual Employment Opportunity (EEO)\n\nRichmond National is an equal employment opportunity employer, the Company's employment decisions and practices are not and will not be unlawfully influenced or affected by race, color, creed, age, religion, national origin, sex, disability, genetic information, veteran status, uniformed services, sexual orientation (including transgender status, gender identity or expression), gender, traits historically associated with race, such as hairstyle, pregnancy, childbirth, or related medical conditions or on any other characteristic protected by applicable federal, state, or local law. This policy of equal employment opportunity applies to all policies and procedures relating to recruitment and hiring, compensation, benefits, and all other terms and conditions of employment.", 'job_highlights': [{'title': 'Qualifications', 'items': ['Proven experience (3+ years) as a Data Engineer, preferably in the P&C insurance industry', 'Strong expertise in data modeling, ETL development (ideally DBT), and data integration techniques', 'Proficiency in programming languages such as Python, Java, or Scala, and expertise with SQL and database technologies', 'Experience with cloud-based data platforms and services, such as AWS, Azure, or Google Cloud Platform', 'Familiarity with data visualization tools and techniques', 'Excellent problem-solving skills and a strong attention to detail', 'Effective communication and collaboration skills to work with cross-functional teams and stakeholders']}, {'title': 'Benefits', 'items': ['Medical, Dental, and Vision insurance plans', 'FSA/HSA plans', 'Basic Life/AD&D/Short Term/Long Term Disability coverage', 'Matching 401k: 100% match on first 3%, 50% match on next 3%', 'Flexible PTO plan, 12 paid company-wide holidays, plus your birthday off']}, {'title': 'Responsibilities', 'items': ['As a Data Engineer, you will work closely with the Chief Actuary and collaborate with cross-functional teams to develop solutions that enable data-driven decision-making across the organization', "Data Infrastructure: Support and maintain the organization's cloud-based data infrastructure, including data pipelines, databases, and data lakes, to support the organization's data needs", 'ETL Development: Develop and optimize Extract, Transform, Load (ETL) processes to extract data from various sources, transform it into a consistent format, and load it into the data ecosystem', 'Data Quality: Adhere to data quality standards and perform regular data quality checks to ensure the accuracy, consistency, and reliability of data', 'Performance Optimization: Identify and implement performance tuning strategies to enhance the efficiency and speed of data processing and analysis', 'Data Security: Collaborate with the data and IT leadership team to implement data privacy and security measures, ensuring compliance with regulations and standards', 'Communication: Collaborate with cross-functional teams to understand the data needs and deliver data solutions that support their requirements', 'Continuous Improvement: Stay up to date with industry trends, emerging technologies, and best practices in data engineering, and proactively introduce new tools and techniques to improve data engineering processes and efficiency', 'Documentation: Maintain comprehensive documentation of data pipelines, workflows, and data structures, ensuring clear documentation for both technical and non-technical stakeholders']}], 'apply_options': [{'title': 'Acturhire', 'link': 'https://www.acturhire.com/jobs/data-engineer-actuarial-team?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChBY3R1YXJpYWwgVGVhbSkiLCJjb21wYW55X25hbWUiOiJSaWNobW9uZCBOYXRpb25hbCIsImFkZHJlc3NfY2l0eSI6IkdsZW4gQWxsZW4sIFZBIiwiaHRpZG9jaWQiOiJaaU5XWklJemFzNGhXYkNQQUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': 'REL Associate Data Engineer - Hybrid', 'company_name': 'CC Pace Systems', 'location': 'Tysons, VA', 'via': 'Adzuna', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=TjyBCil6bravi59eAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNsQrCMBCAYVz7CIJws2gjgotOpRZFHETFtVzikUbau5LL0D6Kb2tdfr7tz76z7HCvrlCoiguYCI6YECr2gYkirOE82hjeEy5iQQmja0AYTiK-pfmhSanXvTGqbe41YQoud9IZYbIymI9Y_afWBiP17XSot7vNkPfsl4uyhBs6gseoiTqFwPAcVVhX8Cp-10LlyJwAAAA&shmds=v1_AdeF8Kj9PZGsfFleTpqs0NfsvJWgPnG28Oa4dnW3-NrNh1p8xQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=TjyBCil6bravi59eAAAAAA%3D%3D', 'extensions': ['8 days ago', 'Full-time and Contractor'], 'detected_extensions': {'posted_at': '8 days ago', 'schedule_type': 'Full-time and Contractor'}, 'description': "Job Description:\nâ€¢ Location: Candidate can be located in Pensacola, FL, Winchester, VA, or Vienna, VA\nâ€¢ Hybrid with the expectation to report onsite a minimum of 2 days per week.\nâ€¢ Contract Length: 6 months with the possibility of extension\n\nThe Associate Data Engineer will design, develop, and optimize data pipelines and systems for efficient data collection, integration, and storage. This role will interpret data to generate insights and implement reliable data processing workflows that ensure data quality and system stability. The position requires applying data governance practices across ETL pipelines, metadata management, security, and lineage tracking. The ideal candidate will develop understanding of business objectives to align data solutions and address increasingly complex data challenges while building proficiency in standard procedures and techniques.\n\nResponsibilities:\nâ€¢ Develop and maintain data pipelines and workflows using Data Factory and SQL Server Integration Services.\nâ€¢ Format data from APIs into SQL servers.\nâ€¢ Collaborate with stakeholders to gather data requirements and ensure alignment with business needs and objectives.\nâ€¢ Design and build data pipelines for data collection, connection, and storage.\nâ€¢ Develop and enforce data engineering policies and best practices to ensure data integrity.\nâ€¢ Proficient in querying MongoDB and integrating data across database systems for efficient data retrieval and processing.\nâ€¢ Ensure data quality and performance at scale.\nâ€¢ Ensure high performance and scalability of data systems and processes.\nâ€¢ Help with the collection, connection, and storage of data.\nâ€¢ Optimize existing data workflows and ensure system reliability.\nâ€¢ Collaborate with team members and participate in team projects and initiatives.\n\nQualifications:\nâ€¢ Bachelor's degree in Computer Science, Engineering, Information Systems, or a related field.\nâ€¢ 3-5 years of experience in data engineering and governance.\nâ€¢ Advanced knowledge of SQL language.\nâ€¢ Advanced understanding of API frameworks and JSON transformations.\nâ€¢ Understanding of various data warehousing architectures (Mongo, Azure, SQL, etc.).\nâ€¢ Basic understanding of business and operating environment.\nâ€¢ Basic knowledge of programming languages (Mongo, Python, PowerShell, R, SQL, C#).\nâ€¢ Experience with data management, processing and analytic tools (Databricks).\nâ€¢ Experience with data integration and processing tools.\nâ€¢ Basic problem-solving skills.\nâ€¢ Basic communication and collaboration skills.\nâ€¢ Working experience with data warehousing solutions, ETL tools, and data system design.\nâ€¢ Basic knowledge of data engineering principles, technologies, and architecture.\n\nCC Pace is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. We do not discriminate on the basis of race, color, religion, sex, national origin, age, disability, genetic information, or any other protected characteristic under federal, state, or local laws.\n\nCC Pace are committed to employing only candidates who are legally authorized to work in the United States. For us to comply with the Immigration Reform and Control Act of 1986, all new employees, as a condition of employment, must complete the Employment Eligibility Verification Form I-9 and provide documentation that establishes identity and authorization to work. E-Verify will be used for employment verification as part of your onboarding process.\n\nCC Pace values integrity throughout our hiring process. As part of our standard verification procedures, candidates will be asked to provide documentation confirming employment history, education, and work authorization.", 'job_highlights': [{'title': 'Qualifications', 'items': ['The position requires applying data governance practices across ETL pipelines, metadata management, security, and lineage tracking', "Bachelor's degree in Computer Science, Engineering, Information Systems, or a related field", '3-5 years of experience in data engineering and governance', 'Advanced knowledge of SQL language', 'Advanced understanding of API frameworks and JSON transformations', 'Understanding of various data warehousing architectures (Mongo, Azure, SQL, etc.)', 'Basic understanding of business and operating environment', 'Basic knowledge of programming languages (Mongo, Python, PowerShell, R, SQL, C#)', 'Experience with data management, processing and analytic tools (Databricks)', 'Experience with data integration and processing tools', 'Basic problem-solving skills', 'Basic communication and collaboration skills', 'Working experience with data warehousing solutions, ETL tools, and data system design', 'Basic knowledge of data engineering principles, technologies, and architecture', 'For us to comply with the Immigration Reform and Control Act of 1986, all new employees, as a condition of employment, must complete the Employment Eligibility Verification Form I-9 and provide documentation that establishes identity and authorization to work']}, {'title': 'Responsibilities', 'items': ['Location: Candidate can be located in Pensacola, FL, Winchester, VA, or Vienna, VA', 'Hybrid with the expectation to report onsite a minimum of 2 days per week', 'Contract Length: 6 months with the possibility of extension', 'The Associate Data Engineer will design, develop, and optimize data pipelines and systems for efficient data collection, integration, and storage', 'This role will interpret data to generate insights and implement reliable data processing workflows that ensure data quality and system stability', 'The ideal candidate will develop understanding of business objectives to align data solutions and address increasingly complex data challenges while building proficiency in standard procedures and techniques', 'Develop and maintain data pipelines and workflows using Data Factory and SQL Server Integration Services', 'Format data from APIs into SQL servers', 'Collaborate with stakeholders to gather data requirements and ensure alignment with business needs and objectives', 'Design and build data pipelines for data collection, connection, and storage', 'Develop and enforce data engineering policies and best practices to ensure data integrity', 'Proficient in querying MongoDB and integrating data across database systems for efficient data retrieval and processing', 'Ensure data quality and performance at scale', 'Ensure high performance and scalability of data systems and processes', 'Help with the collection, connection, and storage of data', 'Optimize existing data workflows and ensure system reliability', 'Collaborate with team members and participate in team projects and initiatives']}], 'apply_options': [{'title': 'Adzuna', 'link': 'https://www.adzuna.com/details/5487450625?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJSRUwgQXNzb2NpYXRlIERhdGEgRW5naW5lZXIgLSBIeWJyaWQiLCJjb21wYW55X25hbWUiOiJDQyBQYWNlIFN5c3RlbXMiLCJhZGRyZXNzX2NpdHkiOiJUeXNvbnMsIFZBIiwiaHRpZG9jaWQiOiJUanlCQ2lsNmJyYXZpNTllQUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': 'Sr Data Engineer II-Manufacturing Systems', 'company_name': 'Alkermes, Inc.', 'location': 'Wilmington, OH', 'via': 'Indeed', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=X827wOOnqCJ4MMPSAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNsQrCMBAAUFz7CU43S9uI4KKLgqIVxKGDY0nDmUbTu5K7Qv0c_1Rd3vqyzyzb1QkOVi0cyQdCTFBVxdXS-LBOxxTIQ_0WxV6ggAu3IGiT64AJTsw-4nzbqQ6yMUYkll7UanCl494wYcuTeXIrfxrpbMIhWsVmtV5O5UB-Afv4wtSj5FCRKyEQ3EPsf60y5XA7fwFXbeB3pAAAAA&shmds=v1_AdeF8KjBEqXvJ8oUBmljLLyI4sON87zSWOGN4CxI64PzAmNM4g&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=X827wOOnqCJ4MMPSAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965bd73d224b1cfd371f5/images/10ad8df2269535f2a66682c0d2dc2ba032c408bd7b9559d27bd01fd34ce26ea5.jpeg', 'extensions': ['24 days ago', 'Full-time'], 'detected_extensions': {'posted_at': '24 days ago', 'schedule_type': 'Full-time'}, 'description': "Job Summary:\n\nThe Senior Data Engineer II will lead manufacturing data collection and processing strategies across the Wilmington facility. This role is responsible for designing, deploying, and maintaining raw data ingestion and time-series data collection configurations, databases, reporting applications, and visualization tools that support manufacturing operations. The successful candidate will build strategic business partnerships, drive business value, and foster a culture of innovation and compliance.\n\nJob Duties:\nâ€¢ Design, document, deploy, and verify data acquisition tags (alarms, events, time-series data) for manufacturing systems.\nâ€¢ Develop, deploy, and maintain SQL-based databases for ingesting raw manufacturing data.\nâ€¢ Create and manage metadata structures to contextualize manufacturing data.\nâ€¢ Administer manufacturing reporting applications and ensure their reliability and performance.\nâ€¢ Lead strategic vendor and partner engagements to support reporting, data acquisition, and database systems.\nâ€¢ Build and maintain visualization dashboards to enable data-driven decision-making for manufacturing teams.\nâ€¢ Manage all data-related projects within the IT Manufacturing Systems department, including during bi-annual facility shutdowns.\nâ€¢ Gather and translate user requirements into technical specifications and project deliverables.Collaborate with validation and quality teams to ensure compliance with regulatory standards (e.g., GxP).\n\nMinimum Education & Experience Requirements:\nâ€¢ Bachelorâ€™s Degree in Information Systems, Engineering, Computer Science, or a related field.\nâ€¢ Minimum of 8 years of professional experience in Data Engineering or Automation Systems.Experience in the pharmaceutical or biotech industry strongly preferred.\n\nKnowledge/Skills Needed:\nâ€¢ Expertise in time-series data systems and SQL database development.\nâ€¢ Strong understanding of manufacturing systems and industrial data flows.\nâ€¢ Experience with data visualization tools and reporting platforms.\nâ€¢ Familiarity with pharmaceutical compliance standards (e.g., GxP, validation).\nâ€¢ Proven ability to lead cross-functional teams and manage complex projects.Strong communication, documentation, and stakeholder engagement skills.\n\nCompetencies:\nâ€¢ Technical Expertise\nâ€¢ Acts as a subject matter expert for assigned technical areas and has knowledge in other key business area(s)\nâ€¢ Understands the technology from a systematic, â€œbig pictureâ€ view\nâ€¢ Manages multi-functional and multi-site teams and projects\nâ€¢ Facilitates knowledge sharing and creative problem solving with the affected business function\nâ€¢ Has a certified mastery of skills in areas such as project management and continuous/process improvement\nâ€¢ Knows and/or has experience with pharmaceutical concepts (e.g. GxP, validation)\nâ€¢ Collaboration and Communication\nâ€¢ Takes a leadership role in driving effective collaboration across IT and relevant business functions\nâ€¢ Uses effective communications to influence project stakeholders and adapt to each situation\nâ€¢ Mentors and motivates members of the project team, including cross-functional members\nâ€¢ Self-Awareness and Adaptability\nâ€¢ Serves as a leader and change agent across IT and cross-functional project teams\nâ€¢ Engages with others in IT and key business areas proactively to determine impacts of upcoming changes\nâ€¢ Drives continuous improvement and innovation within IT\nâ€¢ Sets the highest standards for personal performance regardless of project difficulty\nâ€¢ Grows knowledge in areas outside of IT\nâ€¢ Follows policies and procedures, identifies necessary changes and facilitates updates as required\nâ€¢ Decision Making\nâ€¢ Leads cross-functional project teams in problem solving\nâ€¢ Known as the trusted technical source for knowledge and advice on complex solutions and projects\nâ€¢ Applies innovative approaches and methods to complex projects and solutions\nâ€¢ Takes full accountability and responsibility for projects and decisions with minimal consultation\nâ€¢ Customer-Focus Results\nâ€¢ Anticipates major customersâ€™ needs and is committed to developing value-added solutions that meet them successfully\nâ€¢ Builds broad network of relationships with customers and proactively seeks feedback on IT products and services\n\nWhy join Team Alkermes?\n\nAlkermes applies its deep neuroscience expertise to develop medicines designed to help people living with complex and difficult-to-treat psychiatric and neurological disorders. A fully-integrated, global biopharmaceutical company, headquartered in Ireland with U.S. locations in Massachusetts and Ohio, we are committed to pursuing great science, driven by deep compassion to make a real impact in the lives of patients. Alkermes has a portfolio of proprietary commercial products for the treatment of alcohol dependence, opioid dependence, schizophrenia and bipolar I disorder, and a pipeline of clinical and preclinical candidates in development for various neurological disorders, including narcolepsy.\n\nBeyond our important mission of developing medicines to address unmet patient needs, we actively seek to foster a culture of diversity, inclusion and belonging throughout our business. We strive to ensure that all voices are respected and valued, recognizing that our diversity of thought, background and perspective makes us stronger. We are proud to have been recognized as an employer of choice by many national organizations, including being certified as a Great Place to Work in the U.S. in 2024, honored as a Healthiest Employer in both Ohio and Massachusetts in 2023, a Best Place for Working Parents in 2023, and to have received the Bell Seal at the Platinum level for Workplace Mental Health by Mental Health America for three consecutive years (2021-2023).\n\nAlkermes, Inc. is an equal employment opportunity employer and does not discriminate against any qualified applicant or employee because of race, creed, color, age, national origin, ancestry, religion, gender, sexual orientation, gender expression and identity, disability, genetic information, veteran status, military status, application for military service or any other characteristic protected by local, state or federal law. Alkermes also complies with all work authorization and employment eligibility verification requirements of the Immigration and Nationality Act and IRCA. Alkermes is an E-Verify employer.\n\nDegree LevelBachelor's Degree", 'job_highlights': [{'title': 'Qualifications', 'items': ['Bachelorâ€™s Degree in Information Systems, Engineering, Computer Science, or a related field', 'Minimum of 8 years of professional experience in Data Engineering or Automation Systems', 'Expertise in time-series data systems and SQL database development', 'Strong understanding of manufacturing systems and industrial data flows', 'Experience with data visualization tools and reporting platforms', 'Familiarity with pharmaceutical compliance standards (e.g., GxP, validation)', 'Proven ability to lead cross-functional teams and manage complex projects', 'Strong communication, documentation, and stakeholder engagement skills', 'Technical Expertise', 'Acts as a subject matter expert for assigned technical areas and has knowledge in other key business area(s)', 'Understands the technology from a systematic, â€œbig pictureâ€ view', 'Facilitates knowledge sharing and creative problem solving with the affected business function', 'Has a certified mastery of skills in areas such as project management and continuous/process improvement', 'Knows and/or has experience with pharmaceutical concepts (e.g. GxP, validation)', 'Collaboration and Communication', 'Mentors and motivates members of the project team, including cross-functional members', 'Self-Awareness and Adaptability', 'Serves as a leader and change agent across IT and cross-functional project teams', 'Sets the highest standards for personal performance regardless of project difficulty', 'Follows policies and procedures, identifies necessary changes and facilitates updates as required', 'Leads cross-functional project teams in problem solving', 'Known as the trusted technical source for knowledge and advice on complex solutions and projects', 'Applies innovative approaches and methods to complex projects and solutions']}, {'title': 'Responsibilities', 'items': ['The Senior Data Engineer II will lead manufacturing data collection and processing strategies across the Wilmington facility', 'This role is responsible for designing, deploying, and maintaining raw data ingestion and time-series data collection configurations, databases, reporting applications, and visualization tools that support manufacturing operations', 'The successful candidate will build strategic business partnerships, drive business value, and foster a culture of innovation and compliance', 'Design, document, deploy, and verify data acquisition tags (alarms, events, time-series data) for manufacturing systems', 'Develop, deploy, and maintain SQL-based databases for ingesting raw manufacturing data', 'Create and manage metadata structures to contextualize manufacturing data', 'Administer manufacturing reporting applications and ensure their reliability and performance', 'Lead strategic vendor and partner engagements to support reporting, data acquisition, and database systems', 'Build and maintain visualization dashboards to enable data-driven decision-making for manufacturing teams', 'Manage all data-related projects within the IT Manufacturing Systems department, including during bi-annual facility shutdowns', 'Gather and translate user requirements into technical specifications and project deliverables', 'Collaborate with validation and quality teams to ensure compliance with regulatory standards (e.g., GxP)', 'Manages multi-functional and multi-site teams and projects', 'Takes a leadership role in driving effective collaboration across IT and relevant business functions', 'Uses effective communications to influence project stakeholders and adapt to each situation', 'Engages with others in IT and key business areas proactively to determine impacts of upcoming changes', 'Drives continuous improvement and innovation within IT', 'Grows knowledge in areas outside of IT', 'Takes full accountability and responsibility for projects and decisions with minimal consultation', 'Anticipates major customersâ€™ needs and is committed to developing value-added solutions that meet them successfully', 'Builds broad network of relationships with customers and proactively seeks feedback on IT products and services']}], 'apply_options': [{'title': 'Indeed', 'link': 'https://www.indeed.com/viewjob?jk=b3129c2393736ff4&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'ZipRecruiter', 'link': 'https://www.ziprecruiter.com/c/Alkermes/Job/Sr-Data-Engineer-II-Manufacturing-Systems/-in-Wilmington,OH?jid=60ccb43af986b778&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Ladders', 'link': 'https://www.theladders.com/job/sr-data-engineer-ii-manufacturing-systems-alkermes-wilmington-oh_84042466?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'LinkedIn', 'link': 'https://www.linkedin.com/jobs/view/sr-data-engineer-ii-manufacturing-systems-at-talentally-4318686180?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Oracle', 'link': 'https://hbap.fa.us1.oraclecloud.com/hcmUI/CandidateExperience/en/sites/CX_1/requisitions/preview/13720?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Career.io', 'link': 'https://career.io/job/sr-data-engineer-ii-manufacturing-systems-wilmington-alkermes-356bc9a65ffc2ede75b5d9ac313f6230?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'SimplyHired', 'link': 'https://www.simplyhired.com/job/THRmTdNvUhkJ2OEIIQWq1SQqgIBTT7RR2VLzyriJyexqOnBkC8vIpw?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Glassdoor', 'link': 'https://www.glassdoor.com/job-listing/sr-data-engineer-ii-manufacturing-systems-alkermes-JV_IC1146350_KO0,41_KE42,50.htm?jl=1009915497985&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTciBEYXRhIEVuZ2luZWVyIElJLU1hbnVmYWN0dXJpbmcgU3lzdGVtcyIsImNvbXBhbnlfbmFtZSI6IkFsa2VybWVzLCBJbmMuIiwiYWRkcmVzc19jaXR5IjoiV2lsbWluZ3RvbiwgT0giLCJodGlkb2NpZCI6Ilg4Mjd3T09ucUNKNE1NUFNBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Head of Data Engineering & Analytics Consulting', 'company_name': 'Ippon Technologies USA', 'location': 'United States', 'via': 'LinkedIn', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=j2w4xrL-juSi-mQ9AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWMuwrCQBAAsc0nWG2lIJgTwUar4Ns2pg6XZL07OXeP7Arxp_xGYzPFwEz2nWSnC9oO-AEHqxaO5AIh9oEczKAgGz8aWoE9k7yj_vUSbtyAoO1bD0xwZnYRpzuvmmRrjEjMnagdu7zll2HChgfz5Eb-qMXbHlO0ivV6sxryRG4xv6Y0ru7YeuLILqBAVRYQCCoKih2U4xDlB6_YoJ6xAAAA&shmds=v1_AdeF8KinC2l7DGBbyCtxZq0fxfkTvW9jHQb_885qDSZx-XPTgg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=j2w4xrL-juSi-mQ9AAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965bd73d224b1cfd371f5/images/10ad8df2269535f2c181b68f27f4fad2ac4fa2800dc9a56d2e28fb4f119a6b8d.jpeg', 'extensions': ['22 days ago', '180Kâ€“215K a year', 'Full-time', 'Health insurance', 'Dental insurance', 'Paid time off'], 'detected_extensions': {'posted_at': '22 days ago', 'salary': '180Kâ€“215K a year', 'schedule_type': 'Full-time', 'health_insurance': True, 'dental_coverage': True, 'paid_time_off': True}, 'description': 'Weâ€™re hiring a Head of Data Engineering & Analytics Consulting!\n\nEmployment type: Full-time/Direct Hire only (No Agencies)\n\nLocation: Atlanta, GA, New York, NY, Washington, DC, Richmond, VA, Charlotte, NC, Metropolitan areas\n\nLocation Type: Hybrid - Home Office/Local Ippon office (1-2 days per week onsite)\n\nAbout Ippon:\n\nThe Ippon story started in 2002 in Paris, France - cue in the accordion. Our founder and CEO StÃ©phane Nomis used his unique experience as an international Judo champion to create a culture based on the sports values of ambition and excellence. Our USA Headquarters office is located in Richmond, VA, with additional offices in New York, NY, Washington DC, and Atlanta, GA. We are a technology consulting firm that specializes in helping accelerate our clientâ€™s digital roadmap in the areas of Product Management, Cloud, Data, and Software Engineering.\n\nAbout the position:\n\nIppon is seeking an experienced and dynamic Head of Data Engineering & Analytics Consulting to lead our fastest-growing practice, with a strategic focus on cloud-native, Snowflake-centered solutions. The Head of Data Engineering & Analytics Consulting manages a team of highly-skilled data engineering and analytics consultants, and is a hands-on consulting leader with technical, business, and people leadership responsibilities. This role acts as the subject matter expert and thought leadership advocate for Ipponâ€™s services, responsible for developing, managing, and expanding the practice, and demonstrating standards for collaboration and drive.\n\nRoles and Responsibilities:\n\nPractice & Growth Leadership:\nâ€¢ Drive and evolve Ipponâ€™s Data Engineering & Analytics practice with a clear focus on Snowflake and Cloud-native architectures.\nâ€¢ Hire, mentor, and retain an inclusive, high-performance culture grounded in craft, consulting excellence, experimentation, and continuous learning.\nâ€¢ Partner with our Sales teams and account management to identify opportunities and shape customer roadmaps.\nâ€¢ Develop and foster relationships with Snowflake corporate leaders, account executives, program managers, and sales engineers.\nâ€¢ Represent Ippon as a thought leader in the Snowflake ecosystem, speaking at events, publishing blogs and papers, and contributing to the community.\nâ€¢ Coach team members on effective technical and consulting skills and strategies.\nâ€¢ Promote knowledge sharing through workshops, internal learning sessions, and mentorship programs.\nâ€¢ Partner with HR and senior leadership to develop learning and development content for the practice\n\nTechnical Strategy & Architecture\nâ€¢ Drive the creation of reference implementations, accelerators, and best practices for Snowflake Data Clouds\nâ€¢ Assure security practices such as secrets management, access control, and audit logging are considered in solutions.\nâ€¢ Ensure modern standards in orchestration, IaC, observability, and FinOps, primarily on AWS.\nâ€¢ Drive AI and machine learning Data initiatives, identifying opportunities to integrate AI-driven solutions such as Cortex, Streamlit, and external services within client environments to optimize performance and business outcomes.\n\nProject Delivery & Pre-Sales:\nâ€¢ Provide executive oversight on multi-team Snowflake programs across industries.\nâ€¢ Ensure cloud data engineering projects are delivered on time, within scope, and meet quality standards.\nâ€¢ Serve as a trusted senior consultant to our clients and deliver high-quality data architecture and engineering solutions.\nâ€¢ Build repeatable methods and templates that continuously improve delivery quality.\nâ€¢ Assist account teams with strategic account growth by identifying new opportunities within client portfolios.\nâ€¢ Support pre-sales efforts and solution design alongside account executives to expand our client base.\n\nCompetencies we are looking for:\n\nMinimum Qualifications:\nâ€¢ Bachelorâ€™s degree in Computer Science, Data Engineering, or a related field.\nâ€¢ 10+ years of professional experience in a technical Data Engineering role utilizing technologies such as SQL and Python.\nâ€¢ 5+ years of technology consulting experience.\nâ€¢ 3+ years building with Snowflake as a primary platform (bonus: Cortex, Streamlit, Snowpark).\nâ€¢ 3+ years leading teams and/or in a technical manager role.\nâ€¢ 3+ years working with modern cloud platforms, including AWS, Azure, or Google Cloud (AWS preferred).\nâ€¢ Familiarity with other cloud data platforms such as Databricks, Microsoft Fabric, Redshift, and BigQuery\nâ€¢ Familiarity with DBT, Airflow, Terraform, Kubernetes, and modern orchestration tools.\nâ€¢ Proven track record of growing and leading teams in a consulting or tech environment.\nâ€¢ Strong background in hiring, coaching, and talent development.\nâ€¢ Clear and effective communicator with strong interpersonal and writing skills.\nâ€¢ Experience managing managers.\nâ€¢ Familiarity with Agile development practices.\n\nPreferred Qualifications:\nâ€¢ Hands-on understanding of AI/ML and LLM architectures; Snowflake Cortex experience preferred.\nâ€¢ Experience leading multi-team delivery programs and optimizing team performance.\nâ€¢ Experience with business development, sales support, and proposal creation.\nâ€¢ Understanding of operational KPIs and small organizational management practices.\nâ€¢ Strong decision-making, stakeholder management, and executive communication skills.\nâ€¢ Experience presenting at Snowflake events or publishing Snowflake community content\n\nWhat we offer:\nâ€¢ Competitive salary â€“ Great starting salaries for well-qualified candidates\nâ€¢ Generous Paid Time Off policy - Ippon offers flexible time off to help you be your best\nâ€¢ Health, dental, and vision insurance â€“ We are dedicated to helping our employees live healthier lives through comprehensive health programs\nâ€¢ 401k with company match - Ippon offers an industry-leading 401(k) matching plan\nâ€¢ Family Leave - We support the importance of family and offer maternity, paternity, and family medical leave plans\nâ€¢ A fun, smart, and healthy work environment, focused on our values and teamwork\n\nWe value the diversity and different perspectives each of our employees bring to Ippon Technologies.\n\nIppon Technologies is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, age, national origin, veteran, or disability status.\n\nVisit us on LinkedIn or at https://ipponusa.com/ to learn more.\n\nSo, do YOU speak Ippon?', 'job_highlights': [{'title': 'Qualifications', 'items': ['Bachelorâ€™s degree in Computer Science, Data Engineering, or a related field', '10+ years of professional experience in a technical Data Engineering role utilizing technologies such as SQL and Python', '5+ years of technology consulting experience', '3+ years building with Snowflake as a primary platform (bonus: Cortex, Streamlit, Snowpark)', '3+ years leading teams and/or in a technical manager role', 'Familiarity with other cloud data platforms such as Databricks, Microsoft Fabric, Redshift, and BigQuery', 'Familiarity with DBT, Airflow, Terraform, Kubernetes, and modern orchestration tools', 'Proven track record of growing and leading teams in a consulting or tech environment', 'Strong background in hiring, coaching, and talent development', 'Clear and effective communicator with strong interpersonal and writing skills', 'Experience managing managers', 'Familiarity with Agile development practices']}, {'title': 'Benefits', 'items': ['Competitive salary â€“ Great starting salaries for well-qualified candidates', 'Generous Paid Time Off policy - Ippon offers flexible time off to help you be your best', 'Health, dental, and vision insurance â€“ We are dedicated to helping our employees live healthier lives through comprehensive health programs', '401k with company match - Ippon offers an industry-leading 401(k) matching plan', 'Family Leave - We support the importance of family and offer maternity, paternity, and family medical leave plans', 'A fun, smart, and healthy work environment, focused on our values and teamwork']}, {'title': 'Responsibilities', 'items': ['The Head of Data Engineering & Analytics Consulting manages a team of highly-skilled data engineering and analytics consultants, and is a hands-on consulting leader with technical, business, and people leadership responsibilities', 'This role acts as the subject matter expert and thought leadership advocate for Ipponâ€™s services, responsible for developing, managing, and expanding the practice, and demonstrating standards for collaboration and drive', 'Drive and evolve Ipponâ€™s Data Engineering & Analytics practice with a clear focus on Snowflake and Cloud-native architectures', 'Hire, mentor, and retain an inclusive, high-performance culture grounded in craft, consulting excellence, experimentation, and continuous learning', 'Partner with our Sales teams and account management to identify opportunities and shape customer roadmaps', 'Develop and foster relationships with Snowflake corporate leaders, account executives, program managers, and sales engineers', 'Represent Ippon as a thought leader in the Snowflake ecosystem, speaking at events, publishing blogs and papers, and contributing to the community', 'Coach team members on effective technical and consulting skills and strategies', 'Promote knowledge sharing through workshops, internal learning sessions, and mentorship programs', 'Partner with HR and senior leadership to develop learning and development content for the practice', 'Technical Strategy & Architecture', 'Drive the creation of reference implementations, accelerators, and best practices for Snowflake Data Clouds', 'Assure security practices such as secrets management, access control, and audit logging are considered in solutions', 'Ensure modern standards in orchestration, IaC, observability, and FinOps, primarily on AWS', 'Drive AI and machine learning Data initiatives, identifying opportunities to integrate AI-driven solutions such as Cortex, Streamlit, and external services within client environments to optimize performance and business outcomes', 'Provide executive oversight on multi-team Snowflake programs across industries', 'Ensure cloud data engineering projects are delivered on time, within scope, and meet quality standards', 'Serve as a trusted senior consultant to our clients and deliver high-quality data architecture and engineering solutions', 'Build repeatable methods and templates that continuously improve delivery quality', 'Assist account teams with strategic account growth by identifying new opportunities within client portfolios', 'Support pre-sales efforts and solution design alongside account executives to expand our client base']}], 'apply_options': [{'title': 'LinkedIn', 'link': 'https://www.linkedin.com/jobs/view/head-of-data-engineering-analytics-consulting-at-ippon-technologies-usa-4305361604?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Remote Jobs USA - Remotenow.mysmartprosnetwfh', 'link': 'https://remotenow.mysmartpros.com/job/880469?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Remote Jobs USA', 'link': 'https://noexperienceremotejobs.com/job/880469?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Learn4Good', 'link': 'https://www.learn4good.com/jobs/online_remote/info_technology/4545447100/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJIZWFkIG9mIERhdGEgRW5naW5lZXJpbmcgXHUwMDI2IEFuYWx5dGljcyBDb25zdWx0aW5nIiwiY29tcGFueV9uYW1lIjoiSXBwb24gVGVjaG5vbG9naWVzIFVTQSIsImFkZHJlc3NfY2l0eSI6IlVuaXRlZCBTdGF0ZXMiLCJodGlkb2NpZCI6ImoydzR4ckwtanVTaS1tUTlBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'GCP Data Engineer', 'company_name': 'Infosys', 'location': 'Blue Ridge, TX', 'via': 'WhatJobs', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=mfrhp-8GQszS1bhlAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEOwoCMRAAUGz3BlpNKaKJCDba-WHRSsTCbkmyYxKJMyETYW09ufiK13xHzbTdX-BgqoEj-UiIBRZwZguCprgATNAy-4STbag1y0ZrkaS8VFOjU45fmgktD_rJVv51EkzBnEzFbrVeDiqTn41P9GD5CESCXXojXGPvcQ63-w88ndF_hQAAAA&shmds=v1_AdeF8KgO7lhbvCFaYM7qC9Sue_GtDpdB08YerPccVDBOLi77bA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=mfrhp-8GQszS1bhlAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965bd73d224b1cfd371f5/images/10ad8df2269535f27fa1fde72fab31c11b55a771b7ce514c44ea5f6a4dde7120.png', 'extensions': ['6 days ago', 'Full-time'], 'detected_extensions': {'posted_at': '6 days ago', 'schedule_type': 'Full-time'}, 'description': 'Overview\n\nInfosys is seeking a Google Cloud (GCP) data engineer with experience in Github and python . In this role, you will enable digital transformation for our clients in a global delivery model, research on technologies independently, recommend appropriate solutions and contribute to technology-specific best practices and standards. You will be responsible to interface with key stakeholders and apply your technical proficiency across different stages of the Software Development Life Cycle. You will be part of a learning culture, where teamwork and collaboration are encouraged, excellence is rewarded, and diversity is respected and valued.\nResponsibilities\n\nThe job description text describes responsibilities as part of the role: interface with key stakeholders and apply technical proficiency across different stages of the Software Development Life Cycle in a global delivery model, enabling digital transformation and contributing to technology-specific best practices and standards.\nRequired Qualifications Candidate must be located within commuting distance ofRichardson, TX or be willing to relocate to the area. This position may require travel in the US Bachelors degree or foreign equivalent required from an accredited institution. Will also consider three years of progressive experience in the specialty in lieu of every year of education. Candidates authorized to work for any employer in the United States without employer-based visa sponsorship are welcome to apply. Infosys is unable to provide immigration sponsorship for this role at thistime At least 4years of Information Technology experience. Experience working with technologies like GCP with data engineering data flow / air flow, pub sub/ kafta, data proc/Hadoop, Big Query. ETL development experience with strong SQL background such as Python/R, Scala, Java, Hive, Spark, Kafka Strongknowledge on Python Program development to build reusable frameworks, enhance existing frameworks. Application build experience with core GCP Services like Dataproc, GKE, Composer, Deep understanding GCP IAM & Github. Musthave done IAM set up Knowledge onCICD pipeline using Terraform in Git. Preferred Qualifications Good knowledge onGoogle Big Query, using advance SQL programing techniques to build Big Query Data sets in Ingestion and Transformation layer. Experience in Relational Modeling, Dimensional Modeling and Modeling of Unstructured Data Knowledge on Airflow Dag creation, execution, and monitoring. Good understanding of Agile software development frameworks Ability to work in teams in a diverse, multi-stakeholder environment comprising of Business and Technology teams. Experience and desire towork in a global deliveryenvironment.\n\nThe job may entail extensive travel. The job may also entail sitting as well as working at a computer for extended periods of time. Candidates should be able to effectively communicate by telephone, email, and face to face.\nEEO/About Us\n\nInfosys is a global leader in next-generation digital services and consulting. We enable clients in more than 50 countries to navigate their digital transformation. With over four decades of experience in managing the systems and workings of global enterprises, we expertly steer our clients through their digital journey. We do it by enabling the enterprise with an AI-powered core that helps prioritize the execution of change. We also empower the business with agile digital at scale to deliver unprecedented levels of performance and customer delight. Our always-on learning agenda drives their continuous improvement through building and transferring digital skills, expertise, and ideas from our innovation ecosystem.\n\nInfosys provides equal employment opportunities to applicants and employees without regard to race; color; sex; gender identity; sexual orientation; religious practices and observances; national origin; pregnancy, childbirth, or related medical conditions; status as a protected veteran or spouse/family member of a protected veteran; or disability.\n\nCountry\n\nUSA\n\nState / Region / Province\n\nTexas\n\nWork Location\n\nRichardson, TX\n\nInterest Group\n\nInfosys Limited\n\nDomain\n\nRetail ,CPG and logistics\n\nSkillset\n\nTechnology|Cloud Platform|GCP Core Services\n\nCompany\n\nITL USA\n\nRole Designation\n\n835ATHLDUS Technology Lead\n#J-18808-Ljbffr', 'job_highlights': [{'title': 'Qualifications', 'items': ['Required Qualifications Candidate must be located within commuting distance of', 'This position may require travel in the US Bachelors degree or foreign equivalent required from an accredited institution', 'Will also consider three years of progressive experience in the specialty in lieu of every year of education', 'Candidates authorized to work for any employer in the United States without employer-based visa sponsorship are welcome to apply', 'Infosys is unable to provide immigration sponsorship for this role at thistime At least 4years of Information Technology experience', 'Experience working with technologies like GCP with data engineering data flow / air flow, pub sub/ kafta, data proc/Hadoop, Big Query', 'ETL development experience with strong SQL background such as Python/R, Scala, Java, Hive, Spark, Kafka Strongknowledge on Python Program development to build reusable frameworks, enhance existing frameworks', 'Application build experience with core GCP Services like Dataproc, GKE, Composer, Deep understanding GCP IAM & Github', 'Musthave done IAM set up Knowledge onCICD pipeline using Terraform in Git', 'Google Big Query, using advance SQL programing techniques to build Big Query Data sets in Ingestion and Transformation layer', 'Experience in Relational Modeling, Dimensional Modeling and Modeling of Unstructured Data Knowledge on Airflow Dag creation, execution, and monitoring', 'Good understanding of Agile software development frameworks Ability to work in teams in a diverse, multi-stakeholder environment comprising of Business and Technology teams', 'Experience and desire towork in a global deliveryenvironment', 'The job may entail extensive travel', 'Candidates should be able to effectively communicate by telephone, email, and face to face']}, {'title': 'Responsibilities', 'items': ['In this role, you will enable digital transformation for our clients in a global delivery model, research on technologies independently, recommend appropriate solutions and contribute to technology-specific best practices and standards', 'You will be responsible to interface with key stakeholders and apply your technical proficiency across different stages of the Software Development Life Cycle', 'You will be part of a learning culture, where teamwork and collaboration are encouraged, excellence is rewarded, and diversity is respected and valued', 'The job description text describes responsibilities as part of the role: interface with key stakeholders and apply technical proficiency across different stages of the Software Development Life Cycle in a global delivery model, enabling digital transformation and contributing to technology-specific best practices and standards', 'The job may also entail sitting as well as working at a computer for extended periods of time']}], 'apply_options': [{'title': 'WhatJobs', 'link': 'https://www.whatjobs.com/jobs/gcp-data-engineer?id=2254700987&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJHQ1AgRGF0YSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IkluZm9zeXMiLCJhZGRyZXNzX2NpdHkiOiJCbHVlIFJpZGdlLCBUWCIsImh0aWRvY2lkIjoibWZyaHAtOEdRc3pTMWJobEFBQUFBQT09IiwiaGwiOiJlbiJ9'}, {'title': 'Insight Global', 'company_name': 'Insight Global', 'location': 'Atlanta, GA', 'via': 'Insight Global Jobs', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=whDfg5SUtslTZ2HYAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_13NsQrCMBAAUFz7CULhFhfRRAQXnToF_YlyCSGJnHehd0N3f7y4urz1Dd_dcHiytlINAklEgjO8JIJmXFIFYQgihfL-Uc263r1XJVfU0FpyST5eOEdZ_Vui_pi14pI7oeX5erusrnM5jn9HY5iMkA1PEKYNK0BxgIYAAAA&shmds=v1_AdeF8Kj2SuaioxlEd2k1A_5yEhjUAz-eYBWjfoesu5-4rcP_zg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=whDfg5SUtslTZ2HYAAAAAA%3D%3D', 'extensions': ['Contractor', 'No degree mentioned'], 'detected_extensions': {'schedule_type': 'Contractor', 'qualifications': 'No degree mentioned'}, 'description': 'Insight Global is currently searching for a Financial Reports Data Engineer for one of their financial services customers. This person will be responsible for helping the internal audit engineering team with large scale data analysis and data testing across the org. They will be using Python to conduct data analysis while also helping with reconciliations models, calculations models, and help create solutions within the group using R, Python, or any Machine Learning tools. They must come from the finance industry.', 'job_highlights': [{'title': 'Responsibilities', 'items': ['This person will be responsible for helping the internal audit engineering team with large scale data analysis and data testing across the org', 'They will be using Python to conduct data analysis while also helping with reconciliations models, calculations models, and help create solutions within the group using R, Python, or any Machine Learning tools']}], 'apply_options': [{'title': 'Insight Global Jobs', 'link': 'https://jobs.insightglobal.com/find_a_job/texas/job-309454/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJJbnNpZ2h0IEdsb2JhbCIsImNvbXBhbnlfbmFtZSI6Ikluc2lnaHQgR2xvYmFsIiwiYWRkcmVzc19jaXR5IjoiQXRsYW50YSwgR0EiLCJodGlkb2NpZCI6IndoRGZnNVNVdHNsVFoySFlBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Azure Data Engineer (261)', 'company_name': 'Environmental and Safety Solutions, Inc.', 'location': 'Glendale, AZ', 'via': 'ZipRecruiter', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=I_dqiHoryVQ9nxvhAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNsQ6CQAwA0LjyCU4d1SAgicboRCIxurK5kAIVzhwtuSsG_SP_Ul3e-oLPLEiy9-gITqgIObeGiRws0t1mCWu4SgWe0NUdCMNZpLU0P3aqgz_Esfc2ar2imjqqpY-FqZIpfkjl_5S-Q0eDRaUy3SZTNHC72uf8NE64J1a0gNxAgXfSFxRiRzXCPoQL1xGYX2iJG7QUQnb7AuCHveGsAAAA&shmds=v1_AdeF8KgscjkaOz9ZhgdjBPH_2TqICmegegSIsw9O3OEOs2LdrA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=I_dqiHoryVQ9nxvhAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965bd73d224b1cfd371f5/images/10ad8df2269535f2935c683f1b89a90c7d0900d7bf90c87310b015f7e24f3027.jpeg', 'extensions': ['8 days ago', '44â€“57 an hour', 'Contractor'], 'detected_extensions': {'posted_at': '8 days ago', 'salary': '44â€“57 an hour', 'schedule_type': 'Contractor'}, 'description': "ESS-261\nJob Title: Azure - Software Engineer\nUS Citizenship Required - No exceptions\nPlease include Citizenship on top of the resume.\n\nJob Summary:\nWe are seeking a highly skilled and motivated Software Engineer specializing in Data Engineering to join our growing team. This critical role will focus on designing, developing, and optimizing our data infrastructure within the Azure cloud environment. The ideal candidate possesses a deep understanding of data engineering principles and extensive hands-on experience with Azure Data Lake, Azure Data Factory, Databricks, and SAP Business Objects. You will play a key role in building and maintaining robust data pipelines, ensuring data quality, and enabling data-driven insights for the business. This position requires U.S. Citizenship due to project requirements.\n\nThis position is designated as part-time telework per our global telework policy and will require at least three days of in-person attendance per week at the assigned office or project. Weekly in-person schedules will be determined by the individual and their supervisor, in consultation with functional or project leadership.\n\nMajor Responsibilities:\nâ€¢ Design, develop, and optimize scalable and efficient data processing pipelines and architectures within Azure Data Lake and Databricks, leveraging best practices for performance and maintainability.\nâ€¢ Implement and manage complex ETL (Extract, Transform, Load) processes to seamlessly integrate data from diverse sources (e.g., databases, APIs, streaming platforms) into Azure Data Lake, ensuring data quality and consistency.\nâ€¢ Develop and maintain interactive dashboards and reports using SAP Business Objects and Power BI, translating complex data into actionable business insights. Focus on performance optimization and data accuracy.\nâ€¢ Leverage Azure Data Factory for data orchestration, workflow automation, and scheduling, ensuring reliable and timely data delivery.\nâ€¢ Implement and maintain Azure Security & Governance policies, including access control, data encryption, and compliance frameworks, to ensure data protection and adherence to industry best practices.\nâ€¢ Optimize data storage and retrieval mechanisms within Azure, including performance tuning of Databricks clusters and Azure SQL databases, to improve query performance and scalability.\nâ€¢ Collaborate effectively with cross-functional teams (e.g., business analysts, data scientists, product managers) to understand business requirements, translate them into technical solutions, and communicate technical concepts clearly.\nâ€¢ Implement data quality checks and validation rules throughout the data pipeline to ensure data accuracy, completeness, and consistency.\nâ€¢ Monitor, troubleshoot, and enhance existing data solutions, proactively identifying and resolving performance bottlenecks and data quality issues.\nâ€¢ Create and maintain comprehensive technical documentation, including design specifications, data flow diagrams, and operational procedures, to facilitate knowledge sharing and team collaboration.\n\nEducation and Experience Requirements:\nâ€¢ Bachelorâ€™s degree in computer science, Engineering, or a related field, and 2-5 years of experience or 5+ years of relevant work experience.\nâ€¢ Must be a U.S. Citizen.\n\nRequired Knowledge, Skills, and Abilities:\nâ€¢ 4+ years of hands-on experience in data engineering, data warehousing, and cloud-based data platforms.\nâ€¢ Deep expertise in Azure Data Lake, Azure Data Factory, Azure Security & Governance, Databricks, and SAP Business Objects.\nâ€¢ Strong proficiency in SQL, including complex query writing, query optimization, and performance tuning.\nâ€¢ Proven experience in developing and maintaining Power BI dashboards and reports.\nâ€¢ Hands-on experience with Azure services such as Azure Synapse Analytics, Azure SQL Database, and Azure Blob Storage.\nâ€¢ Solid understanding of data modeling concepts, ETL processes, and big data frameworks (e.g., Spark).\nâ€¢ Experience in optimizing and managing large-scale datasets in cloud environments.\nâ€¢ Experience developing and maintaining ETL packages using SSIS and reports using SSRS.\nâ€¢ Strong analytical and problem-solving skills with a keen attention to detail.\nâ€¢ Excellent communication and collaboration skills.\nâ€¢ Master's degree in a relevant field.\nâ€¢ Familiarity with machine learning models and data science concepts.\nâ€¢ Understanding of DevOps practices and CI/CD pipelines for data applications.\nâ€¢ Experience with data governance tools and frameworks.\nâ€¢ Experience with other cloud platforms (e.g., AWS, GCP).\n\nAbout Environmental and Safety Solutions, Inc.:\n\nEnvironmental and Safety Solutions Inc. (ESS Inc.) is a Cincinnati based certified MBE. Since 2002, ESS Inc. has been providing health and safety, engineering, environmental technical and management services to businesses throughout the United States. We work with businesses from Fortune 500 members to small businesses in the local community, providing world class occupational safety and health consultative services and environmental compliance assistance. Our customers value our personal attention to their needs and the accuracy and flexibility of our services to meet their unique needs. As President and Founder of ESS, Lonnie Grayson brings more than 25 years of professional environmental safety and health leadership to the company. He founded ESS Inc. in 2002 and has grown the firm into a nationally recognized industry leader. With a professional background as a mechanical engineer, Mr. Grayson knows the value of a safety and health program that focuses on the processes that drive the safety and health process in a company to achieve great results. We understand the need for partnership with our customers to fully understand their needs and the culture of each customer to meet and exceed their occupational safety, health and environmental compliance needs.", 'job_highlights': [{'title': 'Qualifications', 'items': ['US Citizenship Required - No exceptions', 'The ideal candidate possesses a deep understanding of data engineering principles and extensive hands-on experience with Azure Data Lake, Azure Data Factory, Databricks, and SAP Business Objects', 'Bachelorâ€™s degree in computer science, Engineering, or a related field, and 2-5 years of experience or 5+ years of relevant work experience', 'Must be a U.S. Citizen', '4+ years of hands-on experience in data engineering, data warehousing, and cloud-based data platforms', 'Deep expertise in Azure Data Lake, Azure Data Factory, Azure Security & Governance, Databricks, and SAP Business Objects', 'Strong proficiency in SQL, including complex query writing, query optimization, and performance tuning', 'Proven experience in developing and maintaining Power BI dashboards and reports', 'Hands-on experience with Azure services such as Azure Synapse Analytics, Azure SQL Database, and Azure Blob Storage', 'Solid understanding of data modeling concepts, ETL processes, and big data frameworks (e.g., Spark)', 'Experience in optimizing and managing large-scale datasets in cloud environments', 'Experience developing and maintaining ETL packages using SSIS and reports using SSRS', 'Strong analytical and problem-solving skills with a keen attention to detail', 'Excellent communication and collaboration skills', "Master's degree in a relevant field", 'Familiarity with machine learning models and data science concepts', 'Understanding of DevOps practices and CI/CD pipelines for data applications', 'Experience with data governance tools and frameworks', 'Experience with other cloud platforms (e.g., AWS, GCP)']}, {'title': 'Responsibilities', 'items': ['This critical role will focus on designing, developing, and optimizing our data infrastructure within the Azure cloud environment', 'You will play a key role in building and maintaining robust data pipelines, ensuring data quality, and enabling data-driven insights for the business', 'This position requires U.S. Citizenship due to project requirements', 'This position is designated as part-time telework per our global telework policy and will require at least three days of in-person attendance per week at the assigned office or project', 'Weekly in-person schedules will be determined by the individual and their supervisor, in consultation with functional or project leadership', 'Design, develop, and optimize scalable and efficient data processing pipelines and architectures within Azure Data Lake and Databricks, leveraging best practices for performance and maintainability', 'Implement and manage complex ETL (Extract, Transform, Load) processes to seamlessly integrate data from diverse sources (e.g., databases, APIs, streaming platforms) into Azure Data Lake, ensuring data quality and consistency', 'Develop and maintain interactive dashboards and reports using SAP Business Objects and Power BI, translating complex data into actionable business insights', 'Focus on performance optimization and data accuracy', 'Leverage Azure Data Factory for data orchestration, workflow automation, and scheduling, ensuring reliable and timely data delivery', 'Implement and maintain Azure Security & Governance policies, including access control, data encryption, and compliance frameworks, to ensure data protection and adherence to industry best practices', 'Optimize data storage and retrieval mechanisms within Azure, including performance tuning of Databricks clusters and Azure SQL databases, to improve query performance and scalability', 'Collaborate effectively with cross-functional teams (e.g., business analysts, data scientists, product managers) to understand business requirements, translate them into technical solutions, and communicate technical concepts clearly', 'Implement data quality checks and validation rules throughout the data pipeline to ensure data accuracy, completeness, and consistency', 'Monitor, troubleshoot, and enhance existing data solutions, proactively identifying and resolving performance bottlenecks and data quality issues', 'Create and maintain comprehensive technical documentation, including design specifications, data flow diagrams, and operational procedures, to facilitate knowledge sharing and team collaboration']}], 'apply_options': [{'title': 'ZipRecruiter', 'link': 'https://www.ziprecruiter.com/c/Environmental-and-Safety-Solutions,-Inc./Job/Azure-Data-Engineer-(261)/-in-Glendale,AZ?jid=f24a7726d4eeea96&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Dice', 'link': 'https://www.dice.com/job-detail/ea7eb4f1-befb-4b9d-b93b-16d8a6a1ec22?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Recruit.net', 'link': 'https://www.recruit.net/job/azure-data-engineer--jobs/33FAD198572F47C6?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobrapido', 'link': 'https://us.jobrapido.com/jobpreview/8048805634390360064?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobs And Careers', 'link': 'https://www.career.com/job/environmental-and-safety-solutions-inc/azure-data-engineer-261/j202511081104486455885?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobs Trabajo.org', 'link': 'https://us.trabajo.org/job-3769-55964edf7cf5dfb5768264041d2422b7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJBenVyZSBEYXRhIEVuZ2luZWVyICgyNjEpIiwiY29tcGFueV9uYW1lIjoiRW52aXJvbm1lbnRhbCBhbmQgU2FmZXR5IFNvbHV0aW9ucywgSW5jLiIsImFkZHJlc3NfY2l0eSI6IkdsZW5kYWxlLCBBWiIsImh0aWRvY2lkIjoiSV9kcWlIb3J5VlE5bnh2aEFBQUFBQT09IiwiaGwiOiJlbiJ9'}, {'title': 'Sr. Manager Customer Data Engineering', 'company_name': 'CaptivateIQ', 'location': 'Austin, TX (+3 others)', 'via': 'Lever', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=GiQq1M-LlBbXj8wtAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWNMQrCQBAAsc0PtNpa9E4EG7WRKKJgIVrYySYsl5Nk97hdJS_xvcZmGKaZ4jsqtrfs4IKMgTKUbzXpBtmjIRw4RCbKkQPM4SwVKGGuGxCGo0hoabJpzJKuvVdtXVBDi7WrpfPCVEnvX1LpH09tMFNq0ei5XC16lzhMxyUmi5-hna4QGXbDPfIM7o8fWoLt8ZkAAAA&shmds=v1_AdeF8KgzcRV5omrfqncAA9lMa5-tb9x6yT5dIX8Q7ymTkEfRrw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=GiQq1M-LlBbXj8wtAAAAAA%3D%3D', 'extensions': ['23 days ago', 'Full-time', 'No degree mentioned', 'Paid time off', 'Dental insurance', 'Health insurance'], 'detected_extensions': {'posted_at': '23 days ago', 'schedule_type': 'Full-time', 'qualifications': 'No degree mentioned', 'paid_time_off': True, 'dental_coverage': True, 'health_insurance': True}, 'description': "CaptivateIQ is transforming the way companies plan, manage, and optimize sales performance. We started by revolutionizing incentive compensation management, and now we're expanding our platform to solve broader sales planning challenges. Recognized by industry analysts like Forrester and G2 and backed by top-tier investors, including Sequoia, ICONIQ and Accel, we empower high-growth companies like Netflix, Figma and Stripe with the flexibility and insights needed to drive revenue performance.\n\nJoin a talented, fast-growing team committed to solving some of the most complex and impactful problems in sales performance management.\n\nAbout the Role\n\nBehind every great software company is the team building it. We are an engineering team of multicultural, highly collaborative, fully distributed, humble learners. We pride ourselves on building best-in-class products powered by innovative technologies.\n\nAs an Data Engineering Manager in the Incentive Commission Plans (ICM) Pillar, you will lead the Payee team. This distributed team ensures that our customers' incentive compensated staff receive timely and accurate insights into their pay and opportunities for earnings in a modern interface targeting their unique needs. You will partner with the local Engineering Director to shape the culture and scale the organization in Europe.\n\nJob Location\n\nThe candidate selected for this opportunity must reside near one of the following locations:\n\nHybrid:\n\nAustin, TX\n\nRemote\n\nRaleigh, NC\n\nNashville, TN\n\nToronto, Canada\n\nResponsibilities\nâ€¢ Lead, inspire, and grow a distributed data engineering team, fostering a culture of high trust and psychological safety.\nâ€¢ Develop engineering talent through effective coaching, mentorship, and performance management, with a focus on growing senior and staff-level engineers.\nâ€¢ Own the end-to-end delivery and operational excellence of customer data integrations, ensuring they are reliable and secure at scale.\nâ€¢ Design and implement customer data integrations using IPaaS platforms like Workato, Boomi, or MuleSoft.\nâ€¢ Write and optimize advanced SQL (Snowflake) â€” including DDL, DML, CTEs, UDFs, and JSON transformations â€” to power analytics and automation.\nâ€¢ Build and maintain software for orchestration of customer data integrations utilizing web APIs and IPaaS connectors to seamlessly exchange data between CaptivateIQ and our customers.\nâ€¢ Ensure customer integrations are secure, testable, fault-tolerant, and designed for long-term maintainability.\nâ€¢ Collaborate with Sales Engineering and Product during discovery and scoping to advise on technical feasibility and data architecture approaches.\nâ€¢ Build relationships with product engineering to ensure scalability and management of our customer data integrations as customer usage grows.\nâ€¢ Translate customer business needs into the technical requirements and solution.\n\nRequirements\nâ€¢ A strong (7+ years of experience) background in software and/or data engineering\nâ€¢ Proven experience working with data pipelines at scale and knowledge of data engineering technologies, such as Snowflake\nâ€¢ Strong proficiency in Python and SQL proficiency\nâ€¢ High sense of ownership and a strong growth mindset; you are a collaborative, humble learner who can thrive in a fast-paced environment.\n\nNice to have\nâ€¢ Youâ€™ve previously operated in a customer facing role.\nâ€¢ Experience configuring and troubleshooting SSO (SAML, OIDC) and SCIM provisioning across Okta, Entra (Azure AD), and OneLogin environments.\nâ€¢ Hands-on experience with setting up modern data stacks (ETL, data warehouses, DBT, reverse ETL)\n\nBenefits:\nâ€¢ (US-ONLY) 100% of medical, dental, and vision covered including 75% for dependents\nâ€¢ Flexible vacation days and quarterly mental health days so you can recharge\nâ€¢ (US-ONLY) 401k plan to participate in and save towards the future\nâ€¢ Newest Apple products to help you do your best work\nâ€¢ Employee Resource Groups (ERGs) to support and celebrate the shared identities and life experiences of communities within CaptivateIQ. ERGs directly support our company-wide DEI goals as a space for developing and retaining diverse talent\n\nNotice to Prospective Candidates:\nâ€¢ Only emails from @captivateiq.com should be trusted.\nâ€¢ We are aware of active recruitment scams using the CaptivateIQ name, in which individuals pose as our recruiters and post fake remote job openings and make fake job offers on the Internet. Please note, we will never do the following:Attempt to correspond with a candidate using a free web-based account, such as an email address that ends in @gmail.com, @yahoo.com, @hotmail.com, etc.\nâ€¢ Make an offer of employment without conducting multiple rounds of interviews face-to-face using secure video-conferencing technology.\nâ€¢ Ask candidates to cash checks to buy equipment on behalf of CaptivateIQ.\nâ€¢ Ask candidates to make a payment in order to be considered for a position.\nâ€¢ Make early requests for candidates' personal information such as date of birth, passport details, credit card numbers, bank details and social security number, etc.\nâ€¢ Please note that weâ€™ll only ask for more sensitive personal information in connection with background checks after an offer is made.\nâ€¢ Participate in an on-call rotation to provide after-hours support, ensuring timely resolution of critical issues and maintaining system uptime.\n\n$186,000 - $256,000 a year\nThe base range represents the minimum and maximum for this position across North America. For candidates in Austin and Raleigh, the range is $206,000â€“$256,000; for Toronto, Nashville locations, the range is $186,000â€“$225,000. The compensation offered for this position will depend on numerous factors, including individual proficiency, anticipated performance, and the location of the selected candidate. Our OTE is just one component of CaptivateIQ's competitive total rewards package.\n\nCaptivateIQ participates in E-Verify, web-based system that allows enrolled employers to confirm the eligibility of their employees to work in the United States.", 'job_highlights': [{'title': 'Qualifications', 'items': ['A strong (7+ years of experience) background in software and/or data engineering', 'Proven experience working with data pipelines at scale and knowledge of data engineering technologies, such as Snowflake', 'Strong proficiency in Python and SQL proficiency', 'High sense of ownership and a strong growth mindset; you are a collaborative, humble learner who can thrive in a fast-paced environment', 'Youâ€™ve previously operated in a customer facing role', 'Experience configuring and troubleshooting SSO (SAML, OIDC) and SCIM provisioning across Okta, Entra (Azure AD), and OneLogin environments', 'Hands-on experience with setting up modern data stacks (ETL, data warehouses, DBT, reverse ETL)']}, {'title': 'Benefits', 'items': ['(US-ONLY) 100% of medical, dental, and vision covered including 75% for dependents', 'Flexible vacation days and quarterly mental health days so you can recharge', '(US-ONLY) 401k plan to participate in and save towards the future', 'Newest Apple products to help you do your best work', 'Employee Resource Groups (ERGs) to support and celebrate the shared identities and life experiences of communities within CaptivateIQ', '$186,000 - $256,000 a year']}, {'title': 'Responsibilities', 'items': ['You will partner with the local Engineering Director to shape the culture and scale the organization in Europe', 'Lead, inspire, and grow a distributed data engineering team, fostering a culture of high trust and psychological safety', 'Develop engineering talent through effective coaching, mentorship, and performance management, with a focus on growing senior and staff-level engineers', 'Own the end-to-end delivery and operational excellence of customer data integrations, ensuring they are reliable and secure at scale', 'Design and implement customer data integrations using IPaaS platforms like Workato, Boomi, or MuleSoft', 'Write and optimize advanced SQL (Snowflake) â€” including DDL, DML, CTEs, UDFs, and JSON transformations â€” to power analytics and automation', 'Build and maintain software for orchestration of customer data integrations utilizing web APIs and IPaaS connectors to seamlessly exchange data between CaptivateIQ and our customers', 'Ensure customer integrations are secure, testable, fault-tolerant, and designed for long-term maintainability', 'Collaborate with Sales Engineering and Product during discovery and scoping to advise on technical feasibility and data architecture approaches', 'Build relationships with product engineering to ensure scalability and management of our customer data integrations as customer usage grows', 'Translate customer business needs into the technical requirements and solution', 'Ask candidates to cash checks to buy equipment on behalf of CaptivateIQ', 'Ask candidates to make a payment in order to be considered for a position', "Make early requests for candidates' personal information such as date of birth, passport details, credit card numbers, bank details and social security number, etc", 'Participate in an on-call rotation to provide after-hours support, ensuring timely resolution of critical issues and maintaining system uptime']}], 'apply_options': [{'title': 'Lever', 'link': 'https://jobs.lever.co/captivateiq/dce03e56-0fd2-4080-a37d-cbaa7091dff4?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Indeed', 'link': 'https://www.indeed.com/viewjob?jk=ea0b7273e19f54ad&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'ZipRecruiter', 'link': 'https://www.ziprecruiter.com/c/CaptivateIQ/Job/Sr.-Manager-Customer-Data-Engineering/-in-Raleigh,NC?jid=986b91334444e975&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Built In', 'link': 'https://builtin.com/job/sr-manager-customer-data-engineering/7496164?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Glassdoor', 'link': 'https://www.glassdoor.com/job-listing/sr-manager-customer-data-engineering-captivateiq-JV_IC1138960_KO0,36_KE37,48.htm?jl=1009920334569&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'LinkedIn', 'link': 'https://www.linkedin.com/jobs/view/sr-manager-customer-data-engineering-at-captivateiq-4332842096?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'SimplyHired', 'link': 'https://www.simplyhired.com/job/gRxBKfG4oS6uhckV1y5q_dmZL2YsOJoIU_112icFKAblAh9P1F0Djg?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Career.io', 'link': 'https://career.io/job/sr-manager-customer-data-engineering-raleigh-captivateiq-64d761705cc2a5fe4dbd9b31563ddd26?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTci4gTWFuYWdlciBDdXN0b21lciBEYXRhIEVuZ2luZWVyaW5nIiwiY29tcGFueV9uYW1lIjoiQ2FwdGl2YXRlSVEiLCJhZGRyZXNzX2NpdHkiOiJBdXN0aW4sIFRYIiwiaHRpZG9jaWQiOiJHaVFxMU0tTGxCYlhqOHd0QUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': 'Snowflake Data Engineer', 'company_name': 'My3Tech Inc', 'location': 'St Peters, MO', 'via': 'WhatJobs', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=SMRxp_iXQim8plrMAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMsQrCQAwAUFz7CeKQWfROFBddFVEoCnUv6RGv1TMpl4D1G_xpdXnjKz6jwlUsr1vCB8EODWHPsWOiDHM4SQNKmEMLwnAQiYnG29as1433qslFNbQuuCBPL0yNDP4ujf6ptcVMfUKjerleDK7nOJ2U79WVft-RA3QMlcGFjLLOoDx_AX935TGOAAAA&shmds=v1_AdeF8KhYetFt_pE967qGnI_S_lGZjUGLSfFkF2mIbSpr4NtdWA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=SMRxp_iXQim8plrMAAAAAA%3D%3D', 'extensions': ['2 days ago', 'Full-time', 'No degree mentioned'], 'detected_extensions': {'posted_at': '2 days ago', 'schedule_type': 'Full-time', 'qualifications': 'No degree mentioned'}, 'description': 'Senior Data Engineer-\n\nResponsibilities:\nâ€¢ Design, develop, and optimize scalable data pipelines and ETL processes utilizing Snowflake and related cloud technologies.\nâ€¢ Develop and implement efficient data ingestion methods, integrating various structured and semi-structured data sources.\nâ€¢ Create and maintain data warehouse schemas, tables, views, and materialized views in Snowflake.\nâ€¢ Ensure data quality, data integrity, and data governance standards are maintained throughout ETL processes.\nâ€¢ Collaborate closely with data analysts, data architect, and business stakeholders to understand and fulfill data requirements.\nâ€¢ Perform regular performance tuning and optimization of Snowflake queries and warehouse operations.\nâ€¢ Develop and enforce best practices for data modeling, ETL architecture, data warehousing, and data security.\nâ€¢ Proactively identify, troubleshoot, and resolve data issues and pipeline failures.\nâ€¢ Document technical processes, system architecture, and data lineage clearly and comprehensively.\n\nCharter Data Engineer:\n\nOverview: Project is related to customer intelligence, has data coming in from different vendors (Google Analytics, HootSuite, Known), different file formats (API, CSV), transforming and ingesting into S3 buckets, from there into Snowflake. Different layers in Snowflake: staging, cleansed, reporting; need to move and curate data with business logic in the cleansing layer.\n\nLeveraging different technologies: Python, Snowflake, Airflow orchestration and DAG creation, Gitlab source control, Flyway for version control (don\'t really need expertise in it, but developers with experience in source/version control and CI/CD) Python to ingest data and build out Airflow-related code, performing transformations by passing parameters. Creating tasks within DAG, DAGs trigger Gitlab, Flyway runs version control from Gitlab, implements schema changes. Using Snowflake procedures, SQL stored procedures for smaller volumes of data. Still solutioning large volumes of data--may end up being a Python/Spark approach.\n\nRequired Skills : Requirements: â€¢ Python â€¢ SQL â€¢ ETL â€¢ Stored procedures â€¢ Snowflake (can be basic + upskilling, acquisition of badges ahead of joining) â€¢ Job orchestration i.e. Airflow (Airflow itself is nice to have) â€¢ Version and Source Control experience Nice to Have: â€¢ Airflow â€¢ Gitlab â€¢ Flyway\n\nBasic Qualification :\n\nAdditional Skills :\n\nBackground Check : Yes\n\nDrug Screen : Yes\n\nNotes :\nSelling points for candidate :\nProject Verification Info :"The information provided below is for Apex Systems AV use only and is not to be distributed publicly, or to any third party. Any distribution of the below information will result in corrective action from Apex Systems Vendor Management. MSA: Blanket Approval Received Client Letter: Will Provide"\nExclusive to Apex :No\nFace to face interview required :No\nCandidate must be local :No\nCandidate must be authorized to work without sponsorship ::No\nInterview times set : :No\nType of project :\nMaster Job Title :\nBranch Code :', 'job_highlights': [{'title': 'Qualifications', 'items': ["Leveraging different technologies: Python, Snowflake, Airflow orchestration and DAG creation, Gitlab source control, Flyway for version control (don't really need expertise in it, but developers with experience in source/version control and CI/CD) Python to ingest data and build out Airflow-related code, performing transformations by passing parameters", 'Creating tasks within DAG, DAGs trigger Gitlab, Flyway runs version control from Gitlab, implements schema changes', 'Using Snowflake procedures, SQL stored procedures for smaller volumes of data', 'Python', 'SQL', 'ETL', 'Stored procedures', 'Snowflake (can be basic + upskilling, acquisition of badges ahead of joining)', 'Job orchestration i.e', 'Airflow (Airflow itself is nice to have)', 'Version and Source Control experience Nice to Have:', 'Airflow', 'Gitlab', 'Flyway', 'Background Check : Yes', 'Drug Screen : Yes', 'Candidate must be local :No', 'Candidate must be authorized to work without sponsorship ::No']}, {'title': 'Responsibilities', 'items': ['Design, develop, and optimize scalable data pipelines and ETL processes utilizing Snowflake and related cloud technologies', 'Develop and implement efficient data ingestion methods, integrating various structured and semi-structured data sources', 'Create and maintain data warehouse schemas, tables, views, and materialized views in Snowflake', 'Ensure data quality, data integrity, and data governance standards are maintained throughout ETL processes', 'Collaborate closely with data analysts, data architect, and business stakeholders to understand and fulfill data requirements', 'Perform regular performance tuning and optimization of Snowflake queries and warehouse operations', 'Develop and enforce best practices for data modeling, ETL architecture, data warehousing, and data security', 'Proactively identify, troubleshoot, and resolve data issues and pipeline failures', 'Document technical processes, system architecture, and data lineage clearly and comprehensively', 'Overview: Project is related to customer intelligence, has data coming in from different vendors (Google Analytics, HootSuite, Known), different file formats (API, CSV), transforming and ingesting into S3 buckets, from there into Snowflake', 'Different layers in Snowflake: staging, cleansed, reporting; need to move and curate data with business logic in the cleansing layer', 'Still solutioning large volumes of data--may end up being a Python/Spark approach']}], 'apply_options': [{'title': 'WhatJobs', 'link': 'https://www.whatjobs.com/jobs/snowflake-data-engineer?id=2262737299&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTbm93Zmxha2UgRGF0YSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6Ik15M1RlY2ggSW5jIiwiYWRkcmVzc19jaXR5IjoiU3QgUGV0ZXJzLCBNTyIsImh0aWRvY2lkIjoiU01SeHBfaVhRaW04cGxyTUFBQUFBQT09IiwiaGwiOiJlbiJ9'}, {'title': 'Database Engineer', 'company_name': 'Cherokee Federal', 'location': 'San Antonio, TX', 'via': 'LinkedIn', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=vScAbiKdBZvMm16PAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEvQoCMQwAYFzvEZwCLiLaiuDiTeIfuOrgdqRnaKs1OZoM9xA-tPgNX_OdNPMjGgZUghPHzEQVVnCVAEpY-wTCcBGJhaZtMht0571qcVENLfeul48XpiCjf0nQf50mrDQUNOo22_XoBo6L2SFRlTcRnOlJFQtkhhsy7NmEsyzh_vgBu_F3-I8AAAA&shmds=v1_AdeF8KgM5pAm4U69oJ4nY23NZvuiURhTYU6pffuXcPbby-G0cQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=vScAbiKdBZvMm16PAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965bff2593a58c4e71a36/images/fcee195cdacc4b716360dc592f5dec707ee6c17e98c6332a4aab35dc302cb038.jpeg', 'extensions': ['22 days ago', '110Kâ€“120K a year', 'Full-time', 'No degree mentioned', 'Health insurance', 'Dental insurance'], 'detected_extensions': {'posted_at': '22 days ago', 'salary': '110Kâ€“120K a year', 'schedule_type': 'Full-time', 'qualifications': 'No degree mentioned', 'health_insurance': True, 'dental_coverage': True}, 'description': 'We are seeking a skilled and detail-oriented Data Engineer to support large-scale data management and reporting efforts for a critical Air Force environmental project. This individual will be responsible for integrating, managing, and transforming data from SharePoint lists and libraries into an Azure SQL database and producing client-facing reports based on that data. The role requires strong technical expertise in cloud-based database design, data pipeline design, operations, data extraction and transformation, automation (including macros), and advanced Excel reporting.\n\nCompensation & Benefits:\n\nEstimated Starting Salary Range for Database Engineer: $110,000 to $120,000\n\nPay commensurate with experience.\n\nFull time benefits include Medical, Dental, Vision, 401K, and other possible benefits as provided. Benefits are subject to change with or without notice.\n\nData Engineer Responsibilities Include:\nâ€¢ Extract and integrate data from large SharePoint lists, libraries, and Excel spreadsheets into Azure SQL databases.\nâ€¢ Develop, document, and design database pipelines to automate data extraction and transformation processes.\nâ€¢ Manage, update, and optimize Azure SQL database schemas to support high-volume data ingestion (100,000+ rows, 25+ columns) across multi-site, multi-year project data.\nâ€¢ Prepare and maintain efficient, reusable, and reliable VBA for Microsoft Office (Excel, Access, SharePoint, Outlook, and Word) applications\nâ€¢ Investigate and troubleshoot issues, monitor defect tracking system, and create and maintain resolution to known problems or errors.\nâ€¢ Create, maintain, and automate generation of client-facing reports, including balance sheet reports, detailed cost estimates, and year over year data comparisons.\nâ€¢ Ensure data quality, consistency, and traceability across complex datasets involving site-level phases, multi-year cost tracking, and historical versions.\nâ€¢ Collaborate with project managers, technical leads, senior cost estimators, and leadership to support reporting deadlines and client requirements.\nâ€¢ Document pipeline, database, and reporting scripts processes and workflows to support knowledge transfer and operational continuity.\n\nData Engineer Experience, Education, Skills, Abilities requested:\nâ€¢ 5+ years of experience in database management and reporting.\nâ€¢ Strong proficiency in Azure SQL Database (design, data ingestion, transformation, optimization).\nâ€¢ Experience pulling and manipulating data from large SharePoint lists and libraries.\nâ€¢ Experience pushing data from SQL to SharePoint lists to provide status updates over the course of each projectâ€™s lifecycle\nâ€¢ Advanced Excel skills, including macro development and complex data processing.\nâ€¢ Experience working with large datasets (100,000+ rows) with attention to performance and scalability.\nâ€¢ Ability to translate raw data into client-ready reports with a focus on clarity and accuracy.\nâ€¢ Strong analytical, troubleshooting, and problem-solving skills.\nâ€¢ Ability to work in a high paced environment.\nâ€¢ Ability to communicate effectively, both orally and in writing.\nâ€¢ Excellent attention to detail and commitment to meeting deadlines.\nâ€¢ Experience working in government contracting environments, especially supporting DoD projects.\nâ€¢ Familiarity with environmental project data, cost estimation reporting, or similar financial reporting.\nâ€¢ Experience with Power BI, SSIS (SQL Server Integration Services), or other ETL (Extract, Transform, Load) tools.\nâ€¢ Knowledge in Adobe Acrobat including form development and maintenance.\n\nCompany Information:\n\nCherokee Nation Strategic Programs (CNSP) is a part of Cherokee Federal â€“ the division of tribally owned federal contracting companies owned by Cherokee Nation Businesses. As a trusted partner for more than 60 federal clients, Cherokee Federal LLCs are focused on building a brighter future, solving complex challenges, and serving the governmentâ€™s mission with compassion and heart. To learn more about CNSP, visit cherokee-federal.com.\n\nCherokee Federal is a military friendly employer. Veterans and active military transitioning to civilian status are encouraged to apply.\n\nSimilar searchable job titles:\nâ€¢ Data Engineer\nâ€¢ Data Coordinator\nâ€¢ Data Manager\nâ€¢ Data Technician\nâ€¢ Data Administrator\n\nKeywords:\nâ€¢ Data Management\nâ€¢ Data Analysis\nâ€¢ Database Administration\nâ€¢ Data Integrity\nâ€¢ Data Reporting\n\nLegal Disclaimer: All qualified applicants will receive consideration for employment without regard to protected veteran status, disability or any other status protected under applicable federal, state or local law. Many of our job openings require access to government buildings or military installations. Candidates must pass pre-employment qualifications of Cherokee Federal.\n\nMany of our job openings require access to government buildings or military installations. Candidates must pass pre-employment qualifications of Cherokee Federal.', 'job_highlights': [{'title': 'Qualifications', 'items': ['The role requires strong technical expertise in cloud-based database design, data pipeline design, operations, data extraction and transformation, automation (including macros), and advanced Excel reporting', '5+ years of experience in database management and reporting', 'Strong proficiency in Azure SQL Database (design, data ingestion, transformation, optimization)', 'Experience pulling and manipulating data from large SharePoint lists and libraries', 'Experience pushing data from SQL to SharePoint lists to provide status updates over the course of each projectâ€™s lifecycle', 'Advanced Excel skills, including macro development and complex data processing', 'Experience working with large datasets (100,000+ rows) with attention to performance and scalability', 'Ability to translate raw data into client-ready reports with a focus on clarity and accuracy', 'Strong analytical, troubleshooting, and problem-solving skills', 'Ability to work in a high paced environment', 'Ability to communicate effectively, both orally and in writing', 'Excellent attention to detail and commitment to meeting deadlines', 'Experience working in government contracting environments, especially supporting DoD projects', 'Familiarity with environmental project data, cost estimation reporting, or similar financial reporting', 'Experience with Power BI, SSIS (SQL Server Integration Services), or other ETL (Extract, Transform, Load) tools', 'Knowledge in Adobe Acrobat including form development and maintenance', 'Data Coordinator', 'Data Technician', 'Data Administrator', 'Database Administration', 'Data Integrity']}, {'title': 'Benefits', 'items': ['Estimated Starting Salary Range for Database Engineer: $110,000 to $120,000', 'Pay commensurate with experience', 'Full time benefits include Medical, Dental, Vision, 401K, and other possible benefits as provided']}, {'title': 'Responsibilities', 'items': ['We are seeking a skilled and detail-oriented Data Engineer to support large-scale data management and reporting efforts for a critical Air Force environmental project', 'This individual will be responsible for integrating, managing, and transforming data from SharePoint lists and libraries into an Azure SQL database and producing client-facing reports based on that data', 'Extract and integrate data from large SharePoint lists, libraries, and Excel spreadsheets into Azure SQL databases', 'Develop, document, and design database pipelines to automate data extraction and transformation processes', 'Manage, update, and optimize Azure SQL database schemas to support high-volume data ingestion (100,000+ rows, 25+ columns) across multi-site, multi-year project data', 'Prepare and maintain efficient, reusable, and reliable VBA for Microsoft Office (Excel, Access, SharePoint, Outlook, and Word) applications', 'Investigate and troubleshoot issues, monitor defect tracking system, and create and maintain resolution to known problems or errors', 'Create, maintain, and automate generation of client-facing reports, including balance sheet reports, detailed cost estimates, and year over year data comparisons', 'Ensure data quality, consistency, and traceability across complex datasets involving site-level phases, multi-year cost tracking, and historical versions', 'Collaborate with project managers, technical leads, senior cost estimators, and leadership to support reporting deadlines and client requirements', 'Document pipeline, database, and reporting scripts processes and workflows to support knowledge transfer and operational continuity']}], 'apply_options': [{'title': 'LinkedIn', 'link': 'https://www.linkedin.com/jobs/view/database-engineer-at-cherokee-federal-4318382953?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Teal', 'link': 'https://www.tealhq.com/job/database-engineer_07a76b75-283b-4d90-a912-0e20669de67e?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Ladders', 'link': 'https://www.theladders.com/job/database-engineer-cherokeenationbusinesses-san-antonio-tx_82549726?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'JobzMall', 'link': 'https://www.jobzmall.com/bexar-county/job/database-engineer-10?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Talents By Vaia', 'link': 'https://talents.vaia.com/companies/cherokee-federal/database-engineer-30116628/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'LazyApply', 'link': 'https://lazyapply.com/jobpreview/database-engineer-maximus-san-antonio-tx_82849107?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobs Trabajo.org', 'link': 'https://us.trabajo.org/job-3640-29f9c8add565ff0501c4bd308c51d605?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEYXRhYmFzZSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IkNoZXJva2VlIEZlZGVyYWwiLCJhZGRyZXNzX2NpdHkiOiJTYW4gQW50b25pbywgVFgiLCJodGlkb2NpZCI6InZTY0FiaUtkQlp2TW0xNlBBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Platform Principal Data Engineer', 'company_name': 'Massgeneralbrigham', 'location': 'Hampton, NH', 'via': 'ZipRecruiter', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=BVMeZXOdvvMPukl3AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNMQoCMRAAQGzvCVZrK3oRQQRtFQ9BuR8cm7AmkWQ3ZFPcV_yt2kw73WfRHceE7SU1w1gju1gwwQUbwpV9ZKIKW7iLBSWsLoAw3ER8ouU5tFb0ZIxq6r02bNH1TrIRJiuzeYvVP5MGrFR-C037w27uC_v16oGqnpgqJlujD5ghMgyYSxPewHP4AphtOlOcAAAA&shmds=v1_AdeF8Kg9bPOM-95Wb7Q-5uLVTQuLsQjjWpyXjHOMEIpGSdJlMQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=BVMeZXOdvvMPukl3AAAAAA%3D%3D', 'extensions': ['7 days ago', 'Full-time', 'Health insurance'], 'detected_extensions': {'posted_at': '7 days ago', 'schedule_type': 'Full-time', 'health_insurance': True}, 'description': 'Site: Mass General Brigham Incorporated\n\nMass General Brigham relies on a wide range of professionals, including doctors, nurses, business people, tech experts, researchers, and systems analysts to advance our mission. As a not-for-profit, we support patient care, research, teaching, and community service, striving to provide exceptional care. We believe that high-performing teams drive groundbreaking medical discoveries and invite all applicants to join us and experience what it means to be part of Mass General Brigham.\n\nSeeking a highly skills and motivated Enterprise Principal Data Engineer!\n\nMGB is excited to bring on a Principal Data Engineer to join the Digital Data Platform Digital Team to help shape the future of our modern, cloud-based data ecosystem! In this high-impact role, you\'ll lead the design and optimization of scalable data platforms - with a focus on DBT, Snowflake, and CI/CD pipelines using Azure DevOps and Azure Repos (Git).\n\nYou will collaborate with the team onsite at MGB\'s HQ site; Somerville, MA Assembly Row weekly.\n\nThis is a unique opportunity to combine strategic leadership with hands-on engineering, guiding platform architecture, governance, and best practices while mentoring a team of data engineers.\n\nYou\'ll also help pioneer AI/ML-driven automation, working on projects that use cutting-edge tools and LLMs to simplify and accelerate the data management lifecycle. You\'ll be at the forefront of enabling scalable, efficient, and intelligent data solutions that empower multiple development teams across the organization.\n\nJob Summary\nThe Opportunity\n\nThe Enterprise Principal Data Engineer is responsible for Data Engineering Standards CoE, driving automation and best practices to standardize and accelerate development. Responsible for overseeing the design, development, implementation, and maintenance of data solutions within the organization. Support team of data engineers, collaborating with cross-functional teams, data scientists, and business stakeholders to ensure the efficient and reliable management of data.\n\nWhat You\'ll Do\n\n-Enhance and administer the DBT and CI/CD platform architecture to improve scalability, performance, and automation.\n-Design and build reusable, high-performance data pipelines using technologies like Snowflake, Python, and DBT to support both operational and analytical needs.\n\n-Lead the design, development and automation of CI/CD pipelines for data workflows using Azure DevOps, Git, and orchestration tools.\n-Evaluate emerging features in DBT, CI/CD, and related tools; lead POCs, define adoption strategies, and support rollout to development teams.\n\n-Act as an escalation point to troubleshoot and solve technical challenges.\n\n-Leverage Python and cutting-edge AI technologies, including Large Language Models (LLMs), to automate and streamline the data management lifecycle - from intelligent metadata tagging and automated code validation to advanced data profiling and pipeline generation.\n\n-Lead Data Engineering platform collaboration sessions and actively drive standard adoption and technical best practices.\n\n-Proactively identify opportunities to innovate and expand platform capabilities in alignment with strategic goals.\n\n-Champion continuous improvement through agile delivery, platform upgrades, and process innovation.\n\n-Mentor and support engineers at all levels, fostering a strong culture of learning and technical excellence.\n\nQualifications\n\nWhat You\'ll Bring\nâ€¢ Bachelor\'s Degree Computer Science required or Bachelor\'s Degree Related Field of Study required\nâ€¢ MGB can consider experience in lieu of a degree\nâ€¢ Experience in professional information technology positions 8-10+ years required and Data warehousing development in large reporting environment(s)\nâ€¢ 5-7 years required and Experience working with data integration tools, ETL frameworks, and workflow management systems (e.g., Apache Airflow) required.\nâ€¢ 2+ years of hands-on experience in DBT tool development and administration. Experience in CI/CD pipeline development using Azure DevOps and Azure Git, including repository management and enforcing best practices. Experience with AI/ML technologies, including LLMs, to support platform automation and innovation.Strong attention to detail and a proactive approach to process improvement and standards development.\nâ€¢ Python experience.\nâ€¢ Highly Preferred: Proficient with DBT tool, comfortable designing, developing, and performing admin tasks. Strong understanding of CI/CD processes using Azure DevOps or Git.\nâ€¢ Platform support at enterprise scale highly preferred\n\nSkills for Success\nâ€¢ Expertise in data engineering principles, data modeling, ETL development, and data warehousing.\nâ€¢ Hands-on experience with DBT, CI/CD pipelines, and cloud data platforms (Azure, Snowflake).\nâ€¢ Strong ability in building data pipelines using Snowflake features ( Snowpipe, SnowSQL, Snow Sight, Data Streams ).\nâ€¢ Proficiency in working with relational databases (e.g., SQL Server, Oracle, PostgreSQL) and NoSQL databases (e.g., MongoDB, Cassandra).\n\nThis role is specifically for platform engineering rather than a development engineer position. As advertised, this role is more focused on platform-based engineering, administration.\n\nAdditional Job Details (if applicable)\n\nWorking Model Hybrid Required, Somerville, MA\nâ€¢ Onsite 1-3x weekly, local or willing to relocate is required for weekly business needs that may vary depending on the project and includes more or less days per week, must be flexible.\nâ€¢ M-F Eastern Business Hours required\nâ€¢ Remote work requires stable, secure, quiet, compliant work station\n\nRemote Type\n\nHybrid\n\nWork Location\n\n399 Revolution Drive\n\nScheduled Weekly Hours\n\n40\n\nEmployee Type\n\nRegular\n\nWork Shift\n\nDay (United States of America)\n\nPay Range\n$55.48 - $80.70/Hourly\n\nGrade\n8\n\nAt Mass General Brigham, we believe in recognizing and rewarding the unique value each team member brings to our organization. Our approach to determining base pay is comprehensive, and any offer extended will take into account your skills, relevant experience if applicable, education, certifications and other essential factors. The base pay information provided offers an estimate based on the minimum job qualifications; however, it does not encompass all elements contributing to your total compensation package. In addition to competitive base pay, we offer comprehensive benefits, career advancement opportunities, differentials, premiums and bonuses as applicable and recognition programs designed to celebrate your contributions and support your professional growth. We invite you to apply, and our Talent Acquisition team will provide an overview of your potential compensation and benefits package.\n\nEEO Statement:\n\nMass General Brigham Incorporated is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religious creed, national origin, sex, age, gender identity, disability, sexual orientation, military service, genetic information, and/or other status protected under law. We will ensure that all individuals with a disability are provided a reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. To ensure reasonable accommodation for individuals protected by Section 503 of the Rehabilitation Act of 1973, the Vietnam Veteran\'s Readjustment Act of 1974, and Title I of the Americans with Disabilities Act of 1990, applicants who require accommodation in the job application process may contact Human Resources at (857)-282-7642.\n\nMass General Brigham Competency Framework\n\nAt Mass General Brigham, our competency framework defines what effective leadership "looks like" by specifying which behaviors are most critical for successful performance at each job level. The framework is comprised of ten competencies (half People-Focused, half Performance-Focused) and are defined by observable and measurable skills and behaviors that contribute to workplace effectiveness and career success. These competencies are used to evaluate performance, make hiring decisions, identify development needs, mobilize employees across our system, and establish a strong talent pipeline.', 'job_highlights': [{'title': 'Qualifications', 'items': ['Seeking a highly skills and motivated Enterprise Principal Data Engineer!', "Bachelor's Degree Computer Science required or Bachelor's Degree Related Field of Study required", 'MGB can consider experience in lieu of a degree', 'Experience in professional information technology positions 8-10+ years required and Data warehousing development in large reporting environment(s)', '5-7 years required and Experience working with data integration tools, ETL frameworks, and workflow management systems (e.g., Apache Airflow) required', '2+ years of hands-on experience in DBT tool development and administration', 'Experience in CI/CD pipeline development using Azure DevOps and Azure Git, including repository management and enforcing best practices', 'Experience with AI/ML technologies, including LLMs, to support platform automation and innovation', 'Strong attention to detail and a proactive approach to process improvement and standards development', 'Python experience', 'Strong understanding of CI/CD processes using Azure DevOps or Git', 'Skills for Success', 'Expertise in data engineering principles, data modeling, ETL development, and data warehousing', 'Hands-on experience with DBT, CI/CD pipelines, and cloud data platforms (Azure, Snowflake)', 'Strong ability in building data pipelines using Snowflake features ( Snowpipe, SnowSQL, Snow Sight, Data Streams )', 'Proficiency in working with relational databases (e.g., SQL Server, Oracle, PostgreSQL) and NoSQL databases (e.g., MongoDB, Cassandra)', 'This role is specifically for platform engineering rather than a development engineer position', 'Onsite 1-3x weekly, local or willing to relocate is required for weekly business needs that may vary depending on the project and includes more or less days per week, must be flexible', 'Remote work requires stable, secure, quiet, compliant work station']}, {'title': 'Benefits', 'items': ['Scheduled Weekly Hours', '$55.48 - $80.70/Hourly', 'Our approach to determining base pay is comprehensive, and any offer extended will take into account your skills, relevant experience if applicable, education, certifications and other essential factors', 'The base pay information provided offers an estimate based on the minimum job qualifications; however, it does not encompass all elements contributing to your total compensation package', 'In addition to competitive base pay, we offer comprehensive benefits, career advancement opportunities, differentials, premiums and bonuses as applicable and recognition programs designed to celebrate your contributions and support your professional growth', 'We invite you to apply, and our Talent Acquisition team will provide an overview of your potential compensation and benefits package']}, {'title': 'Responsibilities', 'items': ["In this high-impact role, you'll lead the design and optimization of scalable data platforms - with a focus on DBT, Snowflake, and CI/CD pipelines using Azure DevOps and Azure Repos (Git)", "You will collaborate with the team onsite at MGB's HQ site; Somerville, MA Assembly Row weekly", 'This is a unique opportunity to combine strategic leadership with hands-on engineering, guiding platform architecture, governance, and best practices while mentoring a team of data engineers', "You'll also help pioneer AI/ML-driven automation, working on projects that use cutting-edge tools and LLMs to simplify and accelerate the data management lifecycle", "You'll be at the forefront of enabling scalable, efficient, and intelligent data solutions that empower multiple development teams across the organization", 'The Enterprise Principal Data Engineer is responsible for Data Engineering Standards CoE, driving automation and best practices to standardize and accelerate development', 'Responsible for overseeing the design, development, implementation, and maintenance of data solutions within the organization', 'Support team of data engineers, collaborating with cross-functional teams, data scientists, and business stakeholders to ensure the efficient and reliable management of data', 'Enhance and administer the DBT and CI/CD platform architecture to improve scalability, performance, and automation', 'Design and build reusable, high-performance data pipelines using technologies like Snowflake, Python, and DBT to support both operational and analytical needs', 'Lead the design, development and automation of CI/CD pipelines for data workflows using Azure DevOps, Git, and orchestration tools', 'Evaluate emerging features in DBT, CI/CD, and related tools; lead POCs, define adoption strategies, and support rollout to development teams', 'Act as an escalation point to troubleshoot and solve technical challenges', 'Leverage Python and cutting-edge AI technologies, including Large Language Models (LLMs), to automate and streamline the data management lifecycle - from intelligent metadata tagging and automated code validation to advanced data profiling and pipeline generation', 'Lead Data Engineering platform collaboration sessions and actively drive standard adoption and technical best practices', 'Proactively identify opportunities to innovate and expand platform capabilities in alignment with strategic goals', 'Champion continuous improvement through agile delivery, platform upgrades, and process innovation', 'Mentor and support engineers at all levels, fostering a strong culture of learning and technical excellence', 'M-F Eastern Business Hours required']}], 'apply_options': [{'title': 'ZipRecruiter', 'link': 'https://www.ziprecruiter.com/c/Massgeneralbrigham/Job/Platform-Principal-Data-Engineer/-in-Hampton,NH?jid=c4642fcf15d018bd&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Talentify', 'link': 'https://www.talentify.io/job/platform-principal-data-engineer-hampton-new-hampshire-us-mass-general-brigham-rq4025205-2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobrapido', 'link': 'https://us.jobrapido.com/jobpreview/6295711447154425856?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'BeBee', 'link': 'https://us.bebee.com/job/adf7f0e016a4840a56261afd1b973eaf?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'J-O-B-Z', 'link': 'https://j-o-b-z.com/seo/job/133451026/nh/hampton/platform-principal-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Job Abstracts', 'link': 'https://jobabstracts.com/Job/Single/?id1=133451026&hash=062198017114199150008098217044227076192138162161041155095143224084048164134247120213106227075106076199161210136147150058075224001024242063191091017087064049018086096238022070123207034224211166&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Learn4Good', 'link': 'https://www.learn4good.com/jobs/hampton/new-hampshire/info_technology/4595499596/e/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobilize', 'link': 'https://www.jobilize.com/job/us-nh-hampton-platform-principal-data-engineer-mass-general-brigham?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJQbGF0Zm9ybSBQcmluY2lwYWwgRGF0YSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6Ik1hc3NnZW5lcmFsYnJpZ2hhbSIsImFkZHJlc3NfY2l0eSI6IkhhbXB0b24sIE5IIiwiaHRpZG9jaWQiOiJCVk1lWlhPZHZ2TVB1a2wzQUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': 'Principal Data Engineer', 'company_name': 'Fidelity TalentSource', 'location': 'Smithfield, NC', 'via': 'WhatJobs', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=Bqu3-Hwl2rZG1Qr_AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEOwoCMRAAUGz3CFbTCaKJCDZa-gMLEdZ-ycYxGcnOhGSE9SKeV3zFa76TxtwKsafsEhycOjhyIEYssISL9FDRFR9BGM4iIeF0F1Vz3VpbazKhqlPyxstghbGX0b6kr_-6Gl3BnJxit96sRpM5zGcnemAi_cDdJWRt5V08AjG0A2l8EqbHAq77H6s2bdSZAAAA&shmds=v1_AdeF8KjNoeDrxaj5QAuvA6zKOTrKhnTAIlAJ5SzFBZxqb9xTBw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=Bqu3-Hwl2rZG1Qr_AAAAAA%3D%3D', 'extensions': ['6 days ago', 'Full-time'], 'detected_extensions': {'posted_at': '6 days ago', 'schedule_type': 'Full-time'}, 'description': "Principal Data Engineer\nSubmit your CV and any additional required information after you have read this description by clicking on the application button.\nFidelity TalentSource is your destination for discovering your next temporary role at Fidelity Investments. We are currently sourcing for a Principal Data Engineer in Smithfield, RI!\nThe FI Data Solutions team is seeking an experienced, highly motivated and skilled Principal Data Engineer to join our Fidelity Institutional Business Enablement Technology data squad in Smithfield, RI. The data infrastructure is going through significant growth and modernization, and this is an opportunity to take a leading role in shaping FI capability in this area. The work involves solution design, data analysis, innovation and collaboration while maintaining the operational and analytical capability in FI's data platforms while testing, production rollout and quality execution on project activities in data lakes using Snowflake, AWS, and Python.\nYou will be responsible for providing leadership on a complex Territory Management Conversion & Client Infrastructure initiative that will deliver a strategic client and territory data experience that will support 2027 streamlined sales segmentation coverage models for our FI Sales clients. You will be part of a dedicated data squad partnering with CRM Sales to execute on this Phase 2 CRM workstream including end-to-end design and integration of client hierarchies, territory logic, and segmentation rules while collaborating with CRM to provide a flexible and scalable workflow that unlocks long term efficiencies for our Sales partners. We will expect strong leadership and influence in driving timely, data-driven coverage planning while reducing manual intervention and improving alignment across platforms.\nThe ideal candidate will have a passion for data and fall in love with the problem at hand, working through systems analysis and team collaborations to solve it.\nThis position requires an independent thinker and collaborator, who demonstrates analytical thinking, active listening and champions continuous improvement.\nThe Team\nThis position will be part of a critical, newly formed data squad converting the existing Territory Management experience and client infrastructure while working with our existing Data Swarm (Bsharks & Tech Mavericks) and CRM Salesforce Delivery squads to form the data integration center for customer relationship management (CRM) within FI. We provide our CRM users with essential data for their day-to-day operations and support our clients. In addition, we deliver CRM team data to operational and reporting platforms, while building lasting and insightful relationships with our consumers. The team partners closely with the Fidelity Institutional (FI) Market Readiness Team, is innovative, diverse, passionate, self-driven and customer obsessed. This is a dynamic environment that operates using mature agile practices and principles. We stress customer-obsessed thinking and a creative culture!\nThe Expertise and Skills You Bring\nBachelor's or master's Degree in a technology related field (e.g. Engineering, Computer Science, etc.) required with 10+ years of experience\n6+ years of experience in Data Warehousing, Data mart concepts & implementations\n4+ years of experience developing ELT/ETL pipelines to move data to and from Snowflake data store\n4+ years of experience using AWS services such as EC2, IAM, S3, EKS, KMS, SMS, CloudWatch, CloudFormation, etc.\n4+ years of experience in object-oriented programming languages (Strong programming skills required in Python or Java)\nYour passion for Data Analysis with the ability to navigate and master complex transactional and warehouse databases\nHands-on experience on SQL query optimization and performance tuning is preferred\nExperience in job scheduling tools (Control-M preferred)\nAdvanced SQL/Snow SQL knowledge is preferred\nStrong data modeling skills doing either Dimensional or Data Vault models\nExperience in Container technologies like Docker and Kubernetes\nExperience with DevOps, Continuous Integration and Continuous Delivery (Maven, Jenkins, Stash, Ansible, Docker)\nExperience in Agile methodologies (Kanban and SCRUM) is a plus\nProven track record to handle ambiguity and work in a fast-paced environment\nGood interpersonal skills to work with multiple teams in the organization\nThe Value You Deliver\nPassion and intellectually curiosity to learn new technologies and business areas\nExploring new technology products and using them to help our business needs\nWorking on teams to improve efficiency\nDelivering system automation by setting up continuous integration/continuous delivery pipelines\nThe hourly pay rate range for this position is $114.73-$74.73 per hour.\nPlacement in the range will vary based on job responsibilities and scope, geographic location, candidate's relevant experience, and other factors.\nDynamic Working\nFidelity's hybrid working model blends the best of both onsite and offsite work experiences. Working onsite is important for our business strategy and our culture. We also value the benefits that working offsite offers associates. Most hybrid roles require associates to work onsite all business days of every other week in a Fidelity office.\nCompany Overview\nFidelity TalentSource, formerly Veritude, is the in-house temporary staffing provider for Fidelity Investments, one of the largest and most diversified global financial services firms in the industry. We recruit individuals from a variety of backgrounds, including technology and customer service, to fill assignments across Fidelity's U.S.-based regional and investor center locations. If you would like to experience Fidelity's diverse and inclusive workplace while expanding your skills and developing your professional network, consider a role with Fidelity TalentSource.\nFor information about working at Fidelity TalentSource, visit FTSJobs.com.\nFidelity TalentSource will reasonably accommodate applicants with disabilities who need adjustments in order to complete the application or interview process. Please email us at if you would like to request an accommodation.\nInformation about Fidelity Investments\nAt Fidelity, we are focused on making our financial expertise broadly accessible and effective in helping people live the lives they want. We are a privately held company that places a high degree of value in creating and championing a work environment that attracts the best talent and reflects our commitment to our associates. We are proud of our diverse and inclusive workplace where we respect and value our associates for their unique perspectives and experiences.\nFidelity Investments and Fidelity TalentSource are equal opportunity employers.", 'job_highlights': [{'title': 'Qualifications', 'items': ['This position requires an independent thinker and collaborator, who demonstrates analytical thinking, active listening and champions continuous improvement', "Bachelor's or master's Degree in a technology related field (e.g. Engineering, Computer Science, etc.) required with 10+ years of experience", '6+ years of experience in Data Warehousing, Data mart concepts & implementations', '4+ years of experience developing ELT/ETL pipelines to move data to and from Snowflake data store', '4+ years of experience using AWS services such as EC2, IAM, S3, EKS, KMS, SMS, CloudWatch, CloudFormation, etc', '4+ years of experience in object-oriented programming languages (Strong programming skills required in Python or Java)', 'Your passion for Data Analysis with the ability to navigate and master complex transactional and warehouse databases', 'Strong data modeling skills doing either Dimensional or Data Vault models', 'Experience in Container technologies like Docker and Kubernetes', 'Experience with DevOps, Continuous Integration and Continuous Delivery (Maven, Jenkins, Stash, Ansible, Docker)', 'Proven track record to handle ambiguity and work in a fast-paced environment', 'Good interpersonal skills to work with multiple teams in the organization', 'Passion and intellectually curiosity to learn new technologies and business areas']}, {'title': 'Benefits', 'items': ['The hourly pay rate range for this position is $114.73-$74.73 per hour']}, {'title': 'Responsibilities', 'items': ["The work involves solution design, data analysis, innovation and collaboration while maintaining the operational and analytical capability in FI's data platforms while testing, production rollout and quality execution on project activities in data lakes using Snowflake, AWS, and Python", 'You will be responsible for providing leadership on a complex Territory Management Conversion & Client Infrastructure initiative that will deliver a strategic client and territory data experience that will support 2027 streamlined sales segmentation coverage models for our FI Sales clients', 'You will be part of a dedicated data squad partnering with CRM Sales to execute on this Phase 2 CRM workstream including end-to-end design and integration of client hierarchies, territory logic, and segmentation rules while collaborating with CRM to provide a flexible and scalable workflow that unlocks long term efficiencies for our Sales partners', 'We will expect strong leadership and influence in driving timely, data-driven coverage planning while reducing manual intervention and improving alignment across platforms', 'The ideal candidate will have a passion for data and fall in love with the problem at hand, working through systems analysis and team collaborations to solve it', 'This position will be part of a critical, newly formed data squad converting the existing Territory Management experience and client infrastructure while working with our existing Data Swarm (Bsharks & Tech Mavericks) and CRM Salesforce Delivery squads to form the data integration center for customer relationship management (CRM) within FI', 'We provide our CRM users with essential data for their day-to-day operations and support our clients', 'In addition, we deliver CRM team data to operational and reporting platforms, while building lasting and insightful relationships with our consumers', 'Exploring new technology products and using them to help our business needs', 'Working on teams to improve efficiency', 'Delivering system automation by setting up continuous integration/continuous delivery pipelines']}], 'apply_options': [{'title': 'WhatJobs', 'link': 'https://www.whatjobs.com/jobs/principal-data-engineer?id=2254667200&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'BeBee', 'link': 'https://us.bebee.com/job/d560a225fe4e34dfa9f79b5f6f6d147d?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJQcmluY2lwYWwgRGF0YSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IkZpZGVsaXR5IFRhbGVudFNvdXJjZSIsImFkZHJlc3NfY2l0eSI6IlNtaXRoZmllbGQsIE5DIiwiaHRpZG9jaWQiOiJCcXUzLUh3bDJyWkcxUXJfQUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': 'Data Engineer III', 'company_name': 'Old National Bank', 'location': 'Lake Elmo, MN', 'via': 'ZipRecruiter', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=cBLkit1JUUCCo2NJAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXLMQ6CQBAF0NhyBKsfK2OUNSY20hmJwSgegQw4WZBlhjBbcAcvrTave8lnkawvFAm5-E6YJxRFgR1uWsOYpqaFCq6qPvAya2Mc7eScWUi9RYpdkzY6OBWudXZvre1PZS1NPAaKXB2O-zkdxW9Wz_BC-SsqFHAm6dEJ7tQz8jDoFo_yC6GUkyuOAAAA&shmds=v1_AdeF8KicdQD470jAjJk8xZQ_UmpyykCy_w4zXrne0iIbQa_abg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=cBLkit1JUUCCo2NJAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965bff2593a58c4e71a36/images/fcee195cdacc4b714565f0a71338f786e95cd69ab1207eb81acdb9891d7c9454.png', 'extensions': ['62,300â€“122,430 a year', 'Full-time', 'Dental insurance', 'Health insurance'], 'detected_extensions': {'salary': '62,300â€“122,430 a year', 'schedule_type': 'Full-time', 'dental_coverage': True, 'health_insurance': True}, 'description': "Overview\n\nOld National Bank has been serving clients and communities since 1834. With over $70 billion in total assets, we are a regional powerhouse deeply rooted in the communities we serve. As a trusted partner, we thrive on helping our clients achieve their goals and dreams, and we are committed to social responsibility and investing in our communities through volunteering and charitable giving.\n\nWe continually seek highly motivated and talented individuals as our people are critical to our success. In return, we offer competitive compensation with our salary and incentive program, in addition to medical, dental, and vision insurance. 401K, continuing education opportunities and an employee assistance program are also included in our benefit suite. Old National also offers a variety of Impact Network Groups led by team members who are passionate about driving engagement, creating awareness of diverse backgrounds and experiences, and building inclusion across the organization. We offer a unique opportunity to join a growing, community and client-focused company that is firmly rooted in its core values.\n\nResponsibilities\n\nWe are seeking a Data Engineer with expertise in Azure, AWS, Databricks, and Python to design, build, and optimize scalable data pipelines and products. This role will focus on developing modern cloud-based data solutions that support analytics, reporting, system integrations, and AI/ML. The ideal candidate will leverage Python and SQL for automation and data processing while ensuring reliability, security, and performance across platforms. Working closely with analysts, data scientists, and business stakeholders, this position plays a key role in delivering high-quality and efficient data solutions.\n\nSalary Range\n\nThe salary range for this position is $62,300 - $122,430 per year. The base salary indicated for this position reflects the compensation range applicable to all levels of the role across the United States. Actual salary offers within this range may vary based on a number of factors, including the specific responsibilities of the position, the candidate's relevant skills and professional experience, educational qualifications, and geographic location.\n\nKey Accountabilities\n\nKey Competencies for Position\nâ€¢ Ability to work independently and be able to collaborate and guide other team members.\nâ€¢ Critical thinking and problem-solving skills to ensure the right-sized solution is developed for the task at hand.\nâ€¢ Technical passion to move the bank towards modern data platform principles.\n\nDuties/Responsibilities\nâ€¢ Design, development, and maintenance (enhancements and maintenance) of enterprise data products and pipelines.\nâ€¢ Design and development of repeatable frameworks and functions to be leveraged by other data engineers within the bank.\nâ€¢ Focus on Operational Excellence- ensuring we are monitoring, validating, and communicating the health and status of our data products.\nâ€¢ Collaborate with Enterprise Architecture, Info Security and Data Governance organizations.\nâ€¢ Confer with relevant team members where necessary. Study systems flow, data usage, and work processes.\nâ€¢ Deliver functional data products that have been thoroughly tested.\nâ€¢ Follow the development team's SDLC process. Accurately estimate time required to complete projects and tasks.\nâ€¢ Meet mutually agreed upon deadlines for completion of modules throughout the program development.\nâ€¢ Work closely with the existing platform owners to design, model, develop, and maintain existing and new objects/products required for all business solutions.\n\nSkills and Qualification\nâ€¢ Bachelor's degree required.\nâ€¢ Over 5 years of experience in data engineering, including developing ETL/ELT, data pipelines, data lakes, data warehouses, and data integration solutions.\nâ€¢ Over 3 years of experience on data engineering teams, preferably in a financial services or banking domain.\nâ€¢ 3-5 years' experience Cloud platforms (AWS, Azure, GCP, Databricks,Snowflake) Databricks preferred and Python programming or advanced SQL coding and performance tuning.\nâ€¢ Experience in a product mindset, using agile frameworks such as Scrum or Kanban.\nâ€¢ Expertise in building scalable, reliable, and secure data products and platforms, using metadata driven, event driven, and API driven approaches.\nâ€¢ Experience in scripting tools such as Python.\nâ€¢ Experience in orchestration tools or job schedulers, such as Azure Data Factory, Airflow, or Luigi.\nâ€¢ Experience in data modeling/mastering, data quality, data governance, and data security standards and best practices.\nâ€¢ Experience in code repositories and version control tools such as Azure DevOps, Git, etc.\nâ€¢ Experience in analytical and data visualization tools such as Power BI, Tableau, or Qlik.\nâ€¢ Experience in working with Salesforce data and digital banking data is a plus.\nâ€¢ Very good working experience of T-SQL. Proficiency in complex stored procedures, user defined functions, and query optimization.\n\nOld National is proud to be an equal opportunity employer focused on fostering an inclusive workplace and committed to hiring a workforce comprised of diverse backgrounds, cultures and thinking styles.\n\nAs such, all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, protected veteran status, status as a qualified individual with disability, sexual orientation, gender identity or any other characteristic protected by law.\n\nWe do not accept resumes from external staffing agencies or independent recruiters for any of our openings unless we have an agreement signed by the Director of Talent Acquisition, SVP, to fill a specific position.\n\nOur culture is firmly rooted in our core values.\n\nWe are optimistic. We are collaborative. We are inclusive. We are agile. We are ethical.\n\nWe are Old National Bank. Join our team.", 'job_highlights': [{'title': 'Qualifications', 'items': ['Ability to work independently and be able to collaborate and guide other team members', 'Critical thinking and problem-solving skills to ensure the right-sized solution is developed for the task at hand', 'Technical passion to move the bank towards modern data platform principles', "Bachelor's degree required", 'Over 5 years of experience in data engineering, including developing ETL/ELT, data pipelines, data lakes, data warehouses, and data integration solutions', 'Over 3 years of experience on data engineering teams, preferably in a financial services or banking domain', 'Experience in a product mindset, using agile frameworks such as Scrum or Kanban', 'Expertise in building scalable, reliable, and secure data products and platforms, using metadata driven, event driven, and API driven approaches', 'Experience in scripting tools such as Python', 'Experience in orchestration tools or job schedulers, such as Azure Data Factory, Airflow, or Luigi', 'Experience in data modeling/mastering, data quality, data governance, and data security standards and best practices', 'Experience in code repositories and version control tools such as Azure DevOps, Git, etc', 'Experience in analytical and data visualization tools such as Power BI, Tableau, or Qlik', 'Very good working experience of T-SQL', 'Proficiency in complex stored procedures, user defined functions, and query optimization']}, {'title': 'Benefits', 'items': ['In return, we offer competitive compensation with our salary and incentive program, in addition to medical, dental, and vision insurance', '401K, continuing education opportunities and an employee assistance program are also included in our benefit suite', 'The salary range for this position is $62,300 - $122,430 per year', 'The base salary indicated for this position reflects the compensation range applicable to all levels of the role across the United States', "Actual salary offers within this range may vary based on a number of factors, including the specific responsibilities of the position, the candidate's relevant skills and professional experience, educational qualifications, and geographic location"]}, {'title': 'Responsibilities', 'items': ['We are seeking a Data Engineer with expertise in Azure, AWS, Databricks, and Python to design, build, and optimize scalable data pipelines and products', 'This role will focus on developing modern cloud-based data solutions that support analytics, reporting, system integrations, and AI/ML', 'The ideal candidate will leverage Python and SQL for automation and data processing while ensuring reliability, security, and performance across platforms', 'Working closely with analysts, data scientists, and business stakeholders, this position plays a key role in delivering high-quality and efficient data solutions', 'Design, development, and maintenance (enhancements and maintenance) of enterprise data products and pipelines', 'Design and development of repeatable frameworks and functions to be leveraged by other data engineers within the bank', 'Focus on Operational Excellence- ensuring we are monitoring, validating, and communicating the health and status of our data products', 'Collaborate with Enterprise Architecture, Info Security and Data Governance organizations', 'Confer with relevant team members where necessary', 'Study systems flow, data usage, and work processes', 'Deliver functional data products that have been thoroughly tested', "Follow the development team's SDLC process", 'Accurately estimate time required to complete projects and tasks', 'Meet mutually agreed upon deadlines for completion of modules throughout the program development', 'Work closely with the existing platform owners to design, model, develop, and maintain existing and new objects/products required for all business solutions']}], 'apply_options': [{'title': 'ZipRecruiter', 'link': 'https://www.ziprecruiter.com/c/Old-National-Bank/Job/Data-Engineer-III/-in-Lake-Elmo,MN?jid=cb0c300053f19e0d&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'LinkedIn', 'link': 'https://www.linkedin.com/jobs/view/data-engineer-iii-at-old-national-bank-4300076365?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'SimplyHired', 'link': 'https://www.simplyhired.com/job/vRTIIc4iBoj6iSXmv3aLIfF7xBBWtbYg1njk2P3TnS__QqqkRZc6Xg?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Snagajob', 'link': 'https://www.snagajob.com/jobs/1162629480?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Lensa', 'link': 'https://lensa.com/job-v1/old-national-bank/lake-elmo-mn/data-engineer/dfcf5db3532aeb46d6fcf9cc1bccee55?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'BeBee', 'link': 'https://us.bebee.com/job/0508b1a4d51d3e41b8fabfd6019fe589?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobilize', 'link': 'https://www.jobilize.com/job/us-mn-lake-elmo-data-engineer-iii-old-national-bancorp-hiring-now?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIElJSSIsImNvbXBhbnlfbmFtZSI6Ik9sZCBOYXRpb25hbCBCYW5rIiwiYWRkcmVzc19jaXR5IjoiTGFrZSBFbG1vLCBNTiIsImh0aWRvY2lkIjoiY0JMa2l0MUpVVUNDbzJOSkFBQUFBQT09IiwiaGwiOiJlbiJ9'}, {'title': 'Sr. Data Engineer', 'company_name': 'CVS Health', 'location': 'Silver Springs, NV', 'via': 'WhatJobs', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=7ylqRy7u5p49sv0aAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNsQoCMQwAUFzvE5wyimgrgouOKoqDS-HWI3eEtlKT0gTxF_xrdXnr6z6zbhGagxMawpljZqIGa7jJCErYpgTCcBGJheaHZFZ1771qcVENLU9ukqcXplHe_iGj_hk0YaNa0GjY7jZvVzku4dgHuBIWS5AZQi6v3xRqyxx1Bff-C57qNoaMAAAA&shmds=v1_AdeF8KglvIW7fptSDGDJli_kqYwoRYaxiYhFp3PVdSx9gSfkEg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=7ylqRy7u5p49sv0aAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965bff2593a58c4e71a36/images/fcee195cdacc4b71a4a4aace01b8ac28f805a36d1196d2405cd9be41d8d3cada.png', 'extensions': ['4 days ago', 'Full-time', 'Health insurance', 'Paid time off'], 'detected_extensions': {'posted_at': '4 days ago', 'schedule_type': 'Full-time', 'health_insurance': True, 'paid_time_off': True}, 'description': "At CVS Health, we're building a world of health around every consumer and surrounding ourselves with dedicated colleagues who are passionate about transforming health care.\nAs the nation's leading health solutions company, we reach millions of Americans through our local presence, digital channels and more than 300,000 purpose-driven colleagues - caring for people where, when and how they choose in a way that is uniquely more connected, more convenient and more compassionate. And we do it all with heart, each and every day.\n\u200b **Position Summary**\nWe're seeking a Sr. Data Engineer to design and implement data pipelines that power analytical capabilities. This hands-on role requires an understanding of data engineering best practices and the ability to translate business requirements into technical solutions.\nYou will be part of a dedicated team creating datasets for analytic and data science workloads.\nKey Responsibilities:\n+ Data Pipeline Development: Design and build ETL/ELT data pipelines to ingest, process, and transform large datasets from multiple sources.\n+ Performance Optimization: Implement best practices for performance tuning, partitioning, and clustering to optimize data queries and reduce costs.\n+ Data Quality & Governance: Establish and enforce data quality standards, data governance frameworks, and security policies for data storage and access.\n+ Data Modeling & Architecture: Develop and optimize data models and schemas to support analytics, reporting, and machine learning requirements.\n+ Data Integration & Transformation: Collaborate with data scientists and analysts to design data solutions that integrate with BI tools and machine learning models.\n+ Documentation & Knowledge Sharing: Create comprehensive documentation for data pipelines, workflows, and processes. Share best practices and mentor junior data engineers.\n+ Design and architect data infrastructure analytical workloads.\nâ€¢ *Required Qualifications**\n+ 5+ years of applicable work experience\n+ Proficiency in Python, specifically with ETL pipelines.\n+ Strong proficiency in SQL and experience in developing complex queries.\n+ Familiarity with pySpark, DBT, or other similar frameworks.\n+ Experience deploying data pipelines in a cloud environment (Azure, AWS, GCP).\n+ Understanding of data warehousing concepts, dimensional modeling, and building data marts.\n+ Excellent communication and interpersonal skills, with the ability to collaborate effectively with data scientists, analysts, and product owners.\nâ€¢ *Preferred Qualifications**\n+ Knowledge of data governance best practices in a cloud environment.\n+ Experience with machine learning flows on GCP.\n+ Experience with data design in BigQuery\n+ Experience working with the Epic data model.\n+ Experience working with healthcare data (Tuva or OMOP Models a plus).\nâ€¢ *Education and Experience**\n+ College degree or certification in related fields\nâ€¢ *Anticipated Weekly Hours**\n40\nâ€¢ *Time Type**\nFull time\nâ€¢ *Pay Range**\nThe typical pay range for this role is:\n$83,430.00 - $222,480.00\nThis pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.\nOur people fuel our future. Our teams reflect the customers, patients, members and communities we serve and we are committed to fostering a workplace where every colleague feels valued and that they belong.\nâ€¢ *Great benefits for great people**\nWe take pride in our comprehensive and competitive mix of pay and benefits - investing in the physical, emotional and financial wellness of our colleagues and their families to help them be the healthiest they can be. In addition to our competitive wages, our great benefits include:\n+ **Affordable medical plan options,** a **401(k) plan** (including matching company contributions), and an **employee stock purchase plan** .\n+ **No-cost programs for all colleagues** including wellness screenings, tobacco cessation and weight management programs, confidential counseling and financial coaching.\n+ **Benefit solutions that address the different needs and preferences of our colleagues** including paid time off, flexible work schedules, family leave, dependent care resources, colleague assistance programs, tuition assistance, retiree medical access and many other benefits depending on eligibility.\nFor more information, visit anticipate the application window for this opening will close on: 11/14/2025\nQualified applicants with arrest or conviction records will be considered for employment in accordance with all federal, state and local laws.\nWe are an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring, promotion, or any other personnel action based on race, ethnicity, color, national origin, sex/gender, sexual orientation, gender identity or expression, religion, age, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law.", 'job_highlights': [{'title': 'Qualifications', 'items': ['This hands-on role requires an understanding of data engineering best practices and the ability to translate business requirements into technical solutions', '5+ years of applicable work experience', 'Proficiency in Python, specifically with ETL pipelines', 'Strong proficiency in SQL and experience in developing complex queries', 'Familiarity with pySpark, DBT, or other similar frameworks', 'Experience deploying data pipelines in a cloud environment (Azure, AWS, GCP)', 'Understanding of data warehousing concepts, dimensional modeling, and building data marts', 'Excellent communication and interpersonal skills, with the ability to collaborate effectively with data scientists, analysts, and product owners', 'Knowledge of data governance best practices in a cloud environment', 'Experience with machine learning flows on GCP', 'Experience with data design in BigQuery', 'Experience working with the Epic data model', 'College degree or certification in related fields']}, {'title': 'Benefits', 'items': ['*Pay Range**', '$83,430.00 - $222,480.00', 'This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls', 'The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors', 'This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above', '*Great benefits for great people**', 'We take pride in our comprehensive and competitive mix of pay and benefits - investing in the physical, emotional and financial wellness of our colleagues and their families to help them be the healthiest they can be', 'In addition to our competitive wages, our great benefits include:', '**Affordable medical plan options,** a **401(k) plan** (including matching company contributions), and an **employee stock purchase plan** ', '**No-cost programs for all colleagues** including wellness screenings, tobacco cessation and weight management programs, confidential counseling and financial coaching', '**Benefit solutions that address the different needs and preferences of our colleagues** including paid time off, flexible work schedules, family leave, dependent care resources, colleague assistance programs, tuition assistance, retiree medical access and many other benefits depending on eligibility']}, {'title': 'Responsibilities', 'items': ['Data Engineer to design and implement data pipelines that power analytical capabilities', 'You will be part of a dedicated team creating datasets for analytic and data science workloads', 'Data Pipeline Development: Design and build ETL/ELT data pipelines to ingest, process, and transform large datasets from multiple sources', 'Performance Optimization: Implement best practices for performance tuning, partitioning, and clustering to optimize data queries and reduce costs', 'Data Quality & Governance: Establish and enforce data quality standards, data governance frameworks, and security policies for data storage and access', 'Data Modeling & Architecture: Develop and optimize data models and schemas to support analytics, reporting, and machine learning requirements', 'Data Integration & Transformation: Collaborate with data scientists and analysts to design data solutions that integrate with BI tools and machine learning models', 'Documentation & Knowledge Sharing: Create comprehensive documentation for data pipelines, workflows, and processes', 'Share best practices and mentor junior data engineers', 'Design and architect data infrastructure analytical workloads', '*Anticipated Weekly Hours**']}], 'apply_options': [{'title': 'WhatJobs', 'link': 'https://www.whatjobs.com/jobs/sr-data-engineer?id=2251585557&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTci4gRGF0YSBFbmdpbmVlciIsImNvbXBhbnlfbmFtZSI6IkNWUyBIZWFsdGgiLCJhZGRyZXNzX2NpdHkiOiJTaWx2ZXIgU3ByaW5ncywgTlYiLCJodGlkb2NpZCI6Ijd5bHFSeTd1NXA0OXN2MGFBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'AI & DATA ENGINEER', 'company_name': "Fayetteville's Hometown Utility", 'location': 'Fayetteville, NC', 'via': 'ZipRecruiter', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=pVopIjVDCiJzHh0YAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_03KsQ6CMBAA0LjyCU43aTRKjYZFp0YRcWAwOpNCLlBTeg13UfgY_9W4ubzpRZ9JtNA5zOCk7xrSIsuLNL3BGq5UAaPp6xbIQ0bUOJweWpHAe6WYXdywGLF1XFOnyGNFg3pSxT9Kbk2PwRnBcptshjj4Zrk7mxFF8GWdwznDhToUent4iHVWRrAe_ssKiuMXiISxraAAAAA&shmds=v1_AdeF8KhZJGo5_Ih3RmfK-aaFOtFoZROuh7moiPEZCikYd_TxjA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=pVopIjVDCiJzHh0YAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965bff2593a58c4e71a36/images/fcee195cdacc4b71e15755f710de906586f24cb35600ab1b88d2724463ad7a3a.png', 'extensions': ['79,165.72â€“104,300.84 a year', 'Full-time'], 'detected_extensions': {'salary': '79,165.72â€“104,300.84 a year', 'schedule_type': 'Full-time'}, 'description': "Job Description\n\nFayetteville Public Works Commission is seeking a technically skilled and forward-thinking AI & Data Engineer to join our Data Analytics team. This role is essential in designing, implementing, and maintaining scalable data infrastructure to support advanced analytics, AI, and machine learning initiatives. If you're passionate about building data-driven solutions and leveraging AI to solve real-world challenges, we encourage you to apply.\n\nKey Responsibilities\nâ€¢ Develop business intelligence dashboards, reports, and visualizations.\nâ€¢ Create and optimize data models for analytics and reporting.\nâ€¢ Build and maintain scalable data pipelines, data lakes, and data warehouses.\nâ€¢ Design and implement AI/ML models for predictive analytics and decision-making.\nâ€¢ Collaborate with cross-functional teams to translate business needs into data solutions.\nâ€¢ Support deployment of BI tools such as Oracle Analytics Cloud and BI Publisher.\nâ€¢ Conduct data validation, performance tuning, and troubleshooting.\nâ€¢ Implement CI/CD pipelines and contribute to data governance and metadata management.\nâ€¢ Perform other related duties as assigned.\n\nRequired Skills & Abilities\nâ€¢ Proficiency in SQL, Python, and PySpark.\nâ€¢ Experience with data pipeline development and AI/ML applications.\nâ€¢ Knowledge of data warehousing, data lake architecture, and Oracle Fusion Analytics.\nâ€¢ Familiarity with machine learning, AI, and Generative AI technologies.\nâ€¢ Strong analytical, problem-solving, and communication skills.\nâ€¢ Ability to work collaboratively in a fast-paced, cross-functional environment.\n\nEducation, Experience, and Certifications\nâ€¢ Graduation from an accredited university or college with a degree in business, technology, data management, or a related field and/or an equivalent combination of education, training, and experience.\nâ€¢ 1-3+ years of relevant experience in AI & Data Engineer\nâ€¢ Must possess and maintain one or more of the following certifications:\nâ€¢ Oracle Cloud Infrastructure AI Foundations Associate\nâ€¢ Oracle Cloud Infrastructure Developer Professional\nâ€¢ Oracle Cloud Infrastructure Data Science Professional\nâ€¢ Oracle Cloud Infrastructure Generative AI Certified Professional\nâ€¢ Or an equivalent certification\n\nPhysical Requirements\nâ€¢ Prolonged periods of sitting and working on a computer.\nâ€¢ No significant lifting (>15 lbs).\n\nWhy Join PWC?\n\nAt Fayetteville Public Works Commission, we are committed to innovation, sustainability, and service excellence. Join a team that values professional growth, collaboration, and the opportunity to make a meaningful impact in the community.\n\nEMPLOYER'S RIGHTS: This job description is general and illustrative of the kind of duties required of this position. It is not exhaustive and does not contain a detailed description of all the duties that may be assigned to the incumbent occupying this position.\nâ€¢ Applicants must be currently authorized to work in the United States on a full-time basis.\nâ€¢ PWC does not offer employment-based visa sponsorship now or in the future.\n\nGrade 413X $79,165.72 - $104,300.84\n\n#FPWCSJ", 'job_highlights': [{'title': 'Qualifications', 'items': ['Proficiency in SQL, Python, and PySpark', 'Experience with data pipeline development and AI/ML applications', 'Knowledge of data warehousing, data lake architecture, and Oracle Fusion Analytics', 'Familiarity with machine learning, AI, and Generative AI technologies', 'Strong analytical, problem-solving, and communication skills', 'Ability to work collaboratively in a fast-paced, cross-functional environment', 'Education, Experience, and Certifications', 'Graduation from an accredited university or college with a degree in business, technology, data management, or a related field and/or an equivalent combination of education, training, and experience', '1-3+ years of relevant experience in AI & Data Engineer', 'Must possess and maintain one or more of the following certifications:', 'Oracle Cloud Infrastructure AI Foundations Associate', 'Oracle Cloud Infrastructure Developer Professional', 'Oracle Cloud Infrastructure Data Science Professional', 'Oracle Cloud Infrastructure Generative AI Certified Professional', 'Or an equivalent certification', 'Prolonged periods of sitting and working on a computer', 'No significant lifting (>15 lbs)', 'Applicants must be currently authorized to work in the United States on a full-time basis']}, {'title': 'Benefits', 'items': ['Grade 413X $79,165.72 - $104,300.84']}, {'title': 'Responsibilities', 'items': ['This role is essential in designing, implementing, and maintaining scalable data infrastructure to support advanced analytics, AI, and machine learning initiatives', "If you're passionate about building data-driven solutions and leveraging AI to solve real-world challenges, we encourage you to apply", 'Develop business intelligence dashboards, reports, and visualizations', 'Create and optimize data models for analytics and reporting', 'Build and maintain scalable data pipelines, data lakes, and data warehouses', 'Design and implement AI/ML models for predictive analytics and decision-making', 'Collaborate with cross-functional teams to translate business needs into data solutions', 'Support deployment of BI tools such as Oracle Analytics Cloud and BI Publisher', 'Conduct data validation, performance tuning, and troubleshooting', 'Implement CI/CD pipelines and contribute to data governance and metadata management', 'Perform other related duties as assigned']}], 'apply_options': [{'title': 'ZipRecruiter', 'link': 'https://www.ziprecruiter.com/c/Fayettevilles-Hometown-Utility/Job/AI-&-DATA-ENGINEER/-in-Fayetteville,NC?jid=1543bcffa5a3b424&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Ladders', 'link': 'https://www.theladders.com/job/ai-data-engineer-fayetteville-s-hometown-utility-fayetteville-nc_81883353?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobs Trabajo.org', 'link': 'https://us.trabajo.org/job-3647-4e4d460d27195219e78e60e69af897d5?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobilize', 'link': 'https://www.jobilize.com/job/us-nc-fayetteville-ai-data-engineer-s-hometown-utility-hiring-now?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJBSSBcdTAwMjYgREFUQSBFTkdJTkVFUiIsImNvbXBhbnlfbmFtZSI6IkZheWV0dGV2aWxsZSdzIEhvbWV0b3duIFV0aWxpdHkiLCJhZGRyZXNzX2NpdHkiOiJGYXlldHRldmlsbGUsIE5DIiwiaHRpZG9jaWQiOiJwVm9wSWpWRENpSnpIaDBZQUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': 'Senior Data Engineer, VP', 'company_name': 'Apple Bank', 'location': 'New York, NY', 'via': 'ZipRecruiter', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=qMFVBwaqlMnojs2UAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCMBAAUFz7CZ1ultqI4KKTRREciiAIncolHGlsvAu5gP0Gv1p8w6u-q8o8iINkOGNBuLAPTJQbeN5hAzexoITZTSAMVxEfqT5OpSQ9GKMaW68FS3Ctk7cRJiuLeYnVf6NOmClFLDTu9tulTezX9SmlSNAhzxAYevrAIHluoB9-NRoj-40AAAA&shmds=v1_AdeF8Ki8cd18D3wEfQRB3vXz-BvJicvmz5IZydYMtkfAMRKQCg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=qMFVBwaqlMnojs2UAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965bff2593a58c4e71a36/images/fcee195cdacc4b7155661891ec12a9b57302321e7694a55f4d8c4c2bf010254d.jpeg', 'extensions': ['170Kâ€“240K a year', 'Full-time'], 'detected_extensions': {'salary': '170Kâ€“240K a year', 'schedule_type': 'Full-time'}, 'description': 'Hybrid/Manhattan, NY\nPay Range: $170,000 - $240,000\n\nThe Senior Data Engineer plays a key role in the design, development, and expansion of databases, data orchestrations, and business intelligence for Apple Bank. This position assists with the build, maintenance, and optimization of data pipelines into our Enterprise Data Warehouse (EDW) and creates extracts for vendors and partners. The Senior Engineer also develops stage tables, operational data store (ODS) databases and tables, data marts, and all ETL processes. The work product provided by the Engineer is required to be fit-for-purpose for each data source, repeatable, and automatable, which involves stored procedures, functions, data analyses, cleansing, validations, and reporting.\n\nThe role collaborates closely with IT infrastructure colleagues, database administrators, business stakeholders, data operations teammates, project managers, vendors, and analysts embedded in our businesses. The successful candidate must be able to ask incisive business questions, tackle complexity, and own delivery. Future opportunities may include sourcing third party and public data, additional source-system integrations, and hybrid cloud proof-of-concepts.\n\nESSENTIAL DUTIES & RESPONSIBILITIES\nâ€¢ Establish solution architecture, SQL Server databases, and schemas.\nâ€¢ Design and build corresponding SSIS ETLs, solving business needs and seizing opportunities.\nâ€¢ Perform data profiling to identify and understand projects thoroughly.\nâ€¢ Map data from source to repository.\nâ€¢ Build and automate data extracts for vendors and partners to specifications.\nâ€¢ Understand and coordinate the granting of permissions to service and proxy accounts to ensure inbound files, all data movement, and output to various destination folders is frictionless.\nâ€¢ Serve as an escalation point for BI analysts with obstacles on complex ad hocs and projects.\nâ€¢ Heavily contribute to broader enterprise data management decisions and initiatives.\nâ€¢ Perform peer code reviews, and other duties as requested.\n\nSKILLS, EDUCATION, & EXPERIENCE\nâ€¢ Bachelorâ€™s degree required.\nâ€¢ 10+ years of progressive experience.\nâ€¢ Proficiency in MS SQL Server, SSIS, Visual Studio, SSMS, and database administration.\nâ€¢ At least 6 years of experience as an ETL developer, specifically into MS SQL Server.\nâ€¢ 8+ years overall experience in ETL design, implementation, and maintenance, including for full and incremental loads, and slowly changing dimensions.\nâ€¢ Some experience with cloud database platforms.\nâ€¢ Experience developing with C# and (desirable) Python.\nâ€¢ Experience combining and de-duping data sources.\nâ€¢ Domain knowledge and experience in banking services.\nâ€¢ Experience with FIS Global platforms and their Business Intelligence Center (BIC) a plus.\n\nVisa sponsorship not available.\n\nWe are an equal opportunity employer and do not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, disability, military and/or veteran status, or any other Federal or State legally-protected classes.', 'job_highlights': [{'title': 'Qualifications', 'items': ['The successful candidate must be able to ask incisive business questions, tackle complexity, and own delivery', 'Bachelorâ€™s degree required', '10+ years of progressive experience', 'Proficiency in MS SQL Server, SSIS, Visual Studio, SSMS, and database administration', 'At least 6 years of experience as an ETL developer, specifically into MS SQL Server', '8+ years overall experience in ETL design, implementation, and maintenance, including for full and incremental loads, and slowly changing dimensions', 'Some experience with cloud database platforms', 'Experience combining and de-duping data sources', 'Domain knowledge and experience in banking services']}, {'title': 'Benefits', 'items': ['Pay Range: $170,000 - $240,000']}, {'title': 'Responsibilities', 'items': ['The Senior Data Engineer plays a key role in the design, development, and expansion of databases, data orchestrations, and business intelligence for Apple Bank', 'This position assists with the build, maintenance, and optimization of data pipelines into our Enterprise Data Warehouse (EDW) and creates extracts for vendors and partners', 'The Senior Engineer also develops stage tables, operational data store (ODS) databases and tables, data marts, and all ETL processes', 'The work product provided by the Engineer is required to be fit-for-purpose for each data source, repeatable, and automatable, which involves stored procedures, functions, data analyses, cleansing, validations, and reporting', 'The role collaborates closely with IT infrastructure colleagues, database administrators, business stakeholders, data operations teammates, project managers, vendors, and analysts embedded in our businesses', 'Future opportunities may include sourcing third party and public data, additional source-system integrations, and hybrid cloud proof-of-concepts', 'Establish solution architecture, SQL Server databases, and schemas', 'Design and build corresponding SSIS ETLs, solving business needs and seizing opportunities', 'Perform data profiling to identify and understand projects thoroughly', 'Map data from source to repository', 'Build and automate data extracts for vendors and partners to specifications', 'Understand and coordinate the granting of permissions to service and proxy accounts to ensure inbound files, all data movement, and output to various destination folders is frictionless', 'Serve as an escalation point for BI analysts with obstacles on complex ad hocs and projects', 'Heavily contribute to broader enterprise data management decisions and initiatives', 'Perform peer code reviews, and other duties as requested']}], 'apply_options': [{'title': 'ZipRecruiter', 'link': 'https://www.ziprecruiter.com/c/Apple-Bank/Job/Senior-Data-Engineer,-VP/-in-New-York,NY?jid=f9209ecdb2d24904&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBFbmdpbmVlciwgVlAiLCJjb21wYW55X25hbWUiOiJBcHBsZSBCYW5rIiwiYWRkcmVzc19jaXR5IjoiTmV3IFlvcmssIE5ZIiwiaHRpZG9jaWQiOiJxTUZWQndhcWxNbm9qczJVQUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': 'Data Engineer, Product Analytics', 'company_name': 'North Dakota Staffing', 'location': 'Golden Valley, ND', 'via': 'WhatJobs', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=DbYqrEnWWnDGAPkNAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWNsQrCMBBAce0nON1caiOCCDoJFcGhCIJruabXJBpzJTmh_R2_1HZ50-O97LfKDhUKwiUYF4hiAffI3VcLnAP6SZxOsIEbt5AIo7bAAa7MxtP6ZEWGdFQqJV-aJDjLpeaP4kAtj-rFbVrQJIuRBo9CzW6_HcshmDyvOYqFCt883x-Cfe-CAbfUfUcBnug9TQXU1R9yIYIYpQAAAA&shmds=v1_AdeF8Kj0c76gvz9MlmAvIQC5LZtSnJq8PI0e_rvVo4XbzsPlpA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=DbYqrEnWWnDGAPkNAAAAAA%3D%3D', 'extensions': ['6 days ago', 'Full-time'], 'detected_extensions': {'posted_at': '6 days ago', 'schedule_type': 'Full-time'}, 'description': "Data Engineer Position at Meta\n\nSummary: As a Data Engineer at Meta, you will shape the future of people-facing and business-facing products we build across our entire family of applications (Facebook, Instagram, Messenger, WhatsApp, Reality Labs, Threads). Your technical skills and analytical mindset will be utilized designing and building some of the world's most extensive data sets, helping to craft experiences for billions of people and hundreds of millions of businesses worldwide.\n\nIn this role, you will collaborate with software engineering, data science, and product management teams to design/build scalable data solutions across Meta to optimize growth, strategy, and user experience for our 3 billion plus users, as well as our internal employee community. You will be at the forefront of identifying and solving some of the most interesting data challenges at a scale few companies can match. By joining Meta, you will become part of a world-class data engineering community dedicated to skill development and career growth in data engineering and beyond.\n\nData Engineering: You will guide teams by building optimal data artifacts (including datasets and visualizations) to address key questions. You will refine our systems, design logging solutions, and create scalable data models. Ensuring data security and quality, and with a strong focus on efficiency, you will suggest architecture and development approaches and data management standards to address complex analytical problems.\n\nProduct leadership: You will use data to shape product development, identify new opportunities, and tackle upcoming challenges. You'll ensure our products add value for users and businesses, by prioritizing projects, and driving innovative solutions to respond to challenges or opportunities.\n\nCommunication and influence: You won't simply present data, but tell data-driven stories. You will convince and influence your partners using clear insights and recommendations. You will build credibility through structure and clarity, and be a trusted strategic partner.\nRequired Skills\n\nData Engineer, Product Analytics\n\nResponsibilities: 1. Conceptualize and own the data architecture for multiple large-scale projects, while evaluating design and operational cost-benefit tradeoffs within systems 2. Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve 3. Collaborate with engineers, product managers, and data scientists to understand data needs, representing key data insights in a meaningful way 4. Define and manage Service Level Agreements for all data sets in allocated areas of ownership 5. Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership 6. Design, build, and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains 7. Solve our most challenging data integration problems, utilizing optimal Extract, Transform, Load (ETL) patterns, frameworks, query techniques, sourcing from structured and unstructured data sources 8. Assist in owning existing processes running in production, optimizing complex code through advanced algorithmic concepts 9. Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts 10. Influence product and cross-functional teams to identify data opportunities to drive impact 11. Mentor team members by giving/receiving actionable feedback\nMinimum Qualifications\n\n12. Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience 13. Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent 14. 4+ years of experience where the primary responsibility involves working with data. This could include roles such as data analyst, data scientist, data engineer, or similar positions 15. 4+ years of experience with SQL, ETL, data modeling, and at least one programming language (e.g., Python, C++, C#, Scala, etc.)\nPreferred Qualifications\n\n16. Master's or Ph.D degree in a STEM field\n\nPublic Compensation: $145,000/year to $204,000/year + bonus + equity + benefits\n\nIndustry: Internet\n\nEqual Opportunity: Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment. Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at", 'job_highlights': [{'title': 'Qualifications', 'items': ['Data Engineer, Product Analytics', "Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience 13", "Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent 14. 4+ years of experience where the primary responsibility involves working with data", 'This could include roles such as data analyst, data scientist, data engineer, or similar positions 15. 4+ years of experience with SQL, ETL, data modeling, and at least one programming language (e.g., Python, C++, C#, Scala, etc.)', 'Meta participates in the E-Verify program in certain locations, as required by law']}, {'title': 'Benefits', 'items': ['Public Compensation: $145,000/year to $204,000/year + bonus + equity + benefits']}, {'title': 'Responsibilities', 'items': ['Summary: As a Data Engineer at Meta, you will shape the future of people-facing and business-facing products we build across our entire family of applications (Facebook, Instagram, Messenger, WhatsApp, Reality Labs, Threads)', "Your technical skills and analytical mindset will be utilized designing and building some of the world's most extensive data sets, helping to craft experiences for billions of people and hundreds of millions of businesses worldwide", 'In this role, you will collaborate with software engineering, data science, and product management teams to design/build scalable data solutions across Meta to optimize growth, strategy, and user experience for our 3 billion plus users, as well as our internal employee community', 'By joining Meta, you will become part of a world-class data engineering community dedicated to skill development and career growth in data engineering and beyond', 'Data Engineering: You will guide teams by building optimal data artifacts (including datasets and visualizations) to address key questions', 'You will refine our systems, design logging solutions, and create scalable data models', 'Ensuring data security and quality, and with a strong focus on efficiency, you will suggest architecture and development approaches and data management standards to address complex analytical problems', 'Product leadership: You will use data to shape product development, identify new opportunities, and tackle upcoming challenges', "You'll ensure our products add value for users and businesses, by prioritizing projects, and driving innovative solutions to respond to challenges or opportunities", "Communication and influence: You won't simply present data, but tell data-driven stories", 'You will convince and influence your partners using clear insights and recommendations', 'You will build credibility through structure and clarity, and be a trusted strategic partner', 'Conceptualize and own the data architecture for multiple large-scale projects, while evaluating design and operational cost-benefit tradeoffs within systems 2', 'Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve 3. Collaborate with engineers, product managers, and data scientists to understand data needs, representing key data insights in a meaningful way 4', 'Define and manage Service Level Agreements for all data sets in allocated areas of ownership 5', 'Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership 6', 'Design, build, and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains 7', 'Solve our most challenging data integration problems, utilizing optimal Extract, Transform, Load (ETL) patterns, frameworks, query techniques, sourcing from structured and unstructured data sources 8', 'Assist in owning existing processes running in production, optimizing complex code through advanced algorithmic concepts 9', 'Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts 10', 'Influence product and cross-functional teams to identify data opportunities to drive impact 11', 'Mentor team members by giving/receiving actionable feedback']}], 'apply_options': [{'title': 'WhatJobs', 'link': 'https://www.whatjobs.com/jobs/data-modeling?id=2254699951&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyLCBQcm9kdWN0IEFuYWx5dGljcyIsImNvbXBhbnlfbmFtZSI6Ik5vcnRoIERha290YSBTdGFmZmluZyIsImFkZHJlc3NfY2l0eSI6IkdvbGRlbiBWYWxsZXksIE5EIiwiaHRpZG9jaWQiOiJEYllxckVuV1duREdBUGtOQUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': 'Lead Data Engineer - Financial Data, Compliance & Business Intelligence', 'company_name': 'Crowe LLP', 'location': 'Miami, FL', 'via': 'ZipRecruiter', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=HbnLs4Eglyfjre53AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_x2KuwrCQBAAsc0HWFhtZSExEcFGOx8RJYJfYNjE5bJy2Q23J_pj_p_RZoqZST6j5FYS3mGPEeEgjoUowBwKFpSG0f9LCjvtes-DIpjC9mnDZwYnieQ9O_r5OZy1BiMMTQsqcFR1niabNsbe1nlu5jNnESM3WaNdrkK1vvOH1vZDZS0G6j1GqparxTvrxc3Gu6AvgrK8AgtcGDtOoSi_c0VpyLgAAAA&shmds=v1_AdeF8KgVsvdP8Phv-7Hpqfpgf5vnfFZBIccQOADvnF8mF4IdVw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=HbnLs4Eglyfjre53AAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965bff2593a58c4e71a36/images/fcee195cdacc4b719f71b025220918366e48727ec43b3395bf378b7817da5f71.jpeg', 'extensions': ['23 days ago', 'Full-time'], 'detected_extensions': {'posted_at': '23 days ago', 'schedule_type': 'Full-time'}, 'description': 'Your Journey at Crowe Starts Here:\n\nAt Crowe, you can build a meaningful and rewarding career. With real flexibility to balance work with life moments, you\'re trusted to deliver results and make an impact. We embrace you for who you are, care for your well-being, and nurture your career. Everyone has equitable access to opportunities for career growth and leadership. Over our 80-year history, delivering excellent service through innovation has been a core part of our DNA across our audit, tax, and consulting groups. That\'s why we continuously invest in innovative ideas, such as AI-enabled insights and technology-powered solutions, to enhance our services. Join us at Crowe and embark on a career where you can help shape the future of our industry.\n\nJob Description:\n\nWe are looking for an experienced ETL Engineer to join our Data Engineering team. The ideal candidate will have a strong understanding of ETL processes and technologies, as well as experience with a variety of data sources. The ETL developer will be responsible for designing, developing, and maintaining ETL pipelines that extract, transform, and load data into our data warehouse. As a member of the Data Analytics team, you will provide meaningful firm-wide contributions that uphold the team\'s dedication to "One Crowe". We strive for an unparalleled client experience and look to you to promote our success.\n\nAdditionally, you will work as part of a team of problem solvers with extensive consulting and industry experience, helping our clients solve their complex business issues from strategy to execution.\n\nSpecific responsibilities include but are not limited to:\nâ€¢ Identify & map source data in various commercial and custom systems\nâ€¢ Design, develop, test and maintain ETL pipelines\nâ€¢ Extract, transform, and load data from a variety of data sources\nâ€¢ Work with data engineers and other stakeholders to define data requirements\nâ€¢ Troubleshoot and debug ETL pipelines\nâ€¢ Stay up-to-date on ETL technologies and best practices\nâ€¢ Formulate development estimates (task, effort, dependencies, etc.)\nâ€¢ Develop test cases and demonstrate results for assigned deliverables.\nâ€¢ Maintain and present daily/weekly status\nâ€¢ Articulate project deliverable details and activities to client and project team members\nâ€¢ Adapt to various industries and quickly learn the client\'s context, language, and jargon to clearly document and translate business requirements to development plans\nâ€¢ Maintain a professional presence and be ready to interact with the client at all times\n\nQualifications\nâ€¢ Bachelor\'s degree in computer science, information technology, or a related field\nâ€¢ 3+ years of experience in ETL development\nâ€¢ Hands on experience with Bank Secrecy Act (BSA), Financial Audits, or Anti-Money Laundering (AML) regulations and compliance requirements.\nâ€¢ Previous consulting experience or experience working with external clients\nâ€¢ Strong understanding of ETL processes and technologies\nâ€¢ Experience with a variety of data sources, including relational databases, flat files, and web APIs\nâ€¢ Experience with Python, Java, or another programming language\nâ€¢ Experience with data warehouse technologies, such as Azure, AWS, and Snowflake\nâ€¢ Excellent problem-solving and debugging skills\nâ€¢ Strong communication and teamwork skills\nâ€¢ Solid understanding of software development life cycle models as well as expert knowledge of both Agile and traditional project management principles and practices and the ability to blend them together in the right proportions to fit a project and business environment\nâ€¢ Technical background, with understanding or hands-on experience in enterprise solutions for different industries\nâ€¢ High-level knowledge of BI architecture or data warehousing\nâ€¢ Excellent written and verbal communication skills\nâ€¢ Effectively managing multiple deliverables, clients and projects in a fast-paced environment\nâ€¢ Strong problem solving and analytical skills\n\nCertifications (Preferred)\nâ€¢ Certifications in BI (ETL tools, visualization tools, cloud architectures) are expected\nâ€¢ Certifications such as CAMS (Certified Anti-Money Laundering Specialist), CFE (Certified Fraud Examiner).\n\nKey Stakeholders This Role Interacts With:\n\nInternal\nâ€¢ Senior BI Analyst\nâ€¢ Data Product Manager\nâ€¢ BI Architect / Senior Architect\nâ€¢ Data Engineers\nâ€¢ Report Developer (Power BI, Tableau)\n\nExternal\nâ€¢ C-Suite executives, business leaders, and department heads\nâ€¢ Data Owners, Data Stewards\nâ€¢ Enterprise Information Management and Data Governance Team\nâ€¢ IT Staff - Architects, DBAs, Developers, etc\n\nWe expect the candidate to uphold Crowe\'s values of Care, Trust, Courage, and Stewardship. These values define who we are. We expect all of our people to act ethically and with integrity at all times.\n\nIn compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification form upon hire. Crowe is not sponsoring for work authorization at this time.\n\nThe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. At Crowe, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current range is $104,500.00 - $208,300.00 per year.\n\nOur Benefits:\nYour exceptional people experience starts here. At Crowe, we know that great peopleare what makes a great firm. We care about our people and offer employees a comprehensive total rewards package. Learn more about what working at Crowe can mean for you!\n\nHow You Can Grow:\nWe will nurture your talent in an inclusive culture that values diversity. You will have the chance to meet on a consistent basis with your Career Coach that will guide you in your career goals and aspirations. Learn more about where talent can prosper!\n\nMore about Crowe:\nCrowe (www.crowe.com) is one of the largest public accounting, consulting and technology firms in the United States. Crowe uses its deep industry expertise to provide audit services to public and private entities while also helping clients reach their goals with tax, advisory, risk and performance services. Crowe is recognized by many organizations as one of the country\'s best places to work. Crowe serves clients worldwide as an independent member of Crowe Global, one of the largest global accounting networks in the world. The network consists of more than 200 independent accounting and advisory services firms in more than 130 countries around the world.\n\nCrowe LLP provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, sexual orientation, gender identity or expression, genetics, national origin, disability or protected veteran status, or any other characteristic protected by federal, state or local laws.\n\nCrowe LLP does not accept unsolicited candidates, referrals or resumes from any staffing agency, recruiting service, sourcing entity or any other third-party paid service at any time. Any referrals, resumes or candidates submitted to Crowe, or any employee or owner of Crowe without a pre-existing agreement signed by both parties covering the submission will be considered the property of Crowe, and free of charge.\n\nCrowe will consider for employment all qualified applicants, including those with criminal histories, in a manner consistent with the requirements of applicable state and local laws, including the City of Los Angeles\' Fair Chance Initiative for Hiring Ordinance, Los Angeles County Fair Chance Ordinance, San Francisco Fair Chance Ordinance, and the California Fair Chance Act.\n\nPlease visit our webpage to see notices of the various state and local Ban-the-Box laws and Fair Chance Ordinances, where applicable.', 'job_highlights': [{'title': 'Qualifications', 'items': ['The ideal candidate will have a strong understanding of ETL processes and technologies, as well as experience with a variety of data sources', "Bachelor's degree in computer science, information technology, or a related field", '3+ years of experience in ETL development', 'Hands on experience with Bank Secrecy Act (BSA), Financial Audits, or Anti-Money Laundering (AML) regulations and compliance requirements', 'Previous consulting experience or experience working with external clients', 'Strong understanding of ETL processes and technologies', 'Experience with a variety of data sources, including relational databases, flat files, and web APIs', 'Experience with Python, Java, or another programming language', 'Experience with data warehouse technologies, such as Azure, AWS, and Snowflake', 'Excellent problem-solving and debugging skills', 'Strong communication and teamwork skills', 'Solid understanding of software development life cycle models as well as expert knowledge of both Agile and traditional project management principles and practices and the ability to blend them together in the right proportions to fit a project and business environment', 'Technical background, with understanding or hands-on experience in enterprise solutions for different industries', 'High-level knowledge of BI architecture or data warehousing', 'Excellent written and verbal communication skills', 'Effectively managing multiple deliverables, clients and projects in a fast-paced environment', 'Strong problem solving and analytical skills', 'BI Architect / Senior Architect', "We expect the candidate to uphold Crowe's values of Care, Trust, Courage, and Stewardship"]}, {'title': 'Benefits', 'items': ['The wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs', 'A reasonable estimate of the current range is $104,500.00 - $208,300.00 per year']}, {'title': 'Responsibilities', 'items': ['The ETL developer will be responsible for designing, developing, and maintaining ETL pipelines that extract, transform, and load data into our data warehouse', 'As a member of the Data Analytics team, you will provide meaningful firm-wide contributions that uphold the team\'s dedication to "One Crowe"', 'We strive for an unparalleled client experience and look to you to promote our success', 'Additionally, you will work as part of a team of problem solvers with extensive consulting and industry experience, helping our clients solve their complex business issues from strategy to execution', 'Identify & map source data in various commercial and custom systems', 'Design, develop, test and maintain ETL pipelines', 'Extract, transform, and load data from a variety of data sources', 'Work with data engineers and other stakeholders to define data requirements', 'Troubleshoot and debug ETL pipelines', 'Stay up-to-date on ETL technologies and best practices', 'Formulate development estimates (task, effort, dependencies, etc.)', 'Develop test cases and demonstrate results for assigned deliverables', 'Maintain and present daily/weekly status', 'Articulate project deliverable details and activities to client and project team members', "Adapt to various industries and quickly learn the client's context, language, and jargon to clearly document and translate business requirements to development plans", 'Maintain a professional presence and be ready to interact with the client at all times', 'Data Product Manager', 'Report Developer (Power BI, Tableau)', 'Data Owners, Data Stewards', 'Enterprise Information Management and Data Governance Team', 'IT Staff - Architects, DBAs, Developers, etc']}], 'apply_options': [{'title': 'ZipRecruiter', 'link': 'https://www.ziprecruiter.com/c/Crowe-LLP/Job/Lead-Data-Engineer-Financial-Data,-Compliance-&-Business-Intelligence/-in-Miami,FL?jid=d88b60e2ff6d990d&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Ladders', 'link': 'https://www.theladders.com/job/lead-data-engineer-financial-data-compliance-business-intelligence-crowe-miami-fl_83795134?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobrapido', 'link': 'https://us.jobrapido.com/jobpreview/8924989012997832704?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJMZWFkIERhdGEgRW5naW5lZXIgLSBGaW5hbmNpYWwgRGF0YSwgQ29tcGxpYW5jZSBcdTAwMjYgQnVzaW5lc3MgSW50ZWxsaWdlbmNlIiwiY29tcGFueV9uYW1lIjoiQ3Jvd2UgTExQIiwiYWRkcmVzc19jaXR5IjoiTWlhbWksIEZMIiwiaHRpZG9jaWQiOiJIYm5MczRFZ2x5ZmpyZTUzQUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': 'Senior Data Engineer, Data Platform - 5687522004', 'company_name': 'TRM Labs', 'location': 'Almont, CO', 'via': 'Bandana.com', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=HqRLJdqNiN7KJOCEAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yXMsQrCMBCAYVz7AA5ON4s2pVgVnUSlIIqi7iUpZxpJ70ouQ5_GZ7Xi8sO3_MlnlJQPJMcBDjpqOJJ1hBhmf968ji8OLcyhWK5XRZ5n2WLAiQ0I6lA3wAQls_U42TYxdrJRSsSnVqKOrk5rbhUTGu7Vm438UkmjA3bDGqu8yPq0IzsdP-8XOGsj4Ah2vmWKM9hfvxrCbr-hAAAA&shmds=v1_AdeF8KgQp3dhYHcAzwV0Af7lexSbc7sqhSNtUtqs1_BjwAVw7w&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=HqRLJdqNiN7KJOCEAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965c1f9aa73feac31380f/images/718d982c06e7b28d6029d9cc1df8ba69c2ec965a655e2fa2ffeb2dff8808f53f.png', 'extensions': ['8 days ago', '190K a year', 'Full-time', 'Paid time off', 'Health insurance'], 'detected_extensions': {'posted_at': '8 days ago', 'salary': '190K a year', 'schedule_type': 'Full-time', 'paid_time_off': True, 'health_insurance': True}, 'description': 'TRM Labs is a blockchain intelligence company committed to fighting crime and creating a safer world. By leveraging blockchain data, threat intelligence, and advanced analytics, our products empower governments, financial institutions, and crypto businesses to combat illicit activity and global security threats. At TRM, you\'ll join a mission-driven, fast-paced team made up of experts in law enforcement, data science, engineering, and financial intelligence, tackling complex global challenges daily. Whether analyzing blockchain data, developing cutting-edge tools, or collaborating with global organizations, you\'ll have the opportunity to make a meaningful and lasting impact.\n\nThe Data Platform team collaborates with an experienced group of data scientists, engineers, and product managers to build highly available and scalable data infrastructure for TRM\'s products and services. As a Senior Data Engineer on the Data Platform team, you will be responsible for executing mission-critical systems and data services that analyze blockchain transaction activity at petabyte scale, and ultimately work to build a safer financial system for billions of people.\n\nThe impact you\'ll have here:\nâ€¢ Build highly reliable data services to integrate with dozens of blockchains.\nâ€¢ Develop complex ETL pipelines that transform and process petabytes of structured and unstructured data in real-time.\nâ€¢ Design and architect intricate data models for optimal storage and retrieval to support sub-second latency for querying blockchain data.\nâ€¢ Oversee the deployment and monitoring of large database clusters with an unwavering focus on performance and high availability.\nâ€¢ Collaborate across departments, partnering with data scientists, backend engineers, and product managers to design and implement novel data models that enhance TRM\'s products.\n\nWhat we\'re looking for:\nâ€¢ A Bachelor\'s degree (or equivalent) in Computer Science or a related field.\nâ€¢ A proven track record, with 5+ years of hands-on experience in architecting distributed system architecture, guiding projects from initial ideation through to successful production deployment.\nâ€¢ Exceptional programming skills in Python, as well as adeptness in SQL or SparkSQL.\nâ€¢ Versatility that spans the entire spectrum of data engineering in one or more of the following areas:\nâ€¢ In-depth experience with data stores such as Iceberg, Trino, BigQuery, and StarRocks, and Citus.\nâ€¢ Proficiency in data pipeline and workflow orchestration tools like Airflow, DBT, etc.\nâ€¢ Expertise in data processing technologies and streaming workflows including Spark, Kafka, and Flink.\nâ€¢ Competence in deploying and monitoring infrastructure within public cloud platforms, utilizing tools such as Docker, Terraform, Kubernetes, and Datadog.\nâ€¢ Proven ability in loading, querying, and transforming extensive datasets.\n\nAbout the Team:\nâ€¢ The Data Platform team is the funnel between all of TRM\'s data world and product world. We care about all layers of stack including petabyte of data stores, pipelines, and processes.\nâ€¢ We have quite a big scope as a the team with new and exciting projects every quarter. As a result, we collaborate across the board with most teams at TRM.\nâ€¢ We believe in async communication and are also not afraid to jump on a quick huddle if that helps to move things faster. We are both scrappy when the situation demands and also process-oriented when we need to achieve our OKRs.\nâ€¢ We are always looking for people who can elevate the quality our tech and our execution. If you enjoy a remote-first and async friendly environment to achieve efficacy and efficiency at petabyte scale, our team could be a great pick for you!\nâ€¢ Team members are based in the US across almost all timezones! Our on-call tends to be in EST/PST shift, whatever suits you the best.\nâ€¢ We do try to reserve some overlap in the day for meetings. Our north star - no IC spends more than 3-4 hours/week in meetings.\n\nLearn about TRM Speed in this position:\nâ€¢ Build scalable engines to optimize routine scaling and maintenance tasks like create self-serve automation for creating new pgbouncer, scaling disks, scaling/updating of clusters, etc.\nâ€¢ Enable tasks to be faster next time and reducing dependency on a single person.\nâ€¢ Identify ways to compress timelines using 80/20 principle. For instance, what does it take to be operational in a new environment? Identify the must have and nice to haves that are need to deploy our stack to be fully operation. Focus on must haves first to get us operational and then use future milestones to harden for customer readiness. We think in terms of weeks and not months.\nâ€¢ Identify first version, a.k.a., "skateboards" for projects. For instance, build an observability dashboard within a week. Gather feedback from stakeholders after to identify more needs or bells and whistles to add to the dashboard.\n\nAbout TRM\'s Engineering Levels:\n\nEngineer: Responsible for helping to define project milestones and executing small decisions independently with the appropriate tradeoffs between simplicity, readability, and performance. Provides mentorship to junior engineers, and enhances operational excellence through tech debt reduction and knowledge sharing.\n\nSenior Engineer: Successfully designs and documents system improvements and features for an OKR/project from the ground up. Consistently delivers efficient and reusable systems, optimizes team throughput with appropriate tradeoffs, mentors team members, and enhances cross-team collaboration through documentation and knowledge sharing.\n\nStaff Engineer: Drives scoping and execution of one or more OKRs/projects that impact multiple teams. Partners with stakeholders to set the team vision and technical roadmaps for one or more products. Is a role model and mentor to the entire engineering organization. Ensures system health and quality with operational reviews, testing strategies, and monitoring rigor.\n\nThe following represents the expected range of compensation for this role:\nâ€¢ Individual pay is determined by skills, qualifications, experience, and location. The compensation details listed in this posting reflect the US base salary only.\nâ€¢ The estimated base salary range for this role is $190,000 - $220,000.\nâ€¢ Additionally, this role may be eligible to participate in TRM\'s equity plan.\nâ€¢ Please note â€“ we factor in the different costs for geographies outside the United States.\n\nLife at TRM Labs\n\nLeadership Principles\n\nOur Leadership Principles shape everything we doâ€”how we make decisions, collaborate, and operate day to day.\nâ€¢ Impact-Oriented Trailblazer â€“ We put customers first, driving for speed, focus, and adaptability.\nâ€¢ Master Craftsperson â€“ We prioritize speed, high standards, and distributed ownership.\nâ€¢ Inspiring Colleague â€“ We value humility, candor, and a one-team mindset.\n\nAccelerate your Career\n\nAt TRM, you\'ll do work that mattersâ€”disrupting terrorist networks, recovering stolen funds, and protecting people around the world. You will:\nâ€¢ Work alongside top experts and learn every day.\nâ€¢ Embrace a growth mindset with development opportunities tailored to your role.\nâ€¢ Take on high-impact challenges in a fast-paced, collaborative environment.\nâ€¢ Thrive in a culture of coaching, where feedback is fast, direct, and built to help you level up.\n\nWhat to Expect at TRM\n\nTRM moves fastâ€”really fast. We know a lot of startups say that, but we mean it. We operate with urgency, ownership, and high standards. As a result, you\'ll be joining a team that\'s highly engaged, mission-driven, and constantly evolving.\n\nTo support this intensity, we\'re also intentional about rest and recharge. We offer generous benefits, including PTO, Holidays, and Parental Leave for full time employees.\n\nThat said, TRM may not be the right fit for everyone. If you\'re optimizing for work life balance, we encourage you to:\nâ€¢ Ask your interviewers how they personally approach balance within their teams, and\nâ€¢ Reflect on whether this is the right season in your life to join a high-velocity environment.\nâ€¢ Be honest with yourself about what energizes youâ€”and what drains you\n\nWe\'re upfront about this because we want every new team member to thriveâ€”not just survive.\n\nThe Stakes Are Real\n\nOur work has direct, real-world impact:\nâ€¢ Jumping online after hours to support urgent government requests tracing ransomware payments.\nâ€¢ Delivering actionable insights during terrorist financing investigations.\nâ€¢ Collaborating across time zones in real time during a major global hack.\nâ€¢ Building new processes in days, not weeks, to stop criminals before they cash out.\nâ€¢ Analyzing blockchain data to recover stolen savings and dismantle trafficking networks.\n\nThrive as a Global Team\n\nAs a remote-first company, TRM Labs is built for global collaboration.\nâ€¢ We cultivate a strong remote culture through clear communication, thorough documentation, and meaningful relationships.\nâ€¢ We invest in offsites, regional meetups, virtual coffee chats, and onboarding buddies to foster collaboration.\nâ€¢ By prioritizing trust and belonging, we harness the strengths of a global team while staying aligned with our mission and values.\n\nJoin our mission!\n\nWe\'re looking for team members who thrive in fast-paced, high-impact environments and love building from the ground up. TRM is remote-first, with an exceptionally talented global team. If you enjoy solving tough problems and seeing your work make a difference for billions of people, we want you here. Don\'t worry if your experience doesn\'t perfectly match a job descriptionâ€” we value passion, problem-solving, and unique career paths. If you\'re excited about TRM\'s mission, we want to hear from you.\n\nRecruitment agencies\n\nTRM Labs does not accept unsolicited agency resumes. Please do not forward resumes to TRM employees. TRM Labs is not responsible for any fees related to unsolicited resumes and will not pay fees to any third-party agency or company without a signed agreement.\n\nPrivacy Policy\n\nBy submitting your application, you are agreeing to allow TRM to process your personal information in accordance with the TRM Privacy Policy\n\nLearn More: Company Values | Interviewing | FAQs', 'job_highlights': [{'title': 'Qualifications', 'items': ["A Bachelor's degree (or equivalent) in Computer Science or a related field", 'A proven track record, with 5+ years of hands-on experience in architecting distributed system architecture, guiding projects from initial ideation through to successful production deployment', 'Exceptional programming skills in Python, as well as adeptness in SQL or SparkSQL', 'Versatility that spans the entire spectrum of data engineering in one or more of the following areas:', 'In-depth experience with data stores such as Iceberg, Trino, BigQuery, and StarRocks, and Citus', 'Proficiency in data pipeline and workflow orchestration tools like Airflow, DBT, etc', 'Expertise in data processing technologies and streaming workflows including Spark, Kafka, and Flink', 'Competence in deploying and monitoring infrastructure within public cloud platforms, utilizing tools such as Docker, Terraform, Kubernetes, and Datadog', 'Proven ability in loading, querying, and transforming extensive datasets', 'We are always looking for people who can elevate the quality our tech and our execution']}, {'title': 'Benefits', 'items': ['Individual pay is determined by skills, qualifications, experience, and location', 'The compensation details listed in this posting reflect the US base salary only', 'The estimated base salary range for this role is $190,000 - $220,000', "Additionally, this role may be eligible to participate in TRM's equity plan", 'We offer generous benefits, including PTO, Holidays, and Parental Leave for full time employees']}, {'title': 'Responsibilities', 'items': ["The Data Platform team collaborates with an experienced group of data scientists, engineers, and product managers to build highly available and scalable data infrastructure for TRM's products and services", 'As a Senior Data Engineer on the Data Platform team, you will be responsible for executing mission-critical systems and data services that analyze blockchain transaction activity at petabyte scale, and ultimately work to build a safer financial system for billions of people', 'Build highly reliable data services to integrate with dozens of blockchains', 'Develop complex ETL pipelines that transform and process petabytes of structured and unstructured data in real-time', 'Design and architect intricate data models for optimal storage and retrieval to support sub-second latency for querying blockchain data', 'Oversee the deployment and monitoring of large database clusters with an unwavering focus on performance and high availability', "Collaborate across departments, partnering with data scientists, backend engineers, and product managers to design and implement novel data models that enhance TRM's products", 'Build scalable engines to optimize routine scaling and maintenance tasks like create self-serve automation for creating new pgbouncer, scaling disks, scaling/updating of clusters, etc', 'Enable tasks to be faster next time and reducing dependency on a single person', 'Identify ways to compress timelines using 80/20 principle', 'Identify the must have and nice to haves that are need to deploy our stack to be fully operation', 'Focus on must haves first to get us operational and then use future milestones to harden for customer readiness', 'Identify first version, a.k.a., "skateboards" for projects', 'For instance, build an observability dashboard within a week', 'Gather feedback from stakeholders after to identify more needs or bells and whistles to add to the dashboard', 'Engineer: Responsible for helping to define project milestones and executing small decisions independently with the appropriate tradeoffs between simplicity, readability, and performance', 'Provides mentorship to junior engineers, and enhances operational excellence through tech debt reduction and knowledge sharing', 'Senior Engineer: Successfully designs and documents system improvements and features for an OKR/project from the ground up', 'Consistently delivers efficient and reusable systems, optimizes team throughput with appropriate tradeoffs, mentors team members, and enhances cross-team collaboration through documentation and knowledge sharing', 'Staff Engineer: Drives scoping and execution of one or more OKRs/projects that impact multiple teams', 'Partners with stakeholders to set the team vision and technical roadmaps for one or more products', 'Is a role model and mentor to the entire engineering organization', 'Ensures system health and quality with operational reviews, testing strategies, and monitoring rigor', "At TRM, you'll do work that mattersâ€”disrupting terrorist networks, recovering stolen funds, and protecting people around the world", 'Work alongside top experts and learn every day', 'Embrace a growth mindset with development opportunities tailored to your role', 'Take on high-impact challenges in a fast-paced, collaborative environment', 'Thrive in a culture of coaching, where feedback is fast, direct, and built to help you level up', 'Ask your interviewers how they personally approach balance within their teams, and', 'Delivering actionable insights during terrorist financing investigations', 'Collaborating across time zones in real time during a major global hack', 'Building new processes in days, not weeks, to stop criminals before they cash out', 'Analyzing blockchain data to recover stolen savings and dismantle trafficking networks']}], 'apply_options': [{'title': 'Bandana.com', 'link': 'https://bandana.com/jobs/9ba828d5-ad14-4c04-b124-0f944dfa2ca9?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBFbmdpbmVlciwgRGF0YSBQbGF0Zm9ybSAtIDU2ODc1MjIwMDQiLCJjb21wYW55X25hbWUiOiJUUk0gTGFicyIsImFkZHJlc3NfY2l0eSI6IkFsbW9udCwgQ08iLCJodGlkb2NpZCI6IkhxUkxKZHFOaU43S0pPQ0VBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Staff Product Manager, Data Engineering', 'company_name': 'Liftoff', 'location': 'Redwood City, CA', 'via': 'Built In San Francisco', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=2KSLwcLXXxAqgBh8AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNQQrCMBAAQLz2CXrZs9RGBC8KglQRREH0AWWbbNNI3S3JivUrvla9zHWyzyjb3BSbBi5R3NMqnJHRU8xhh4qwZx-YKAb2MIOj1JAIo21BGA4ivqPxulXt08qYlLrCJ0UNtrDyMMJUy2DuUqc_VWoxUt-hUrVYzoeiZz-dnEKj8usDw5XcS8RBGfSdQ7n9AksxA6WdAAAA&shmds=v1_AdeF8Kh_ainvCgV390JGgjLgs-4-ISHBzLFUpptfVOjZ-UXaYA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=2KSLwcLXXxAqgBh8AAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965c1f9aa73feac31380f/images/718d982c06e7b28d02cb97618c0e5772b8260e87f1ac4d4f5b5b070fca266999.png', 'extensions': ['10 days ago', '181Kâ€“235K a year', 'Full-time', 'No degree mentioned', 'Dental insurance', 'Health insurance'], 'detected_extensions': {'posted_at': '10 days ago', 'salary': '181Kâ€“235K a year', 'schedule_type': 'Full-time', 'qualifications': 'No degree mentioned', 'dental_coverage': True, 'health_insurance': True}, 'description': "Liftoff is a leading AI-powered performance marketing platform for the mobile app economy. Our end-to-end technology stack helps app marketers acquire and retain high-value users, while enabling publishers to maximize revenue across programmatic and direct demand.\n\nLiftoffâ€™s solutions, including Accelerate, Direct, Monetize, Intelligence, and Vungle Exchange, support over 6,600 mobile businesses across 74 countries in sectors such as gaming, social, finance, ecommerce, and entertainment. Founded in 2012 and headquartered in Redwood City, CA, Liftoff has a diverse, global presence.\n\nLiftoff is seeking a Senior Product Manager, Data Engineering to lead the roadmap for our data infrastructure and engineering systems that power machine learning and analytics across the organization. This is a highly technical product role requiring a background as a Data Engineer and a strong ability to communicate across both technical and business teams.\n\nThe ideal candidate is proactive, technically fluent, and an excellent communicator who can translate complex requirements from analysts and ML engineers into structured, actionable engineering specifications. This position partners closely with Data Engineering, Machine Learning, Data Science, and Analytics teams across both U.S. and Beijing time zones, requiring strong coordination, documentation, and clarity in asynchronous communication.\n\nKey Responsibilities:\nâ€¢ Define and execute the product vision and roadmap for Liftoffâ€™s data infrastructure, ensuring scalability, reliability, and performance.\nâ€¢ Translate complex machine learning and analytics requirements into detailed technical documentation and product specifications for engineering teams.\nâ€¢ Partner with cross-functional teams across the U.S. and Beijing to design, prioritize, and deliver high-impact data pipelines, APIs, and storage architectures.\nâ€¢ Establish and monitor data quality, latency, and reliability KPIs, driving continuous improvement and standardization.\nâ€¢ Collaborate with ML and analytics stakeholders to ensure the data platform enables rapid experimentation and model development.\nâ€¢ Anticipate data and infrastructure needs, proactively shaping architecture and tooling strategy.\nâ€¢ Advocate for data best practices, including governance, lineage, and documentation across global teams.\n\nRequirements\nâ€¢ 5â€“8 years of total experience in data-intensive environments, including:\nâ€¢ Minimum 2 years as a Data Engineer, building or maintaining large-scale pipelines and distributed data systems.\nâ€¢ Minimum 3 years in Product Management or Technical Program Management, owning data or infrastructure products.\nâ€¢ Deep understanding of data engineering concepts: ETL frameworks, data modeling, distributed processing, and streaming systems (e.g., Spark, Airflow, Kafka, Flink).\nâ€¢ Strong familiarity with cloud data platforms (Snowflake, BigQuery, Redshift) and data lake architectures.\nâ€¢ Proficient in SQL; comfortable understanding Python or similar scripting languages for technical discussions.\nâ€¢ Demonstrated ability to write clear technical documentation and product specifications.\nâ€¢ Excellent verbal and written communication, capable of bridging technical and non-technical stakeholders across global teams.\nâ€¢ Highly proactive and self-directed, able to manage complex cross-time-zone projects with minimal supervision.\n\nPreferred Experience:\nâ€¢ Background in ad tech, mobile marketing, or large-scale programmatic data systems.\nâ€¢ Experience with model training pipelines, feature stores, or data observability tooling.\nâ€¢ Knowledge of data governance, schema evolution, and metadata management practices.\nâ€¢ Experience collaborating with distributed teams across U.S. and Asia time zones.\n\nWhy Join Liftoff?\nâ€¢ Innovative Environment: Be part of a company at the forefront of mobile app marketing, utilizing cutting-edge technologies to drive results.\u200b\nâ€¢ Professional Growth: Opportunities for continuous learning and career advancement in a dynamic industry.\u200b\nâ€¢ Collaborative Culture: Work with a diverse team of talented professionals who are passionate about delivering excellence.\nâ€¢ Comprehensive Benefits: Competitive salary, health benefits, and other perks to support your well-being.\n\nWorking at Liftoff is fast-paced, fun, and challenging, and we thrive on innovation. Come join the rocket ship and help shape the future of the mobile app ecosystem with us!\n\nLocation:\n\nThis role is eligible for full-time remote work in California or near one of our US hubs in Redwood City or Los Angeles. This position is located in the Pacific Time Zone.\n\nTravel Expectations:\nWe offer several opportunities for in-person team gatherings, including but not limited to project meetings, regional meetups, and company-wide events. We expect our employees to attend these gatherings at least once per quarter. These gatherings provide essential opportunities for collaboration, communication, and team building.\n\nThis position will require travel at least once per quarter to work with the Beijing team.\n\nCompensation:\n\nLiftoff offers all employees a full compensation package that includes equity and health/vision/dental benefits associated with your country of residence. Base compensation will vary based on candidate's location and experience.\n\nThe following are our base salary ranges for this role:\nâ€¢ SF Bay Area/Los Angeles/Orange County/Seattle: $210,000 to $235,000\nâ€¢ All other California and Washington state locations: $193,200 to $216,200\nâ€¢ All other locations in our approved states: $180,600 to $202,100\n\n#LI-VM1\n\n#LI-Remote\n\nWe use Covey as part of our hiring and / or promotional process for jobs in NYC and certain features may qualify it as an AEDT. As part of the evaluation process we provide Covey with job requirements and candidate submitted applications. We began using Covey Scout for Inbound on January 22, 2024.\n\nPlease see the independent bias audit report covering our use of Covey here.\n\nLiftoff offers a fast-paced, collaborative, and innovative work environment where employees are empowered to grow and make an impact. Weâ€™re shaping the future of the mobile app ecosystemâ€”join us and help accelerate whatâ€™s next.\n\nLiftoffâ€™s compensation strategy includes competitive salaries, equity, and benefits designed to support employee well-being and performance. We benchmark compensation based on role, level, and location to ensure fairness and market alignment. Benefits may include medical coverage, wellness stipends, and additional perks based on your country of residence.\n\nLiftoff is an equal opportunity employer. We are committed to creating an inclusive environment for all employees and applicants regardless of race, ethnicity, national origin, age, marital status, disability, sexual orientation, gender identity, religion, veteran status, or any other characteristic protected by applicable law.\n\nAgency and Third Party Recruiter Notice:\n\nLiftoff does not accept unsolicited resumes from individual recruiters or third-party recruiting agencies in response to job postings. No fee will be paid to third parties who submit unsolicited candidates directly to our hiring managers or Recruiting Team. All candidates must be submitted via our Applicant Tracking System by approved Liftoff vendors who have been expressly requested to make a submission by our Recruiting Team for a specific job opening. No placement fees will be paid to any firm unless such a request has been made by the Liftoff Recruiting Team and such a candidate was submitted to the Liftoff Recruiting Team via our Applicant Tracking System.", 'job_highlights': [{'title': 'Qualifications', 'items': ['This is a highly technical product role requiring a background as a Data Engineer and a strong ability to communicate across both technical and business teams', 'The ideal candidate is proactive, technically fluent, and an excellent communicator who can translate complex requirements from analysts and ML engineers into structured, actionable engineering specifications', '5â€“8 years of total experience in data-intensive environments, including:', 'Minimum 2 years as a Data Engineer, building or maintaining large-scale pipelines and distributed data systems', 'Minimum 3 years in Product Management or Technical Program Management, owning data or infrastructure products', 'Deep understanding of data engineering concepts: ETL frameworks, data modeling, distributed processing, and streaming systems (e.g., Spark, Airflow, Kafka, Flink)', 'Strong familiarity with cloud data platforms (Snowflake, BigQuery, Redshift) and data lake architectures', 'Proficient in SQL; comfortable understanding Python or similar scripting languages for technical discussions', 'Demonstrated ability to write clear technical documentation and product specifications', 'Excellent verbal and written communication, capable of bridging technical and non-technical stakeholders across global teams', 'Highly proactive and self-directed, able to manage complex cross-time-zone projects with minimal supervision']}, {'title': 'Benefits', 'items': ['Professional Growth: Opportunities for continuous learning and career advancement in a dynamic industry.\u200b', 'Collaborative Culture: Work with a diverse team of talented professionals who are passionate about delivering excellence', 'Comprehensive Benefits: Competitive salary, health benefits, and other perks to support your well-being', 'We offer several opportunities for in-person team gatherings, including but not limited to project meetings, regional meetups, and company-wide events', 'Liftoff offers all employees a full compensation package that includes equity and health/vision/dental benefits associated with your country of residence', "Base compensation will vary based on candidate's location and experience", 'SF Bay Area/Los Angeles/Orange County/Seattle: $210,000 to $235,000', 'Liftoffâ€™s compensation strategy includes competitive salaries, equity, and benefits designed to support employee well-being and performance', 'We benchmark compensation based on role, level, and location to ensure fairness and market alignment', 'Benefits may include medical coverage, wellness stipends, and additional perks based on your country of residence']}, {'title': 'Responsibilities', 'items': ['This position partners closely with Data Engineering, Machine Learning, Data Science, and Analytics teams across both U.S. and Beijing time zones, requiring strong coordination, documentation, and clarity in asynchronous communication', 'Define and execute the product vision and roadmap for Liftoffâ€™s data infrastructure, ensuring scalability, reliability, and performance', 'Translate complex machine learning and analytics requirements into detailed technical documentation and product specifications for engineering teams', 'Partner with cross-functional teams across the U.S. and Beijing to design, prioritize, and deliver high-impact data pipelines, APIs, and storage architectures', 'Establish and monitor data quality, latency, and reliability KPIs, driving continuous improvement and standardization', 'Collaborate with ML and analytics stakeholders to ensure the data platform enables rapid experimentation and model development', 'Anticipate data and infrastructure needs, proactively shaping architecture and tooling strategy', 'Advocate for data best practices, including governance, lineage, and documentation across global teams', 'We expect our employees to attend these gatherings at least once per quarter', 'These gatherings provide essential opportunities for collaboration, communication, and team building', 'This position will require travel at least once per quarter to work with the Beijing team']}], 'apply_options': [{'title': 'Built In San Francisco', 'link': 'https://www.builtinsf.com/job/staff-product-manager-data-engineering/7450490?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Built In', 'link': 'https://builtin.com/job/staff-product-manager-data-engineering/7450490?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'ZipRecruiter', 'link': 'https://www.ziprecruiter.com/c/Liftoff-Mobile/Job/Staff-Product-Manager,-Data-Engineering/-in-Redwood-City,CA?jid=ea547c5fd809441b&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Startup Jobs', 'link': 'https://startup.jobs/staff-product-manager-data-engineering-liftoff-7383184?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobrapido', 'link': 'https://us.jobrapido.com/jobpreview/4545009951839354880?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Talent.com', 'link': 'https://www.talent.com/view?id=037afedc69b3&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobs Trabajo.org', 'link': 'https://us.trabajo.org/job-3310-6b2ef5f5a4f1ac59aff413ca70fb09b2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobilize', 'link': 'https://www.jobilize.com/job/us-ca-redwood-staff-product-manager-data-engineering-liftoff-hiring?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTdGFmZiBQcm9kdWN0IE1hbmFnZXIsIERhdGEgRW5naW5lZXJpbmciLCJjb21wYW55X25hbWUiOiJMaWZ0b2ZmIiwiYWRkcmVzc19jaXR5IjoiUmVkd29vZCBDaXR5LCBDQSIsImh0aWRvY2lkIjoiMktTTHdjTFhYeEFxZ0JoOEFBQUFBQT09IiwiaGwiOiJlbiJ9'}, {'title': 'Enterprise Engineer Sr - Senior Data Center Network Engineer', 'company_name': 'PNC', 'location': 'Pittsburgh, PA', 'via': 'PNC Careers', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=FVFQev_z-0doHeliAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_0XKsQrCMBAAUFy7uzjdLLYRwUUnqUVwKIV-QEnKkURrLtyd2F_yL6WTy5te8V0VfZMUOXMUhCb5mBAZeoYSekyRGK5WLdS4LGhRP8TPfyzhTg4ELY8BKMGNyE-4OQfVLCdjRKbKi1qNYzXSy1BCR7N5kJOFQYJlzJNVHA7H_Vzl5Lfrrq0hJuiiqrg3-7CD7vIDrjxbf6wAAAA&shmds=v1_AdeF8KjPV0Hx5ypsmQbGXYEt6jg5ENZST8OWmMiDHlb_HL07rQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=FVFQev_z-0doHeliAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965c1f9aa73feac31380f/images/718d982c06e7b28d906e1fe1526861dcc5a21f2d4cd51b1699f8726a8eba4aa5.jpeg', 'extensions': ['Full-time', 'Health insurance', 'Paid time off', 'Dental insurance'], 'detected_extensions': {'schedule_type': 'Full-time', 'health_insurance': True, 'paid_time_off': True, 'dental_coverage': True}, 'description': 'Position Overview\n\nAt PNC, our people are our greatest differentiator and competitive advantage in the markets we serve. We are all united in delivering the best experience for our customers. We work together each day to foster an inclusive workplace culture where all of our employees feel respected, valued and have an opportunity to contribute to the companyâ€™s success.\n\nThis position is primarily based in a location within PNC\'s footprint. Responsibilities require time in the office or in a field on a regular basis.\n\nSr. Data Center Network Engineer with experience in design, implementation and 24x7 support of complex data center networks across multiple locations.\n\nKnowledge of Cisco network equipment including Arista 7010TX,7060CX2, 7280CR2/3, 7280SR2/3 series of Data Center switches running EOS and Cisco Nexus 9K/7K/5K/3K/2K series of Data Center switches running NX-OS, Cisco CRS, ASR9K & NCS 55XX devices running IOS-XR, Cisco ISR4K, ASR1K, Catalyst 8300/9400 series of routers/switches running IOS-XE.\n\nKnowledge of Internet Protocols/network/services/technologies such as TCP/IP, OSPF, BGP, SDN, VXLAN, EVPN, Leaf/Spine fabric, etc.\n\nAbility to identify network operational tasks and automate them via Python, Ansible, etc.\nâ€¢ ** PNC will not provide sponsorship for employment visas or participate in STEM OPT for this position. ***\n\nPNC will not provide sponsorship for employment visas or participate in STEM OPT for this position.\n\nJob Description\nâ€¢ Provides subject matter expertise for enterprise engineering teams, full stack engineers, and clients. Uses technical knowledge and industry experience to design, build and maintain technology solutions.\nâ€¢ Leads in the implementation of enterprise engineering strategies and best practices to support business objectives.\nâ€¢ Independently provides guidance to engineers across the organization regarding the development of software components and hardware.\nâ€¢ Maintains relationships with stakeholders, addresses client requirements and provides consultation to shape the success of enterprise engineering.\nâ€¢ Independently provides expert advice and resolves complex problems and issues involved in enterprise engineering activities.\n\nPNC Employees take pride in our reputation and to continue building upon that we expect our employees to be:\nâ€¢ Customer Focused - Knowledgeable of the values and practices that align customer needs and satisfaction as primary considerations in all business decisions and able to leverage that information in creating customized customer solutions.\nâ€¢ Managing Risk - Assessing and effectively managing all of the risks associated with their business objectives and activities to ensure they adhere to and support PNC\'s Enterprise Risk Management Framework.\n\nQualifications\n\nSuccessful candidates must demonstrate appropriate knowledge, skills, and abilities for a role. Listed below are skills, competencies, work experience, education, and required certifications/licensures needed to be successful in this position.\n\nPreferred Skills\nCompetitive Advantages, Customer Solutions, Design, Enterprise Architecture Framework, Machine Learning, Risk Assessments, Technical Knowledge\n\nCompetencies\nApplication Delivery Process, Consulting, Effectiveness Measurement, Industry Knowledge, IT Industry: Trends & Directions, IT Standards, Procedures & Policies, Planning: Tactical, Strategic, Problem Solving, Standard Operating Procedures\n\nWork Experience\nRoles at this level typically require a university / college degree. Higher level education such as a Masters degree, PhD, or certifications is desirable. Industry relevant experience is typically 8+ years. Specific certifications are often required. In lieu of a degree, a comparable combination of education, job specific certification(s), and experience (including military service) may be considered.\n\nEducation\nBachelors\n\nCertifications\nNo Required Certification(s)\n\nLicenses\nNo Required License(s)\n\nPay Transparency\n\nBase Salary: $80,000.00 â€“ $192,050.00\n\nSalaries may vary based on geographic location, market data and on individual skills, experience, and education. This role is incentive eligible with the payment based upon company, business and/or individual performance.\n\nApplication Window\n\nGenerally, this opening is expected to be posted for two business days from 08/26/2025, although it may be longer with business discretion.\n\nBenefits\n\nPNC offers a comprehensive range of benefits to help meet your needs now and in the future. Depending on your eligibility, options for full-time employees include: medical/prescription drug coverage (with a Health Savings Account feature), dental and vision options; employee and spouse/child life insurance; short and long-term disability protection; 401(k) with PNC match, pension and stock purchase plans; dependent care reimbursement account; back-up child/elder care; adoption, surrogacy, and doula reimbursement; educational assistance, including select programs fully paid; a robust wellness program with financial incentives.\n\nIn addition, PNC generally provides the following paid time off, depending on your eligibility: maternity and/or parental leave; up to 11 paid holidays each year; 8 occasional absence days each year, unless otherwise required by law; between 15 to 25 vacation days each year, depending on career level; and years of service.\n\nTo learn more about these and other programs, including benefits for full time and part-time employees, visit Your PNC Total Rewards.\n\nDisability Accommodations Statement\n\nIf an accommodation is required to participate in the application process, please contact us via email at AccommodationRequest@pnc.com. Please include â€œaccommodation requestâ€ in the subject line title and be sure to include your name, the job ID, and your preferred method of contact in the body of the email. Emails not related to accommodation requests will not receive responses. Applicants may also call 877-968-7762 and say "Workday" for accommodation assistance. All information provided will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.\n\nAt PNC we foster an inclusive and accessible workplace. We provide reasonable accommodations to employment applicants and qualified individuals with a disability who need an accommodation to perform the essential functions of their positions.\n\nEqual Employment Opportunity (EEO)\n\nPNC provides equal employment opportunity to qualified persons regardless of race, color, sex, religion, national origin, age, sexual orientation, gender identity, disability, veteran status, or other categories protected by law.\n\nThis position is subject to the requirements of Section 19 of the Federal Deposit Insurance Act (FDIA) and, for any registered role, the Secure and Fair Enforcement for Mortgage Licensing Act of 2008 (SAFE Act) and/or the Financial Industry Regulatory Authority (FINRA), which prohibit the hiring of individuals with certain criminal history.\n\nCalifornia Residents\n\nRefer to the California Consumer Privacy Act Privacy Notice to gain understanding of how PNC may use or disclose your personal information in our hiring practices.', 'job_highlights': [{'title': 'Qualifications', 'items': ['S, Cisco CRS, ASR9K & NCS 55XX devices running IOS-XR, Cisco ISR4K, ASR1K, Catalyst 8300/9400 series of routers/switches running IOS-XE', 'Knowledge of Internet Protocols/network/services/technologies such as TCP/IP, OSPF, BGP, SDN, VXLAN, EVPN, Leaf/Spine fabric, etc', 'Ability to identify network operational tasks and automate them via Python, Ansible, etc', '** PNC will not provide sponsorship for employment visas or participate in STEM OPT for this position', 'Successful candidates must demonstrate appropriate knowledge, skills, and abilities for a role', 'Listed below are skills, competencies, work experience, education, and required certifications/licensures needed to be successful in this position', 'Competitive Advantages, Customer Solutions, Design, Enterprise Architecture Framework, Machine Learning, Risk Assessments, Technical Knowledge', 'Application Delivery Process, Consulting, Effectiveness Measurement, Industry Knowledge, IT Industry: Trends & Directions, IT Standards, Procedures & Policies, Planning: Tactical, Strategic, Problem Solving, Standard Operating Procedures', 'Roles at this level typically require a university / college degree', 'Industry relevant experience is typically 8+ years', 'Specific certifications are often required', 'In lieu of a degree, a comparable combination of education, job specific certification(s), and experience (including military service) may be considered', 'Bachelors', 'No Required Certification(s)', 'No Required License(s)']}, {'title': 'Benefits', 'items': ['Pay Transparency', 'Base Salary: $80,000.00 â€“ $192,050.00', 'Salaries may vary based on geographic location, market data and on individual skills, experience, and education', 'This role is incentive eligible with the payment based upon company, business and/or individual performance', 'PNC offers a comprehensive range of benefits to help meet your needs now and in the future', 'Depending on your eligibility, options for full-time employees include: medical/prescription drug coverage (with a Health Savings Account feature), dental and vision options; employee and spouse/child life insurance; short and long-term disability protection; 401(k) with PNC match, pension and stock purchase plans; dependent care reimbursement account; back-up child/elder care; adoption, surrogacy, and doula reimbursement; educational assistance, including select programs fully paid; a robust wellness program with financial incentives', 'In addition, PNC generally provides the following paid time off, depending on your eligibility: maternity and/or parental leave; up to 11 paid holidays each year; 8 occasional absence days each year, unless otherwise required by law; between 15 to 25 vacation days each year, depending on career level; and years of service', 'To learn more about these and other programs, including benefits for full time and part-time employees, visit Your PNC Total Rewards']}, {'title': 'Responsibilities', 'items': ['Responsibilities require time in the office or in a field on a regular basis', 'Data Center Network Engineer with experience in design, implementation and 24x7 support of complex data center networks across multiple locations', 'Knowledge of Cisco network equipment including Arista 7010TX,7060CX2, 7280CR2/3, 7280SR2/3 series of Data Center switches running EOS and Cisco Nexus 9K/7K/5K/3K/2K series of Data Center switches running NX-O', 'Provides subject matter expertise for enterprise engineering teams, full stack engineers, and clients', 'Uses technical knowledge and industry experience to design, build and maintain technology solutions', 'Leads in the implementation of enterprise engineering strategies and best practices to support business objectives', 'Independently provides guidance to engineers across the organization regarding the development of software components and hardware', 'Maintains relationships with stakeholders, addresses client requirements and provides consultation to shape the success of enterprise engineering', 'Independently provides expert advice and resolves complex problems and issues involved in enterprise engineering activities', 'Customer Focused - Knowledgeable of the values and practices that align customer needs and satisfaction as primary considerations in all business decisions and able to leverage that information in creating customized customer solutions', "Managing Risk - Assessing and effectively managing all of the risks associated with their business objectives and activities to ensure they adhere to and support PNC's Enterprise Risk Management Framework"]}], 'apply_options': [{'title': 'PNC Careers', 'link': 'https://careers.pnc.com/global/en/job/R199191/Enterprise-Engineer-Sr-Senior-Data-Center-Network-Engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Indeed', 'link': 'https://www.indeed.com/viewjob?jk=04673a6839d143cd&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Glassdoor', 'link': 'https://www.glassdoor.com/job-listing/enterprise-engineer-sr-senior-data-center-network-engineer-pnc-financial-services-group-JV_IC1152990_KO0,58_KE59,87.htm?jl=1009855736484&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Built In', 'link': 'https://builtin.com/job/enterprise-engineer-sr-senior-data-center-network-engineer/6980532?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'LinkedIn', 'link': 'https://www.linkedin.com/jobs/view/enterprise-engineer-sr-senior-data-center-network-engineer-at-pnc-4289694111?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'SimplyHired', 'link': 'https://www.simplyhired.com/job/CXUf7X0cfdh8Rcvepx0EOUPPsAXxMZ7suGP-ocMekguh-C3dvJD5VA?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Remote Rocketship', 'link': 'https://www.remoterocketship.com/company/pnc/jobs/senior-data-center-network-engineer-pittsburgh-hybrid/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Ladders', 'link': 'https://www.theladders.com/job/enterprise-engineer-sr-senior-data-center-dot-net-work-engineer-thepncfinancialservicesgroup-pittsburgh-pa_83071239?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJFbnRlcnByaXNlIEVuZ2luZWVyIFNyIC0gU2VuaW9yIERhdGEgQ2VudGVyIE5ldHdvcmsgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJQTkMiLCJhZGRyZXNzX2NpdHkiOiJQaXR0c2J1cmdoLCBQQSIsImh0aWRvY2lkIjoiRlZGUWV2X3otMGRvSGVsaUFBQUFBQT09IiwiaGwiOiJlbiJ9'}, {'title': 'Senior Big Data Engineer', 'company_name': 'iCube Consulting Services', 'location': 'Jacksonville, FL', 'via': 'Glassdoor', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=uDkLqTFEWbp1brmZAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEOw7CMAwAULH2CEyeEW0QEgtslI9UsfUAlRNZqSHYVZxWXIa7It7wqu-qcj0Ja4YzR7hgQbhKZCHKUEOnHowwhxFU4K4aE61PYymTHZ0zS020goVDE_TtVMjrxz3V27_BRsw0JSw07A-7TzNJ3NTczp6gVbE5FZYIPeWFAxmwQIfhZSoLp0RbuD1-Z_6RC6AAAAA&shmds=v1_AdeF8KgkibE_NliRMhuu5CGoB2eu5u1SVg0KM4WbYP5IpKAcPQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=uDkLqTFEWbp1brmZAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965c1f9aa73feac31380f/images/718d982c06e7b28d858383a0e8795d9f31d863618830f75ea1dda823c05d3ace.jpeg', 'extensions': ['24 days ago', 'Full-time', 'Health insurance', 'Dental insurance', 'Paid time off'], 'detected_extensions': {'posted_at': '24 days ago', 'schedule_type': 'Full-time', 'health_insurance': True, 'dental_coverage': True, 'paid_time_off': True}, 'description': 'Life at ICUBE CSI\n\nAre you interested in working with a startup in cutting edge Technology space ?\n\nAre you excited about Artificial Intelligence, Machine Learning, Data Analytics ?\n\nCome join our ICUBE CSI team! Our team comes from all over the world with backgrounds in different types of industries. We are building a close knit team and meaningful culture.\n\nOur benefits: Weâ€™re happy when youâ€™re happy. To make this happen, we offer competitive compensation, big-company benefits and a startup culture with vibrant energy and cool perks of a startup.\nâ€¢ Health care (medical and dental)\nâ€¢ 401k/Retirement Benefits\nâ€¢ Life/LTD Insurance\nâ€¢ Flexible schedules\nâ€¢ Paid time off\nâ€¢ Training & development\nâ€¢ Fun, diverse and intellectually eager coworkers\nâ€¢ Team happy hours and retreats\nâ€¢ Workplace perks such as food/coffee\n\nWe are building innovation around Data Analytics, Data Management using open source technologies. Our company thrives on selling big data analytics solutions. Our in-house advanced big data analytics team comes with backing of its leadership team with 100+ combined years of experience in data engineering, data processing, infrastructure designs, machine learning and visualization. We are an equal opportunity employer.\n\nEveryone can push forward in good times and when all is going perfectly as planned. But when it doesnâ€™t go perfectly, youâ€™re ready to attack problems head on. Business isnâ€™t always easy, but youâ€™re known for being there through thick and thin.\n\nResponsibilities:\nâ€¢ Design, Develop and Implement Big data engineering projects in Hadoop ecosystem.\nâ€¢ Engineer solutions with Cloudera, MapR or HDP for both batch & streaming data with high quality and with a sense of urgency.\nâ€¢ Develop application and custom integration solutions using spark streaming and Hive.\nâ€¢ Understand specifications, plan, design and develop software solutions, adhering to process â€“ either individually or collectively within a project team\nâ€¢ Work in state-of-the art programming languages and utilize object-oriented approaches in designing, coding, testing and debugging programs.\nâ€¢ Work with support teams in resolving operational & performance issues\nâ€¢ Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities\nâ€¢ Integrate data from multiple data sources, Implementing ETL process using APACHE NIFI\nâ€¢ Monitoring performance and advising any necessary infrastructure changes\nâ€¢ Management of Hadoop cluster, with all included services such as Hive, HBase, mapReduce and Sqoop\nâ€¢ Cleaning data as per business requirements using streaming APIâ€™s or user defined functions.\nâ€¢ Build distributed, reliable and scalable data pipelines to ingest and process data in real-time, defining Hadoop Job Flows.\nâ€¢ Managing Hadoop jobs using scheduler.\nâ€¢ Apply different HDFS formats and structure like Parquet, Avro, etc. to speed up analytics.\nâ€¢ Work with various hadoop ecosystem tools like Hive, pig, Hbase , spark etc.\nâ€¢ Reviewing and managing Hadoop log files.\nâ€¢ Assess the quality of datasets for a hadoop data lake.\nâ€¢ Fine tune Hadoop applications for high performance and throughput.\nâ€¢ Troubleshoot and debug any Hadoop ecosystem run time issues.\n\nBeing a part of a POC effort to help build new Hadoop clusters\n\nEducation:\n\nBachelorâ€™s Degree or higher in Computer Science, Information Systems or related engineering disciplines\n\nGeneral Knowledge, Skills & Abilities\nâ€¢ Be a good detail-oriented data engineer\nâ€¢ Systematic and organizational skills important.\nâ€¢ Willing to commit for completing deliverable on time.\n\nPreferred Qualifications:\nâ€¢ Must have experience with Spark, Hive, Scala or py spark.\nâ€¢ Preferred experience in one of the following technologies: Nifi, Kafka, or any other streaming technologies.\nâ€¢ 3+ years experience in data engineering building ETL pipelines using JAVA or Python or Scala\nâ€¢ Should be good at Pig, HIVE scripting.\nâ€¢ Solid understanding of HDFS is important.\nâ€¢ Work experience within a Data Warehousing/Business Intelligence/Data analytics group, and have handâ€™s-on experience with Hadoop\nâ€¢ Create tables/views in Hive or other relevant scripting language\nâ€¢ Have experience with Agile development methodologies\nâ€¢ Experience with NoSQL databases, such as HBase, Cassandra, MongoDB.\n\nExperience Architecting Solutions Utilizing any of the following:\nâ€¢ JAVA or Python or Scala programming languages\nâ€¢ Nifi, Kafka-topics, or any other streaming technologies\nâ€¢ Parquet/Avro/ORC/XML/JSON/ORC/CSV/TXT formats\n\nLocation: Jacksonville, FL\n\nSend your resumes to careers@ICUBE CSI.com', 'job_highlights': [{'title': 'Qualifications', 'items': ['Apply different HDFS formats and structure like Parquet, Avro, etc', 'Reviewing and managing Hadoop log files', 'Assess the quality of datasets for a hadoop data lake', 'Fine tune Hadoop applications for high performance and throughput', 'Troubleshoot and debug any Hadoop ecosystem run time issues', 'Bachelorâ€™s Degree or higher in Computer Science, Information Systems or related engineering disciplines', 'General Knowledge, Skills & Abilities', 'Be a good detail-oriented data engineer', 'Systematic and organizational skills important', 'Willing to commit for completing deliverable on time', 'Experience Architecting Solutions Utilizing any of the following:', 'JAVA or Python or Scala programming languages', 'Nifi, Kafka-topics, or any other streaming technologies']}, {'title': 'Benefits', 'items': ['To make this happen, we offer competitive compensation, big-company benefits and a startup culture with vibrant energy and cool perks of a startup', 'Health care (medical and dental)', '401k/Retirement Benefits', 'Life/LTD Insurance', 'Flexible schedules', 'Paid time off', 'Training & development', 'Fun, diverse and intellectually eager coworkers', 'Team happy hours and retreats', 'Workplace perks such as food/coffee']}, {'title': 'Responsibilities', 'items': ['Design, Develop and Implement Big data engineering projects in Hadoop ecosystem', 'Engineer solutions with Cloudera, MapR or HDP for both batch & streaming data with high quality and with a sense of urgency', 'Develop application and custom integration solutions using spark streaming and Hive', 'Understand specifications, plan, design and develop software solutions, adhering to process â€“ either individually or collectively within a project team', 'Work in state-of-the art programming languages and utilize object-oriented approaches in designing, coding, testing and debugging programs', 'Work with support teams in resolving operational & performance issues', 'Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities', 'Integrate data from multiple data sources, Implementing ETL process using APACHE NIFI', 'Monitoring performance and advising any necessary infrastructure changes', 'Management of Hadoop cluster, with all included services such as Hive, HBase, map', 'Reduce and Sqoop', 'Cleaning data as per business requirements using streaming APIâ€™s or user defined functions', 'Build distributed, reliable and scalable data pipelines to ingest and process data in real-time, defining Hadoop Job Flows', 'Managing Hadoop jobs using scheduler', 'to speed up analytics', 'Work with various hadoop ecosystem tools like Hive, pig, Hbase , spark etc', 'Being a part of a POC effort to help build new Hadoop clusters']}], 'apply_options': [{'title': 'Glassdoor', 'link': 'https://www.glassdoor.com/job-listing/senior-big-data-engineer-icube-consulting-services-JV_IC1154093_KO0,24_KE25,50.htm?jl=1008918337910&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'SimplyHired', 'link': 'https://www.simplyhired.com/job/L9LPnirmslGfn2d1hOfmEipurgCP_RadZgTysLtk50Rrob3BleZ0Fw?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'BeBee', 'link': 'https://us.bebee.com/job/97f84c23858bfb1c32dc450790f127d8?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTZW5pb3IgQmlnIERhdGEgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJpQ3ViZSBDb25zdWx0aW5nIFNlcnZpY2VzIiwiYWRkcmVzc19jaXR5IjoiSmFja3NvbnZpbGxlLCBGTCIsImh0aWRvY2lkIjoidURrTHFURkVXYnAxYnJtWkFBQUFBQT09IiwiaGwiOiJlbiJ9'}, {'title': 'Prognostics & Health Management (PHM) Data Engineer - LiftSystem', 'company_name': 'Rolls Royce', 'location': 'New Whiteland, IN', 'via': 'Women For Hire- Job Board', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=J1na3kBcctl76r8vAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNwQ7BQBCA4bj2EZzmJAitSFy4EiUqwsFRpjV2V7YzTWcSvJnHU5cv-U9_8u0l11MrjkUtVAoDyAmjeSiQ0VFNbDA85cUI1mgIG3aBiVqYwiE87PJRo7qLvZSghG3lQRi2Ii5Sf-XNGl1mmWpMnRp2h7SSOhOmUt7ZU0r9c1OPLTURjW7zxeydNuzGcJYYtfNTEQSGI73g6oNRRL5PYHf8Adrn-mG7AAAA&shmds=v1_AdeF8KhU1YgKRqF5U3ceyuewx6IkulZocKzVz58c0mcruu5AIQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=J1na3kBcctl76r8vAAAAAA%3D%3D', 'extensions': ['2 days ago', 'Full-time', 'Paid time off', 'Dental insurance', 'Health insurance'], 'detected_extensions': {'posted_at': '2 days ago', 'schedule_type': 'Full-time', 'paid_time_off': True, 'dental_coverage': True, 'health_insurance': True}, 'description': "Job Description\n\nJob Title: Prognostics & Health Management (PHM) Data Engineer - LiftSystem\n\nWorking Pattern: Full time\n\nWorking location: Indianapolis, IN/Hybrid - 3 Office Days/Week)\n\nThe Rolls-Royce LiftSystem EHM-SDI team is seeking a skilled Data Engineer to design, build, and maintain data infrastructure that powers Prognostics & Health Management (PHM) capability for F35B LiftSystem. This role will ensure that critical health data is captured, stored, and made accessible for analytics, machine learning models, and decision-making systems. You will be responsible for database design, performance optimization, data quality assurance, and integration with other system. This is a highly technical role that includes data engineering, data pipeline development, and data analysis and visualization to support predictive maintenance and fleet health monitoring.\n\nWhy Rolls-Royce?\n\nRolls-Royce is one of the most enduring and iconic brands in the world and has been at the forefront of innovation for over a century. We design, build and service systems that provide critical power to customers where safety and reliability are paramount.\n\nWe are proud to be a force for progress, powering, protecting and connecting people everywhere.\n\nWe want to ensure that the excellence and ingenuity that has shaped our history continues into our future, and we need people like you to come and join us on this journey.\n\nRolls-Royce has been recognized as the top employer in the Engineering & Manufacturing category on the prestigious Forbes Top Employers for Engineers list for 2025. This ranking highlights our commitment to innovation, employee development, and fostering a collaborative environment where engineers can thrive.\n\nBe part of a team that sets the industry standard and drives groundbreaking solutions.\n\nAt Rolls-Royce, we are committed to creating a workplace where all employees feel respected, supported, and empowered to do their best work. We foster a welcoming and innovative work environment that invests in you, giving you access to an incredible breadth and depth of opportunities where you can grow your career and make a difference.\n\nRolls-Royce is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to any protected characteristics.\n\nWhat you will be doing\nâ€¢ Design, implement, and maintain scalable and secure database system for PHM data.\nâ€¢ Optimize database performance, query speed, and storage efficiency.\nâ€¢ Build ETS (Extract, Transform, Load) pipeline to establish automated data and file exchange capabilities for all PHM data.\nâ€¢ Ensure seamless integration between OHM databases and analytics/visualization tools.\nâ€¢ Manage ongoing relationship with Rolls-Royce IT with respect to enterprise systems and LiftSystem-specific systems supported by the IT.\nâ€¢ Provide technical advice on data architecture and database technologies to broader PHM capability.\nâ€¢ Develop and implement data validation strategies to ensure accuracy and consistency.\n\nWho we're looking for:\n\nAt Rolls-Royce we put safety first, do the right thing, keep it simple and make a difference. These principles form the behaviours that guide us and are an essential component of our assessment process. They are the fundamental qualities that we seek for all roles.\n\nBasic Qualifications:\nâ€¢ Bachelor's Degree in Computer Science or Engineering and 2+ years of experience in PHM/EHM system design, data analysis, data engineering, software development, OR\nâ€¢ Master's Degree in Computer Science or Engineering\nâ€¢ Must be U.S. Citizen to be considered for this opportunity\n\nPreferred Requirements:\nâ€¢ Experience in working on production database systems, database design, administration and data engineering\nâ€¢ Strong proficiency in SQL and relational database management systems\nâ€¢ Familiarity with time-series data and its processing/storage\nâ€¢ Experience building ETL data pipelines using tools such as Python\nâ€¢ Knowledge of cloud services is a plus\nâ€¢ Knowledge of API development for data access\nâ€¢ Familiarity with PHM, experience with big data platforms, exposure to ML workflows\n\nWhat we offer:\n\nWe offer excellent development opportunities, a competitive salary, and exceptional benefits. These include bonus, employee support assistance and employee discounts.\n\nYour needs are as unique as you are. Hybrid working is a way in which our people can balance their time between the office, home, or another remote location. It's a locally managed and flexed informal discretionary arrangement. As a minimum we're all expected to attend the workplace for collaboration and other specific reasons, on average three days per week.\n\nRelocation assistance will be provided if applicable.\n\nGlobal Grade/Level: GG10\n\nClosing date: 11/28/2025\n\n#CLODEF\n\n#CLOLI\n\nJob Category\n\nEngineering for Services\n\nJob Posting Date\n\n12 Nov 2025; 00:11\n\nPay Range\n\n$90,985 - $136,477-Annually\n\nLocation:\n\nIndianapolis, IN\n\nBenefits\n\nRolls-Royce provides a comprehensive and competitive Total Rewards package that includes base pay and a discretionary bonus plan. Eligible employees may have the opportunity to enroll in other benefits, including health, dental, vision, disability, life and accidental death & dismemberment insurance; a flexible spending account; a health savings account; a 401(k) retirement savings plan with a company match; Employee Assistance Program; Paid Time Off; certain paid holidays; paid parental and family care leave; tuition reimbursement; and a long-term incentive plan. The options available to an employee may vary depending on eligibility factors such as date of hire, employment type, and the applicability of collective bargaining agreements.", 'job_highlights': [{'title': 'Qualifications', 'items': ["Bachelor's Degree in Computer Science or Engineering and 2+ years of experience in PHM/EHM system design, data analysis, data engineering, software development, OR", "Master's Degree in Computer Science or Engineering", 'Must be U.S. Citizen to be considered for this opportunity']}, {'title': 'Benefits', 'items': ['We offer excellent development opportunities, a competitive salary, and exceptional benefits', 'These include bonus, employee support assistance and employee discounts', "As a minimum we're all expected to attend the workplace for collaboration and other specific reasons, on average three days per week", '$90,985 - $136,477-Annually', 'Rolls-Royce provides a comprehensive and competitive Total Rewards package that includes base pay and a discretionary bonus plan', 'Eligible employees may have the opportunity to enroll in other benefits, including health, dental, vision, disability, life and accidental death & dismemberment insurance; a flexible spending account; a health savings account; a 401(k) retirement savings plan with a company match; Employee Assistance Program; Paid Time Off; certain paid holidays; paid parental and family care leave; tuition reimbursement; and a long-term incentive plan']}, {'title': 'Responsibilities', 'items': ['This role will ensure that critical health data is captured, stored, and made accessible for analytics, machine learning models, and decision-making systems', 'You will be responsible for database design, performance optimization, data quality assurance, and integration with other system', 'This is a highly technical role that includes data engineering, data pipeline development, and data analysis and visualization to support predictive maintenance and fleet health monitoring', 'Design, implement, and maintain scalable and secure database system for PHM data', 'Optimize database performance, query speed, and storage efficiency', 'Build ETS (Extract, Transform, Load) pipeline to establish automated data and file exchange capabilities for all PHM data', 'Ensure seamless integration between OHM databases and analytics/visualization tools', 'Manage ongoing relationship with Rolls-Royce IT with respect to enterprise systems and LiftSystem-specific systems supported by the IT', 'Provide technical advice on data architecture and database technologies to broader PHM capability', 'Develop and implement data validation strategies to ensure accuracy and consistency']}], 'apply_options': [{'title': 'Women For Hire- Job Board', 'link': 'https://jobs.womenforhire.com/job/usa/new-whiteland-in/prognostics-health-management-phm-data-engineer-liftsystem-667508/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJQcm9nbm9zdGljcyBcdTAwMjYgSGVhbHRoIE1hbmFnZW1lbnQgKFBITSkgRGF0YSBFbmdpbmVlciAtIExpZnRTeXN0ZW0iLCJjb21wYW55X25hbWUiOiJSb2xscyBSb3ljZSIsImFkZHJlc3NfY2l0eSI6Ik5ldyBXaGl0ZWxhbmQsIElOIiwiaHRpZG9jaWQiOiJKMW5hM2tCY2N0bDc2cjh2QUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': 'Data Fabric Architect, Autonomous Networks', 'company_name': 'Nokia', 'location': 'United States', 'via': 'Nokia Careers', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=LyTTbdFBaidCxak1AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWNsQrCMBBAce0HODjdLJqI4KJTQRQcuohzucSjTZvmSu6K_RQ_17q86T1e8V0V5RUV4YYuBw9l9m1Q8rqDclJOPPAkUJF-OPcCe3iwAyFcNOAEd-Ym0ubSqo5ytlYkmkYUNXjjebCcyPFsO3byRy0tZhojKtXH02E2Y2q264r7gBASvNKyfsNz6Ul-e8-wjpsAAAA&shmds=v1_AdeF8KiLyr1hYN8PkudCAw3s8QqYH213C4qVyXEnrIt0JFiZDw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=LyTTbdFBaidCxak1AAAAAA%3D%3D', 'extensions': ['Full-time'], 'detected_extensions': {'schedule_type': 'Full-time'}, 'description': "Nokia's Cloud and Network Services (CNS) business group is building out a suite of products and capabilities that drive our Autonomous Networks vision. We are creating a data fabric that brings together all the information that we gather from telecom networks with the goal of feeding intelligent decision making for autonomous operations. The Data Engineering Architect for Autonomous Networks will be responsible for the data architecture of the Autonomous Network Fabric (ANF), a set of SaaS as well as on-premises services operating at petabyte scale. This is a design and architecture role with a strong hands-on engineering component.", 'job_highlights': [{'title': 'Qualifications', 'items': ['This is a design and architecture role with a strong hands-on engineering component']}, {'title': 'Responsibilities', 'items': ['The Data Engineering Architect for Autonomous Networks will be responsible for the data architecture of the Autonomous Network Fabric (ANF), a set of SaaS as well as on-premises services operating at petabyte scale']}], 'apply_options': [{'title': 'Nokia Careers', 'link': 'https://jobs.nokia.com/en/job/20516?utm_medium=search+engine&amp;utm_source=google&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Indeed', 'link': 'https://www.indeed.com/viewjob?jk=aef7ec40f0da6802&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'BeBee', 'link': 'https://us.bebee.com/job/8a369fd5f398e369ccf73e086785af40?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEYXRhIEZhYnJpYyBBcmNoaXRlY3QsIEF1dG9ub21vdXMgTmV0d29ya3MiLCJjb21wYW55X25hbWUiOiJOb2tpYSIsImFkZHJlc3NfY2l0eSI6IlVuaXRlZCBTdGF0ZXMiLCJodGlkb2NpZCI6Ikx5VFRiZEZCYWlkQ3hhazFBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Data Engineer/Analyst Internship at Austin FC Austin, TX', 'company_name': 'Austin FC', 'location': 'Austin, TX', 'via': 'Friends Of Type', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=GS8Pyhb6a_wYgcRZAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_0XNPQrCQBBAYWxzAcFqatGsCDZaBf_QxsbCLkzCsLuyzoSdEeKJvKYKQpqP173iPSouOzSEPfvIRNlVjOmlBic2yqwhdoAG1VMtMhy2_5rB9QZzOEsDSpjbAMJwFPGJJptg1unaOdVUejW02JatPJwwNdK7uzT6o9aAmbqERvVytejLjv10PJy-DrMPZD0cf6oAAAA&shmds=v1_AdeF8KjHUzTt9-d2_VaOF0C7G0dWf_waBjLgUm-EMutY_0fcvQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=GS8Pyhb6a_wYgcRZAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965c1f9aa73feac31380f/images/718d982c06e7b28dce6fe9667ccc70c9f58e09b6d9a4fa82081286f6ea68e1cb.png', 'extensions': ['21 days ago', 'Full-time'], 'detected_extensions': {'posted_at': '21 days ago', 'schedule_type': 'Full-time'}, 'description': 'Data Engineer/Analyst Internship job at Austin FC. Austin, TX.\n\nAustin FC joined Major League Soccer (MLS) as the 27th club in January 2019. Austin FC began competing in MLS in April 2021 and played its first match at home on June 19, 2021, in the new, 100% privately financed, state-of-the-art Q2 Stadium. Austin FC currently holds the longest active sellout streak in MLS.\n\nWe are actively seeking a Data Engineer / Analyst Intern to join Austin FCâ€™s Analysis department.\n\nIn this internship, you will gain valuable exposure to a professional environment by helping to design and build systems to collect, store, and analyze data within the analysis department. You will also gain experience in video and data capturing, drone flying, learning the Austin FC style of play, and helping coaches and players utilize data as a learning tool.\n\nThis position will be based in Austin, TX and the role will be in-person at St. Davidâ€™s Performance Center.\n\nAre You Someone That:\n\nÂ· Is passionate about designing, developing, and deploying database structures and extracting data?\n\nÂ· Is interested in finding data insights to help drive performance?\n\nÂ· Wants to learn and develop their information gathering skills to gain an edge over our opponents?\n\nÂ· Enjoys working as a positive team player to meet goals?\n\nÂ· Is agile and able to respond effectively to the changing needs of a fast-paced growing organization?\n\nYou Will:\n\nÂ· Be involved in building and maintaining data pipelines to efficiently store internal and external data in an easily accessible fashion; and\n\nÂ· Learn how to build and maintain scalable and secure data storage solutions using cloud platforms to store structured and unstructured data; and\n\nÂ· Assist in designing and implementing scalable, secure, and efficient API solutions; and\n\nÂ· Assist the Analysis Department in opponent scouting and post-match analysis; and\n\nÂ· Learn how to fly a drone and film training sessions; and\n\nÂ· Work with the Analysis staff to distribute data and video to coaching staff and players; and\n\nÂ· Gain exposure to high level soccer players and coaches; and\n\nÂ· Attend all home games through the duration of the Internship; and\n\nÂ· Work with First Team Analysis staff on projects related to club-wide philosophies as needed.\n\nYou Have:\n\nÂ· Currently studying computer science, data engineering, data science, statistics, mathematics or a comparable qualification\n\nÂ· Experience in API design, development, and architecture\n\nÂ· Expertise in programming languages such as R and Python\n\nÂ· Knowledge of data visualization tools such as Tableau\n\nÂ· Demonstrated interest in performance analysis in sports\n\nÂ· High level of communication skills with different groups of people\n\nÂ· Ability to learn to fly a drone\n\nOther Details:\n\nÂ· It is the policy of Austin FC not to discriminate against any employee or applicant for internships because of race, color, sex, national origin, religion, age, gender, sexual orientation, gender identity, gender expression, physical or mental disability, marital status, genetic information, or any other characteristic protected by applicable law.\n\nÂ· All selected candidates are subject to passing a background check prior to acceptance as an intern.\n\nÂ· Hours include some evenings and weekends. The internship time commitment is generally between 15 â€“ 20 hours per week.\n\nÂ· This is a part-time unpaid internship and students ideally be able to gain academic credit for their participation.\n\nÂ· The position can be remote at times, but in-person participation and matchday attendance is preferred.', 'job_highlights': [{'title': 'Qualifications', 'items': ['Is passionate about designing, developing, and deploying database structures and extracting data?', 'Wants to learn and develop their information gathering skills to gain an edge over our opponents?', 'Enjoys working as a positive team player to meet goals?', 'Is agile and able to respond effectively to the changing needs of a fast-paced growing organization?', 'Currently studying computer science, data engineering, data science, statistics, mathematics or a comparable qualification', 'Experience in API design, development, and architecture', 'Expertise in programming languages such as R and Python', 'Knowledge of data visualization tools such as Tableau', 'Demonstrated interest in performance analysis in sports', 'High level of communication skills with different groups of people', 'Ability to learn to fly a drone', 'This is a part-time unpaid internship and students ideally be able to gain academic credit for their participation']}, {'title': 'Benefits', 'items': ['The position can be remote at times, but in-person participation and matchday attendance is preferred']}, {'title': 'Responsibilities', 'items': ['In this internship, you will gain valuable exposure to a professional environment by helping to design and build systems to collect, store, and analyze data within the analysis department', 'You will also gain experience in video and data capturing, drone flying, learning the Austin FC style of play, and helping coaches and players utilize data as a learning tool', 'Be involved in building and maintaining data pipelines to efficiently store internal and external data in an easily accessible fashion; and', 'Learn how to build and maintain scalable and secure data storage solutions using cloud platforms to store structured and unstructured data; and', 'Assist in designing and implementing scalable, secure, and efficient API solutions; and', 'Assist the Analysis Department in opponent scouting and post-match analysis; and', 'Learn how to fly a drone and film training sessions; and', 'Work with the Analysis staff to distribute data and video to coaching staff and players; and', 'Gain exposure to high level soccer players and coaches; and', 'Attend all home games through the duration of the Internship; and', 'Work with First Team Analysis staff on projects related to club-wide philosophies as needed', 'Hours include some evenings and weekends', 'The internship time commitment is generally between 15 â€“ 20 hours per week']}], 'apply_options': [{'title': 'Friends Of Type', 'link': 'https://friendsoftype.com/job-listing/job/data-engineeranalyst-internship-at-austin-fc-austin-tx-elhYS3diSkdkTmtxVkJIL1RHR1o1bHJwUGc9PQ==?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Rsupb.com', 'link': 'https://rsupb.com/job-library/job/data-engineeranalyst-internship-at-austin-fc-austin-tx-RnUxcWIyVVdDbnNpVzdxaHpHeGUvdVp2eUE9PQ==?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Beach House Films', 'link': 'https://beachhousefilms.com.au/job-listing/job/data-engineeranalyst-internship-at-austin-fc-austin-tx-VEhTZkxUbGN3SGZWa3gzNFVhOHh4UUU0T2c9PQ==?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Aquesabeelairelibre.com', 'link': 'https://aquesabeelairelibre.com/library/job/copywriter-redactor-at-austin-fc-austin-tx-YjZjQndGVkNmbmdSNUlCYVQ5YmQ3akpD?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Nurtured Mumma', 'link': 'https://nurturedmumma.com/opportunity/job/data-engineeranalyst-internship-at-austin-fc-austin-tx-S0dSRk5yTlBmdC9TTjJOaWhNNUpsTWtSTkE9PQ==?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'A Class Apart', 'link': 'https://aclassapartmovie.com/career/job/data-engineeranalyst-internship-at-austin-fc-austin-tx-OGlydEhPZFRmNVo0T1ZYN0NXMWpqRXRueGc9PQ==?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Punktservice', 'link': 'http://punktservice.de/library/job/data-engineeranalyst-internship-at-austin-fc-austin-tx-MHBpbjdWK2pFUFBsYkhVRk8weVJPVDkyd0E9PQ==?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Airmanage.dk', 'link': 'https://airmanage.dk/vacancy/job/data-engineeranalyst-internship-at-austin-fc-austin-tx-djVQcDJmTUcxT05HNkp4Z0ZRbTBIWGRuaVE9PQ==?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyL0FuYWx5c3QgSW50ZXJuc2hpcCBhdCBBdXN0aW4gRkMgQXVzdGluLCBUWCIsImNvbXBhbnlfbmFtZSI6IkF1c3RpbiBGQyIsImFkZHJlc3NfY2l0eSI6IkF1c3RpbiwgVFgiLCJodGlkb2NpZCI6IkdTOFB5aGI2YV93WWdjUlpBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Lead Data Engineer (Enterprise Platform Technology)', 'company_name': 'Capital One', 'location': 'Shiloh, PA', 'via': 'WhatJobs', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=RKurMVsdTOlO4ZnCAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMsQrCMBAAUFz7BzrdqKKNCC46iRahCAq6l2s9k0h6F3IZ6v_4oeryxld8RkV9JnzAETNCxdYzUYJpxZlSTF4JrgHzU1IPd-ocSxD7nsESamlBCVPnQBhOIjbQZOdyjro1RjWUVjNm35Wd9EaYWhnMS1r906jDRPE3U7PerIYysp2PDxh9xgAXJvAMN-eDuAVc919BIlnmpwAAAA&shmds=v1_AdeF8KhtfxSVs9kek_Rcexfjpj-v6xfIqzZDuR40uOIl1k1F-w&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=RKurMVsdTOlO4ZnCAAAAAA%3D%3D', 'extensions': ['4 days ago', 'Full-time and Part-time', 'Health insurance'], 'detected_extensions': {'posted_at': '4 days ago', 'schedule_type': 'Full-time and Part-time', 'health_insurance': True}, 'description': "Lead Data Engineer (Enterprise Platform Technology) at Capital One summary:\n\nThe Lead Data Engineer at Capital One designs, develops, tests, and implements advanced data solutions using big data technologies and cloud platforms in a collaborative Agile environment. They work with distributed microservices, machine learning, and full-stack development tools to drive enterprise platform transformations and deliver scalable, cloud-based data warehousing solutions. This role requires deep experience with programming languages such as Python, Java, and Scala, as well as expertise in databases, streaming applications, and mentoring engineering teams.\n\nLead Data Engineer (Enterprise Platform Technology)\n\nDo you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative,inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Lead Data Engineer, youâ€™ll have the opportunity to be on the forefront of driving a major transformation within Capital One.\n\nEnterprise Platforms Technology (EPTech) comprises many of Capital Oneâ€™s most important enterprise platforms. We play an essential role in establishing practices for building technology solutions across the company, while also delivering capabilities that exemplify those practices.\n\nWhat Youâ€™ll Do:\n\nCollaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies\n\nWork with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems\n\nUtilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake\n\nShare your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community\n\nCollaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment\n\nPerform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance\n\nBasic Qualifications:\n\nBachelorâ€™s Degree\n\nAt least 4 years of experience in application development (Internship experience does not apply)\n\nAt least 2 years of experience in big data technologies\n\nAt least 1 year experience with cloud computing (AWS, Microsoft Azure, Google Cloud)\n\nPreferred Qualifications:\n\n7+ years of experience in application development including Python, SQL, Scala, or Java\n\n4+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)\n\n4+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)\n\n4+ year experience working on real-time data and streaming applications\n\n4+ years of experience with NoSQL implementation (Mongo, Cassandra)\n\n4+ years of data warehousing experience (Redshift or Snowflake)\n\n4+ years of experience with UNIX/Linux including basic commands and shell scripting\n\n2+ years of experience with Agile engineering practices\n\nAt this time, Capital One will not sponsor a new applicant for employment authorization, or offer any immigration related support for this position (i.e. H1B, F-1 OPT, F-1 STEM OPT, F-1 CPT, J-1, TN, E-2, E-3, L-1 and O-1, or any EADs or other forms of work authorization that require immigration support from an employer).\n\nThe minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.\nMcLean, VA: $193,400 - $220,700 for Lead Data Engineer\n\nRichmond, VA: $175,800 - $200,700 for Lead Data Engineer\n\nCandidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidateâ€™s offer letter.\nThis role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n\nCapital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at theCapital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.\nThis role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer (EOE, including disability/vet) committed to non-discrimination in compliance with applicable federal, state, and local laws. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections ; New York Cityâ€™s Fair Chance Act; Philadelphiaâ€™s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n\nIf you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at or via email at . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.\n\nFor technical support or questions about Capital One's recruiting process, please send an email to\n\nCapital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.\n\nCapital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).\n\nKeywords:\n\nLead Data Engineer, Big Data, Cloud Computing, AWS, Snowflake, Redshift, Java, Python, Scala, Machine Learning, Agile, Data Warehousing, NoSQL, Distributed Systems", 'job_highlights': [{'title': 'Qualifications', 'items': ['This role requires deep experience with programming languages such as Python, Java, and Scala, as well as expertise in databases, streaming applications, and mentoring engineering teams', 'Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake', 'Bachelorâ€™s Degree', 'At least 4 years of experience in application development (Internship experience does not apply)', 'At least 2 years of experience in big data technologies', 'At least 1 year experience with cloud computing (AWS, Microsoft Azure, Google Cloud)', '7+ years of experience in application development including Python, SQL, Scala, or Java', '4+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)', '4+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)', '4+ year experience working on real-time data and streaming applications', '4+ years of experience with NoSQL implementation (Mongo, Cassandra)', '4+ years of data warehousing experience (Redshift or Snowflake)', '4+ years of experience with UNIX/Linux including basic commands and shell scripting', '2+ years of experience with Agile engineering practices', 'At this time, Capital One will not sponsor a new applicant for employment authorization, or offer any immigration related support for this position (i.e. H1B, F-1 OPT, F-1 STEM OPT, F-1 CPT, J-1, TN, E-2, E-3, L-1 and O-1, or any EADs or other forms of work authorization that require immigration support from an employer)']}, {'title': 'Benefits', 'items': ['Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked', 'McLean, VA: $193,400 - $220,700 for Lead Data Engineer', 'Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidateâ€™s offer letter', 'This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI)', 'Incentives could be discretionary or non discretionary depending on the plan', 'Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being']}, {'title': 'Responsibilities', 'items': ['The Lead Data Engineer at Capital One designs, develops, tests, and implements advanced data solutions using big data technologies and cloud platforms in a collaborative Agile environment', 'They work with distributed microservices, machine learning, and full-stack development tools to drive enterprise platform transformations and deliver scalable, cloud-based data warehousing solutions', 'Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies', 'Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems', 'Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community', 'Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment', 'Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance']}], 'apply_options': [{'title': 'WhatJobs', 'link': 'https://www.whatjobs.com/jobs/enterprise-solutions?id=2258110756&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJMZWFkIERhdGEgRW5naW5lZXIgKEVudGVycHJpc2UgUGxhdGZvcm0gVGVjaG5vbG9neSkiLCJjb21wYW55X25hbWUiOiJDYXBpdGFsIE9uZSIsImFkZHJlc3NfY2l0eSI6IlNoaWxvaCwgUEEiLCJodGlkb2NpZCI6IlJLdXJNVnNkVE9sTzRabkNBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Staff Data Engineer, Square Banking', 'company_name': 'Block, Inc.', 'location': 'San Francisco, CA', 'via': 'Karkidi', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=69GrXxGuSy-qeETNAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMOw7CMAwAULH2CEyeUUkQEkuZKD_B2gNUrpWmocEucZB6Fy4LLG98xWdRVE3GvocTZoQz-8DOpRKa1xuTgxp5DOxhDXfpQB0mGkAYriI-uuV-yHnSylrVaLxmzIEMydMKu05m-5BO_7Q6_LYpYnbtdreZzcR-BXUUGku4MRkIDA0yXBIyBSUp4Xj4Akz_W2yeAAAA&shmds=v1_AdeF8Kj6k9JX_LQv4p51fkszS7ei-NGI46ZtzYmZ1oXwmhxEYA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=69GrXxGuSy-qeETNAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965c1f9aa73feac31380f/images/718d982c06e7b28d5d9c571edc8a4b5b15a4aafe2c25ea3aeac30f68b3468b51.jpeg', 'extensions': ['202Kâ€“248K a year', 'Full-time', 'No degree mentioned', 'Health insurance', 'Paid time off', 'Dental insurance'], 'detected_extensions': {'salary': '202Kâ€“248K a year', 'schedule_type': 'Full-time', 'qualifications': 'No degree mentioned', 'health_insurance': True, 'paid_time_off': True, 'dental_coverage': True}, 'description': "The Square Banking team is building a suite of new financial products for Square sellers. We offer business checking accounts, savings accounts, credit card, and loans to help our sellers manage their business cash flow. Investing in a Financial Data Mesh Platform is not just about managing data; it's about unleashing the full potential of our organization's most valuable asset. Itâ€™s a critical strategic move that not only empowers us to use the distinctive value of data but also extends its positive impact to our customers, our Sellers, and users of the Banking platform.\n\nAs an Engineer focused on Data for Square Banking, you will help us build our own Square Banking Financial Data Mesh Platform, using real-time Big Data technologies and Medallion architecture. You will work directly with product, engineering, data science and machine learning teams to understand their use-case, develop reliable, trusted datasets that accelerate the decision-making process of important products.\n\nYou will:\nâ€¢ You'll design large-scale, distributed data processing systems and pipelines to ensure efficient and reliable data ingestion, storage, transformation, and analysis\nâ€¢ Promote high-quality software engineering practices towards building data infrastructure and pipelines at scale\nâ€¢ You'll build core datasets to serve as unique sources of truth for product and departments (product, marketing, sales, finance, customer experience, data science, business operations, IT, engineering)\nâ€¢ You'll partner with data scientists and other cross-functional partners to understand their needs and build pipelines to scale.\nâ€¢ Identify and address data quality and integrity issues through data validation, cleansing, and data modeling techniques. You'll implement automated workflows that lower manual/operational cost for team members, define and uphold SLAs for timely delivery of data, move us closer to democratizing data and a self-serve model (query exploration, dashboards, data catalog, data discovery)\nâ€¢ Learn about Big Data architecture via technologies such as AWS, DataBricks and Kafka.\nâ€¢ Stay up to date with emerging technologies, best practices, and industry trends in data engineering and software development\nâ€¢ Mentor and provide guidance to junior data engineers fostering inclusivity and growth.\nâ€¢ Work remotely with a team of distributed colleagues #LI-Remote\nâ€¢ Report to the Engineering Manager of Banking - Data Engineering\n\nQualifications\n\nYou Have:\nâ€¢ 8+ years as a data engineer or software engineer, with a focus on large-scale data processing and analytics\nâ€¢ You've spent 4+ years as a data engineer building core datasets.\nâ€¢ You are passionate about analytics use cases, data models and solving complex data problems.\nâ€¢ You have hands-on experience shipping scalable data solutions in the cloud (e.g AWS, GCP, Azure), across multiple data stores (e.g Databricks, Snowflake, Redshift, Hive, SQL/NoSQL, columnar storage formats) and methodologies (e.g dimensional modeling, data marts, star/snowflake schemas)\nâ€¢ You have hands-on experience with highly scalable and reliable data pipelines using BigData (e.g Airflow, DBT, Spark, Hive, Parquet/ORC, Protobuf/Thrift, etc)\nâ€¢ Optimized and tuned data pipelines to enhance overall system performance, reliability, and scalability\nâ€¢ Knowledge of programming languages (e.g. Go, Ruby, Java, Python)\nâ€¢ Willingness to participate in professional development activities to stay current on industry knowledge and passion for trying new things.\n\nAdditional Information\n\nBlock takes a market-based approach to pay, and pay may vary depending on your location. U.S. locations are categorized into one of four zones based on a cost of labor index for that geographic area. The successful candidateâ€™s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future.\n\nZone A: USD $202,500 - USD $247,500\n\nZone B: USD $192,400 - USD $235,200\n\nZone C: USD $182,300 - USD $222,800\n\nZone D: USD $172,200 - USD $210,400\n\nTo find a locationâ€™s zone designation, please refer to this resource. If a location of interest is not listed, please speak with a recruiter for additional information.\n\nFull-time employee benefits include the following:\nâ€¢ Healthcare coverage (Medical, Vision and Dental insurance)\nâ€¢ Health Savings Account and Flexible Spending Account\nâ€¢ Retirement Plans including company match\nâ€¢ Employee Stock Purchase Program\nâ€¢ Wellness programs, including access to mental health, 1:1 financial planners, and a monthly wellness allowance\nâ€¢ Paid parental and caregiving leave\nâ€¢ Paid time off (including 12 paid holidays)\nâ€¢ Paid sick leave (1 hour per 26 hours worked (max 80 hours per calendar year to the extent legally permissible) for non-exempt employees and covered by our Flexible Time Off policy for exempt employees)\nâ€¢ Learning and Development resources\nâ€¢ Paid Life insurance, AD&D, and disability benefits\n\nThese benefits are further detailed in Block's policies. This role is also eligible to participate in Block's equity plan subject to the terms of the applicable plans and policies, and may be eligible for a sign-on bonus. Sales roles may be eligible to participate in a commission plan subject to the terms of the applicable plans and policies. Pay and benefits are subject to change at any time, consistent with the terms of any applicable compensation or benefit plans.\n\nWeâ€™re working to build a more inclusive economy where our customers have equal access to opportunity, and we strive to live by these same values in building our workplace. Block is a proud equal opportunity employer. We work hard to evaluate all employees and job applicants consistently, without regard to race, color, religion, gender, national origin, age, disability, veteran status, pregnancy, gender expression or identity, sexual orientation, citizenship, or any other legally protected class.\n\nWe believe in being fair, and are committed to an inclusive interview experience, including providing reasonable accommodations to disabled applicants throughout the recruitment process. We encourage applicants to share any needed accommodations with their recruiter, who will treat these requests as confidentially as possible. Want to learn more about what weâ€™re doing to build a workplace that is fair and square? Check out our I+D page.\n\nAdditionally, we consider qualified applicants with criminal histories for employment on our team, assessing candidates in a manner consistent with the requirements of the San Francisco Fair Chance Ordinance.", 'job_highlights': [{'title': 'Qualifications', 'items': ['8+ years as a data engineer or software engineer, with a focus on large-scale data processing and analytics', "You've spent 4+ years as a data engineer building core datasets", 'You are passionate about analytics use cases, data models and solving complex data problems', 'You have hands-on experience shipping scalable data solutions in the cloud (e.g AWS, GCP, Azure), across multiple data stores (e.g Databricks, Snowflake, Redshift, Hive, SQL/NoSQL, columnar storage formats) and methodologies (e.g dimensional modeling, data marts, star/snowflake schemas)', 'You have hands-on experience with highly scalable and reliable data pipelines using BigData (e.g Airflow, DBT, Spark, Hive, Parquet/ORC, Protobuf/Thrift, etc)', 'Optimized and tuned data pipelines to enhance overall system performance, reliability, and scalability', 'Knowledge of programming languages (e.g. Go, Ruby, Java, Python)', 'Willingness to participate in professional development activities to stay current on industry knowledge and passion for trying new things']}, {'title': 'Benefits', 'items': ['Zone B: USD $192,400 - USD $235,200', 'Zone C: USD $182,300 - USD $222,800', 'USD $172,200 - USD $210,400', 'Healthcare coverage (Medical, Vision and Dental insurance)', 'Health Savings Account and Flexible Spending Account', 'Retirement Plans including company match', 'Employee Stock Purchase Program', 'Wellness programs, including access to mental health, 1:1 financial planners, and a monthly wellness allowance', 'Paid parental and caregiving leave', 'Paid time off (including 12 paid holidays)', 'Paid sick leave (1 hour per 26 hours worked (max 80 hours per calendar year to the extent legally permissible) for non-exempt employees and covered by our Flexible Time Off policy for exempt employees)', 'Learning and Development resources', 'Paid Life insurance, AD&D, and disability benefits', "These benefits are further detailed in Block's policies", "This role is also eligible to participate in Block's equity plan subject to the terms of the applicable plans and policies, and may be eligible for a sign-on bonus", 'Sales roles may be eligible to participate in a commission plan subject to the terms of the applicable plans and policies', 'Pay and benefits are subject to change at any time, consistent with the terms of any applicable compensation or benefit plans']}, {'title': 'Responsibilities', 'items': ['You will work directly with product, engineering, data science and machine learning teams to understand their use-case, develop reliable, trusted datasets that accelerate the decision-making process of important products', "You'll design large-scale, distributed data processing systems and pipelines to ensure efficient and reliable data ingestion, storage, transformation, and analysis", 'Promote high-quality software engineering practices towards building data infrastructure and pipelines at scale', "You'll build core datasets to serve as unique sources of truth for product and departments (product, marketing, sales, finance, customer experience, data science, business operations, IT, engineering)", "You'll partner with data scientists and other cross-functional partners to understand their needs and build pipelines to scale", 'Identify and address data quality and integrity issues through data validation, cleansing, and data modeling techniques', "You'll implement automated workflows that lower manual/operational cost for team members, define and uphold SLAs for timely delivery of data, move us closer to democratizing data and a self-serve model (query exploration, dashboards, data catalog, data discovery)", 'Learn about Big Data architecture via technologies such as AWS, DataBricks and Kafka', 'Stay up to date with emerging technologies, best practices, and industry trends in data engineering and software development', 'Mentor and provide guidance to junior data engineers fostering inclusivity and growth', 'Work remotely with a team of distributed colleagues #LI-Remote', 'Report to the Engineering Manager of Banking - Data Engineering']}], 'apply_options': [{'title': 'Karkidi', 'link': 'https://www.karkidi.com/job-details/76925-staff-data-engineer-square-banking-job?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTdGFmZiBEYXRhIEVuZ2luZWVyLCBTcXVhcmUgQmFua2luZyIsImNvbXBhbnlfbmFtZSI6IkJsb2NrLCBJbmMuIiwiYWRkcmVzc19jaXR5IjoiU2FuIEZyYW5jaXNjbywgQ0EiLCJodGlkb2NpZCI6IjY5R3JYeEd1U3ktcWVFVE5BQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Data Engineer (AWS, Databricks)', 'company_name': 'NISC', 'location': 'West Virginia', 'via': 'Built In', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=Q1J19Hm3vbqh5uIuAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNoQ7CMBCA4WDnMaiTQGAlJEyAIkAICMwSJpdrc-kKpbf0TuwheGiY-cVn_uI7KaozKsIl-ZCIMsyPTb2C0WwO7i0LWMOdLQhhdh1wgiuzjzQ7dKq97I0RiaUXRQ2udPwxnMjyYF5sZUwrHWbqIyq1291mKPvkl9PHrT5BSNCQKDxD_t8D_gAjIBVajwAAAA&shmds=v1_AdeF8KgShMyi5rnOW36wdzofVoOgkROY4iTmrifSYiHmU2-d4Q&shem=damc&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=Q1J19Hm3vbqh5uIuAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965c29b762d608ab12cd1/images/816525ee76494dfc471e048881532f61f6d415b9756cdee3feeb221da2bff9cd.jpeg', 'extensions': ['19 days ago', 'Full-time', 'Dental insurance', 'Paid time off', 'Health insurance'], 'detected_extensions': {'posted_at': '19 days ago', 'schedule_type': 'Full-time', 'dental_coverage': True, 'paid_time_off': True, 'health_insurance': True}, 'description': "Company Overview:\n\nFor more than 50 years, NISC has worked to develop technology solutions for our customers, who we call our â€œMembersâ€. Those Members are comprised primarily of 960+ utilities and broadbands across North America. Our mission is to deliver technology solutions and services that are Member-focused, quality driven and valued priced. Our Members have over 16 million end customers (residential and businesses who receive power, internet, television and/or telephone services) that our enterprise software solution enables our Member's employees to compete effectively in the industry, while excelling in providing customer service to their end customers.\n\nNISC exist to serve our Members and help them serve their communities through our innovative software products, services and outstanding customer support.\n\nPosition Overview:\n\nWe are seeking an experienced Data Engineer to join our growing team of data analytics experts. The hire will be responsible for curating and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for various application teams. The Data Engineer will support our application experts, software developers, database architects, and data analysts on a Data Roadmap strategy and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be comfortable supporting the data needs of multiple teams, systems, and products. We look for individuals who will thrive in a team environment, be committed to accomplishing a common goal and excited by the prospect of optimizing or even re-designing our companyâ€™s data architecture and have some fun along the way.\n\nWork Schedule:\u202f\nâ€¢ Hybrid from one of our office locations:\nâ€¢ Cedar Rapids, IA\nâ€¢ Lake Saint Louis, MO\nâ€¢ Mandan, ND\u202f\nâ€¢ Hybrid Schedule: Minimum of working 3 day per week out of an office location and ability to work up to all 5 days a week from an office location.\nâ€¢ Required Days from an Office Location: Tuesday and Wednesday - the third required day will be up to the candidate and their supervisor to choose.\nâ€¢ Virtual Office: Candidates working from a remote location within approved states will be considered for those who have applicable industry experience with Databricks.\n\nEssential Duties and Responsibilities:\nâ€¢ Assemble large, complex data sets that meet functional / non-functional business requirements.\nâ€¢ Understanding of Data Warehouse and Data Lakehouse paradigms.\nâ€¢ Design and build optimal data pipelines from a wide variety of data sources using AWS technologies.\nâ€¢ Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.\nâ€¢ Create data tools for analytics and data scientist team members that assist them in building and optimizing a unified data stream.\nâ€¢ Work with other data engineering experts to strive for greater functionality while making data more discoverable, addressable, trustworthy, and secure.\nâ€¢ Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nâ€¢ Create and maintain a culture of engagement and one that is conducive of NISCâ€™s Statement of Shared Values.\nâ€¢ Commitment to NISCâ€™s Statement of Shared Values.\n\nKnowledge, Skills & Abilities Preferred:\nâ€¢ Experience building and optimizing data pipelines, architectures, and data sets.\nâ€¢ Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\nâ€¢ Strong analytic skills related to working with unstructured datasets.\nâ€¢ Build ETL processes supporting data transformation, data structures, metadata, dependency, and workload management.\nâ€¢ Working knowledge of message queuing, stream processing, and highly scalable data stores.\nâ€¢ Experience supporting and working with cross-functional teams in a dynamic environment.\nâ€¢ Candidate with experience in a Data Engineer role, who has attained a BS or MS degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field. They should also have experience using the following software/tools:\nâ€¢ Experience with AWS: Lambda, S3, SQS, SNS, CloudWatch, etc.\nâ€¢ Experience with Databricks and Delta Lake.\nâ€¢ Experience with big data tools: Hadoop, Spark, Kafka, etc.\nâ€¢ Experience with relational SQL and NoSQL databases, including Oracle, Postgres Cassandra, and DynamoDb.\nâ€¢ Experience with data pipeline and workflow management tools: Hevo Data, Airflow, etc.\nâ€¢ Experience with AWS cloud services: EC2, Databricks, EMR\nâ€¢ Experience with stream-processing systems: Apache Spark, Kafka Streams, Spring Cloud, etc.\nâ€¢ Experience with object-oriented languages: Java, Scala.\nâ€¢ Nice-to-have\nâ€¢ Experience with scripting languages: Python, JavaScript, Bash, etc.\nâ€¢ Strong verbal and written communication skills.\nâ€¢ Ability to demonstrate composure and think analytically in high pressure situations.\n\nNISCâ€™s Shared Values & Competencies:\nâ€¢ Integrity â€“ We are committed to doing the right thing â€“ always.\nâ€¢ Relationships â€“ We are committed to building and preserving lasting relationships.\nâ€¢ Innovation â€“ We promote the spirit of creativity and champion new ideas.\nâ€¢ Teamwork â€“ We exemplify the cooperative spirit by working together.\nâ€¢ Empowerment â€“ We believe individuals have the power to make a difference.\nâ€¢ Personal Development â€“ We believe the free exchange of knowledge and information is absolutely necessary to the success of each individual and the organization.\n\nBenefits:\nâ€¢ Medical, Dental and Vision Insurance.\nâ€¢ Health Savings Account (HSA) with $100 monthly contributions from NISC.\nâ€¢ Like to walk? Improve your overall wellness knowledge? Ability to earn up to $800 additional dollars into your HSA each year through our Wellness Rewards program.\nâ€¢ Dependent Care Flexible Spending Account (FSA) thru Paylocity.\nâ€¢ Fully covered life insurance up to x3 annual base salary.\nâ€¢ Fully covered short- and long-term disability.\nâ€¢ 401(k), traditional or Roth, with employee match up to 6% and employer 4% salary base contributions.\nâ€¢ PTO accrual levels dependent on years of service, 120 Life Leave Event hours, 9 paid holidays and an annual holiday week.\nâ€¢ $2,500 Interest-FREE technology loan program.\nâ€¢ $25,000 employee educational assistance program.\nâ€¢ Volunteer, Wellness, Family Events and other employee fun supplied by our committees.\nâ€¢ Employee Assistance Program; assisting employees and dependents with virtually any life event\nâ€¢ Benevolence Committee to support employees with financial hardships like unexpected medical bills, funerals and other unfortunate hardships.\n\nEducation Preferred:\nâ€¢ Bachelorâ€™s degree in Computer Science, Statistics, Informatics, Information Systems or similar discipline, preferred.\nâ€¢ Certification in Database Administration, along with relevant experience in lieu of 4-year degree.\n\nMinimum Physical Requirements:\n\nThe physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this position. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the essential functions of this position, employees must be able to see and communicate.\u202f Employees are regularly required to maintain a stationary position, move, and operate computer keyboards or office equipment.\n\nDisclaimer:\n\nManagement may modify this job description by assigning or reassigning duties and responsibilities at any time.\n\nKey Words:\u202f\n\nSQL | Data | Big Data | Databricks | ETL | Spark | Scala | DBA| Lakehouse | Postgres | Python | AWS", 'job_highlights': [{'title': 'Qualifications', 'items': ['They must be comfortable supporting the data needs of multiple teams, systems, and products', 'We look for individuals who will thrive in a team environment, be committed to accomplishing a common goal and excited by the prospect of optimizing or even re-designing our companyâ€™s data architecture and have some fun along the way', 'They should also have experience using the following software/tools:', 'Experience with AWS: Lambda, S3, SQS, SNS, CloudWatch, etc', 'Experience with Databricks and Delta Lake', 'Experience with big data tools: Hadoop, Spark, Kafka, etc', 'Experience with relational SQL and NoSQL databases, including Oracle, Postgres Cassandra, and DynamoDb', 'Experience with data pipeline and workflow management tools: Hevo Data, Airflow, etc', 'Experience with AWS cloud services: EC2, Databricks, EMR', 'Experience with stream-processing systems: Apache Spark, Kafka Streams, Spring Cloud, etc', 'Experience with object-oriented languages: Java, Scala', 'Nice-to-have', 'Experience with scripting languages: Python, JavaScript, Bash, etc', 'Strong verbal and written communication skills', 'Ability to demonstrate composure and think analytically in high pressure situations', 'Integrity â€“ We are committed to doing the right thing â€“ always', 'Relationships â€“ We are committed to building and preserving lasting relationships', 'Innovation â€“ We promote the spirit of creativity and champion new ideas', 'Teamwork â€“ We exemplify the cooperative spirit by working together', 'Empowerment â€“ We believe individuals have the power to make a difference', 'Personal Development â€“ We believe the free exchange of knowledge and information is absolutely necessary to the success of each individual and the organization', 'The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this position', 'While performing the essential functions of this position, employees must be able to see and communicate.\u202f Employees are regularly required to maintain a stationary position, move, and operate computer keyboards or office equipment']}, {'title': 'Benefits', 'items': ['Medical, Dental and Vision Insurance', 'Health Savings Account (HSA) with $100 monthly contributions from NISC', 'Ability to earn up to $800 additional dollars into your HSA each year through our Wellness Rewards program', 'Dependent Care Flexible Spending Account (FSA) thru Paylocity', 'Fully covered life insurance up to x3 annual base salary', 'Fully covered short- and long-term disability', '401(k), traditional or Roth, with employee match up to 6% and employer 4% salary base contributions', 'PTO accrual levels dependent on years of service, 120 Life Leave Event hours, 9 paid holidays and an annual holiday week', '$2,500 Interest-FREE technology loan program', '$25,000 employee educational assistance program', 'Volunteer, Wellness, Family Events and other employee fun supplied by our committees', 'Employee Assistance Program; assisting employees and dependents with virtually any life event', 'Benevolence Committee to support employees with financial hardships like unexpected medical bills, funerals and other unfortunate hardships']}, {'title': 'Responsibilities', 'items': ['The hire will be responsible for curating and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for various application teams', 'The Data Engineer will support our application experts, software developers, database architects, and data analysts on a Data Roadmap strategy and will ensure optimal data delivery architecture is consistent throughout ongoing projects', 'Hybrid Schedule: Minimum of working 3 day per week out of an office location and ability to work up to all 5 days a week from an office location', 'Required Days from an Office Location: Tuesday and Wednesday - the third required day will be up to the candidate and their supervisor to choose', 'Virtual Office: Candidates working from a remote location within approved states will be considered for those who have applicable industry experience with Databricks', 'Assemble large, complex data sets that meet functional / non-functional business requirements', 'Understanding of Data Warehouse and Data Lakehouse paradigms', 'Design and build optimal data pipelines from a wide variety of data sources using AWS technologies', 'Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs', 'Create data tools for analytics and data scientist team members that assist them in building and optimizing a unified data stream', 'Work with other data engineering experts to strive for greater functionality while making data more discoverable, addressable, trustworthy, and secure', 'Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc', 'Create and maintain a culture of engagement and one that is conducive of NISCâ€™s Statement of Shared Values', 'Commitment to NISCâ€™s Statement of Shared Values', 'Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions']}], 'apply_options': [{'title': 'Built In', 'link': 'https://builtin.com/job/data-engineer-aws-databricks/7289428?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChBV1MsIERhdGFicmlja3MpIiwiY29tcGFueV9uYW1lIjoiTklTQyIsImFkZHJlc3NfY2l0eSI6Ildlc3QgVmlyZ2luaWEiLCJodGlkb2NpZCI6IlExSjE5SG0zdmJxaDV1SXVBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Sr. Professional, Data Engineering - Full-time', 'company_name': 'Under Armour, Inc.', 'location': 'Anywhere', 'via': 'Snagajob', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=IKwxMuGnHfK5b7ovAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXGsQrCQAwAUFz7CU5xlfYqgotOBWvRSRDnktZ4Pbkm5RKhf-RvqsvjZZ9FVt-Sg2uSJ6kGYYw5HNEQavaBiVJgDwWc3jEWFkb6_SIdKGHqBxCGRsRHWh4Gs0n3ZakanVdDC73rZSyFqZO5fEmnf1odMNEU0ajd7jazm9ivV3d-UIIqjfJOOZy5dxAYKovIhjk01Re5-amoqgAAAA&shmds=v1_AdeF8KgnPtHV_5lLXavlCDV39Im36ja8KX6wlK1R1k-yIxgbnA&shem=damc&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=IKwxMuGnHfK5b7ovAAAAAA%3D%3D', 'extensions': ['5 days ago', 'Work from home', 'Full-time', 'Health insurance'], 'detected_extensions': {'posted_at': '5 days ago', 'work_from_home': True, 'schedule_type': 'Full-time', 'health_insurance': True}, 'description': 'Sr. Professional, Data Engineering\nâ€¢ *Sr. Professional, Data Engineering**\nâ€¢ *Values & Innovation**\n\nAt Under Armour, we are committed to empowering those who strive for more, and the company\'s values - Act Sustainably, Celebrate the Wins, Fight on Together, Love Athletes and Stand for Equality - serve as both a roadmap for our teams and the qualities expected of every teammate.\n\nOur Values define and unite us, the beliefs that are the red thread that connects everyone at Under Armour. Our values are rallying cries, reminding us why we\'re here, and fueling everything we do.\n\nOur pursuit of better begins with innovation and with our team\'s mission of being the best. With us, you get the freedom to go further - no matter your role. That means developing, delivering, and selling the state-of-the-art products and digital tools that make top performers even better.\n\nIf you are a current Under Armour teammate, apply to this position on theInternal Career Site Here. (https://performancemanager8.successfactors.com/sf/careers/jobsearch?bplte\\_company=ua&\\_s.crb=aNMP8gWoYkBDFn%252bz2BldysgcgQHZpVs6tHzE9smSuXE%253d)\nâ€¢ *Purpose of Role**\n\nAt Under Armour, weâ€™re building the data products and services to power the future of our direct to consumer experience as well as other core aspects of our performance brand.\n\nPerforming alongside data analytics, data science, and digital marketing teams and across channels such as MapMyFitness and UA.com, weâ€™d like you to join our cross-cutting Data Platform team in building real solutions to centralize, enrich, and activate data across the enterprise. You will learn, grow, and play in an environment that focuses on results and delivery, all backed by one of the strongest consumer brands in history.\n\nWe\'re looking to add a new Sr. Data Platform Engineer to join our Enterprise Data Management team, which powers the storage, processing, integration and cataloging of our data. In this role, you will work across teams to build pipelines, services, and tools that enable both UA teammates and our integrated systems with the data, information, and knowledge to fulfill Under Armourâ€™s mission to make all athletes better.\nâ€¢ *Your Impact**\n\n+ Work across teams to build pipelines, services, and tools that enable both UA teammates and our integrated systems with the data, information, and knowledge to fulfill Under Armourâ€™s mission to make all athletes better.\n\n+ Design and build data products and data flows for the continued expansion of the data platform mission; deployed code is performant, well-styled, validated, and documented.\n\n+ Implement data platform architectural designs/approaches.\n\n+ Instrument and monitor the systems and products of the data platform.\n\n+ Design, build, integrate, and maintain data ingestion pipelines from 3rd Party source systems and or data providers\n\n+ Lead one or two medium or large projects at a time.\n\n+ Design and integrate data replication solutions from various UA enterprise or other back end services to the Data Platform.\n\n+ Design, build, integrate, and maintain data ingestion pipelines from offline sources, to include various 3rd Party services, to the Data Platform.\n\n+ Support data catalog initiatives and adoption across the Data Platform user community.\n\n+ Support data cleansing and data quality initiatives across the Data Platform for its foundational data.\n\n+ Support identity resolution initiatives across the Data Platform.\n\n+ Other data engineering duties as assigned.\nâ€¢ *Qualifications**\n\n+ Bachelor\'s degree in Computer Science, Information Systems, or in closely related technical field followed by 5 years of progressively responsible data and/or software engineering experience OR masterâ€™s degree in Computer Science, Information Systems, or in closely related technical field and 3 years of progressively responsible data and/or software engineering experience.\n\nExperience MUST include:\n\n2 years of experience with each of the following:\n\n+ Data engineering fundamentals (design patterns, common practices)\n\n+ Building high volume data products and services\n\n+ Security and privacy \'by designâ€™ frameworks\n\n+ Data science lifecycle\n\n+ Programming in SQL\n\n+ Programming in Python\n\n+ Snowflake\n\nDemonstrated knowledge* of the following:\n\n+ Job Orchestration Tools such as Airflow or similar tools\n\n+ AWS data-related products such as EMR, Glue, S3, and/or Lambda\nâ€¢ *Workplace Location**\n\n+ **Location:** 101 Performance Drive, Baltimore, MD 21230\n\n+ **Work Schedule:** 100% remote; may be performed from anywhere in the US.\nâ€¢ *Relocation**\n\n+ No relocation provided\nâ€¢ *\\#LI-DNP**\nâ€¢ *\\#LI-DNI**\nâ€¢ *Base Compensation**\n\nSALARY RANGE: $148,845 to $188,089/year\nâ€¢ *Benefits & Perks**\n\n+ Paid "UA Give Back" Volunteer Days: Work alongside your team to support initiatives in your local community\n\n+ Under Armour Merchandise Discounts\n\n+ Competitive 401(k) plan matching\n\n+ Maternity and Parental Leave for eligible and FMLA-eligible teammates\n\n+ Health & fitness benefits, discounts and resources- We offer teammates across the country programs to promote physical activity and overall well-being\nâ€¢ *Our Commitment to Equal Opportunity**\n\nAt Under Armour, we are committed to providing an environment of mutual respect where equal employment opportunities are available to all applicants and teammates without regard to race, color, religion or belief, sex, pregnancy (including childbirth, lactation and related medical conditions), national origin, age, physical and mental disability, marital status, sexual orientation, gender identity, gender expression, genetic information (including characteristics and testing), military and veteran status, family or paternal status and any other characteristic protected by applicable law. Under Armour seeks to recruit, develop and retain the most talented people representing a wide variety of backgrounds and perspectives. If a reasonable accommodation is needed to participate in the job application or interview process, please contact our Human Resources team via candidateaccommodations@underarmour.com.\n\nRequisition ID: 162989\n\nLocation:\n\nRemote, US\n\nBusiness Unit: Corporate\n\nRegion: North America\n\nEmployee Class: Full Time\n\nEmployment Type: Salaried\n\nLearn more about our Benefits here\n\nSr. Professional, Data Engineering\nâ€¢ *Sr. Professional, Data Engineering**\nâ€¢ *Values & Innovation**\n\nAt Under Armour, we are committed to empowering those who strive for more, and the company\'s values - Act Sustainably, Celebrate the Wins, Fight on Together, Love Athletes and Stand for Equality - serve as both a roadmap for our teams and the qualities expected of every teammate.\n\nOur Values define and unite us, the beliefs that are the red thread that connects everyone at Under Armour. Our values are rallying cries, reminding us why we\'re here, and fueling everything we do.\n\nOur pursuit of better begins with innovation and with our team\'s mission of being the best. With us, you get the freedom to go further - no matter your role. That means developing, delivering, and selling the state-of-the-art products and digital tools that make top performers even better.\n\nIf you are a current Under Armour teammate, apply to this position on theInternal Career Site Here. (https://performancemanager8.successfactors.com/sf/careers/jobsearch?bplte\\_company=ua&\\_s.crb=aNMP8gWoYkBDFn%252bz2BldysgcgQHZpVs6tHzE9smSuXE%253d)\nâ€¢ *Purpose of Role**\n\nAt Under Armour, weâ€™re building the data products and services to power the future of our direct to consumer experience as well as other core aspects of our performance brand.\n\nPerforming alongside data analytics, data science, and digital marketing teams and across channels such as MapMyFitness and UA.com, weâ€™d like you to join our cross-cutting Data Platform team in building real solutions to centralize, enrich, and activate data across the enterprise. You will learn, grow, and play in an environment that focuses on results and delivery, all backed by one of the strongest consumer brands in history.\n\nWe\'re looking to add a new Sr. Data Platform Engineer to join our Enterprise Data Management team, which powers the storage, processing, integration and cataloging of our data. In this role, you will work across teams to build pipelines, services, and tools that enable both UA teammates and our integrated systems with the data, information, and knowledge to fulfill Under Armourâ€™s mission to make all athletes better.\nâ€¢ *Your Impact**\n\n+ Work across teams to build pipelines, services, and tools that enable both UA teammates and our integrated systems with the data, information, and knowledge to fulfill Under Armourâ€™s mission to make all athletes better.\n\n+ Design and build data products and data flows for the continued expansion of the data platform mission; deployed code is performant, well-styled, validated, and documented.\n\n+ Implement data platform architectural designs/approaches.\n\n+ Instrument and monitor the systems and products of the data platform.\n\n+ Design, build, integrate, and maintain data ingestion pipelines from 3rd Party source systems and or data providers\n\n+ Lead one or two medium or large projects at a time.\n\n+ Design and integrate data replication solutions from various UA enterprise or other back end services to the Data Platform.\n\n+ Design, build, integrate, and maintain data ingestion pipelines from offline sources, to include various 3rd Party services, to the Data Platform.\n\n+ Support data catalog initiatives and adoption across the Data Platform user community.\n\n+ Support data cleansing and data quality initiatives across the Data Platform for its foundational data.\n\n+ Support identity resolution initiatives across the Data Platform.\n\n+ Other data engineering duties as assigned.\nâ€¢ *Qualifications**\n\n+ Bachelor\'s degree in Computer Science, Information Systems, or in closely related technical field followed by 5 years of progressively responsible data and/or software engineering experience OR masterâ€™s degree in Computer Science, Information Systems, or in closely related technical field and 3 years of progressively responsible data and/or software engineering experience.\n\nExperience MUST include:\n\n2 years of experience with each of the following:\n\n+ Data engineering fundamentals (design patterns, common practices)\n\n+ Building high volume data products and services\n\n+ Security and privacy \'by designâ€™ frameworks\n\n+ Data science lifecycle\n\n+ Programming in SQL\n\n+ Programming in Python\n\n+ Snowflake\n\nDemonstrated knowledge* of the following:\n\n+ Job Orchestration Tools such as Airflow or similar tools\n\n+ AWS data-related products such as EMR, Glue, S3, and/or Lambda\nâ€¢ *Workplace Location**\n\n+ **Location:** 101 Performance Drive, Baltimore, MD 21230\n\n+ **Work Schedule:** 100% remote; may be performed from anywhere in the US.\nâ€¢ *Relocation**\n\n+ No relocation provided\nâ€¢ *\\#LI-DNP**\nâ€¢ *\\#LI-DNI**\nâ€¢ *Base Compensation**\n\nSALARY RANGE: $148,845 to $188,089/year\nâ€¢ *Benefits & Perks**\n\n+ Paid "UA Give Back" Volunteer Days: Work alongside your team to support initiatives in your local community\n\n+ Under Armour Merchandise Discounts\n\n+ Competitive 401(k) plan matching\n\n+ Maternity and Parental Leave for eligible and FMLA-eligible teammates\n\n+ Health & fitness benefits, discounts and resources- We offer teammates across the country programs to promote physical activity and overall well-being\nâ€¢ *Our Commitment to Equal Opportunity**\n\nAt Under Armour, we are committed to providing an environment of mutual respect where equal employment opportunities are available to all applicants and teammates without regard to race, color, religion or belief, sex, pregnancy (including childbirth, lactation and related medical conditions), national origin, age, physical and mental disability, marital status, sexual orientation, gender identity, gender expression, genetic information (including characteristics and testing), military and veteran status, family or paternal status and any other characteristic protected by applicable law. Under Armour seeks to recruit, develop and retain the most talented people representing a wide variety of backgrounds and perspectives. If a reasonable accommodation is needed to participate in the job application or interview process, please contact our Human Resources team via candidateaccommodations@underarmour.com.\n\nRequisition ID: 162989\n\nLocation:\n\nRemote, US\n\nBusiness Unit: Corporate\n\nRegion: North America\n\nEmployee Class: Full Time\n\nEmployment Type: Salaried\n\nLearn more about our Benefits here', 'job_highlights': [{'title': 'Qualifications', 'items': ["Bachelor's degree in Computer Science, Information Systems, or in closely related technical field followed by 5 years of progressively responsible data and/or software engineering experience OR masterâ€™s degree in Computer Science, Information Systems, or in closely related technical field and 3 years of progressively responsible data and/or software engineering experience", '2 years of experience with each of the following:', 'Data engineering fundamentals (design patterns, common practices)', 'Building high volume data products and services', "Security and privacy 'by designâ€™ frameworks", 'Data science lifecycle', 'Programming in SQL', 'Programming in Python', 'Snowflake', 'Job Orchestration Tools such as Airflow or similar tools', 'AWS data-related products such as EMR, Glue, S3, and/or Lambda', '100% remote; may be performed from anywhere in the US', "Bachelor's degree in Computer Science, Information Systems, or in closely related technical field followed by 5 years of progressively responsible data and/or software engineering experience OR masterâ€™s degree in Computer Science, Information Systems, or in closely related technical field and 3 years of progressively responsible data and/or software engineering experience", '2 years of experience with each of the following:', 'Data engineering fundamentals (design patterns, common practices)', 'Building high volume data products and services', "Security and privacy 'by designâ€™ frameworks", 'Data science lifecycle', 'Programming in SQL', 'Programming in Python', 'Snowflake', 'Job Orchestration Tools such as Airflow or similar tools', 'AWS data-related products such as EMR, Glue, S3, and/or Lambda', '100% remote; may be performed from anywhere in the US']}, {'title': 'Benefits', 'items': ['SALARY RANGE: $148,845 to $188,089/year', '*Benefits & Perks**', 'Paid "UA Give Back" Volunteer Days: Work alongside your team to support initiatives in your local community', 'Under Armour Merchandise Discounts', 'Competitive 401(k) plan matching', 'Maternity and Parental Leave for eligible and FMLA-eligible teammates', 'Health & fitness benefits, discounts and resources- We offer teammates across the country programs to promote physical activity and overall well-being', '*Our Commitment to Equal Opportunity**', 'Employment Type: Salaried', 'SALARY RANGE: $148,845 to $188,089/year', '*Benefits & Perks**', 'Paid "UA Give Back" Volunteer Days: Work alongside your team to support initiatives in your local community', 'Under Armour Merchandise Discounts', 'Competitive 401(k) plan matching', 'Maternity and Parental Leave for eligible and FMLA-eligible teammates', 'Health & fitness benefits, discounts and resources- We offer teammates across the country programs to promote physical activity and overall well-being', '*Our Commitment to Equal Opportunity**']}, {'title': 'Responsibilities', 'items': ['Data Platform Engineer to join our Enterprise Data Management team, which powers the storage, processing, integration and cataloging of our data', 'In this role, you will work across teams to build pipelines, services, and tools that enable both UA teammates and our integrated systems with the data, information, and knowledge to fulfill Under Armourâ€™s mission to make all athletes better', 'Work across teams to build pipelines, services, and tools that enable both UA teammates and our integrated systems with the data, information, and knowledge to fulfill Under Armourâ€™s mission to make all athletes better', 'Design and build data products and data flows for the continued expansion of the data platform mission; deployed code is performant, well-styled, validated, and documented', 'Implement data platform architectural designs/approaches', 'Instrument and monitor the systems and products of the data platform', 'Design, build, integrate, and maintain data ingestion pipelines from 3rd Party source systems and or data providers', 'Lead one or two medium or large projects at a time', 'Design and integrate data replication solutions from various UA enterprise or other back end services to the Data Platform', 'Design, build, integrate, and maintain data ingestion pipelines from offline sources, to include various 3rd Party services, to the Data Platform', 'Support data catalog initiatives and adoption across the Data Platform user community', 'Support data cleansing and data quality initiatives across the Data Platform for its foundational data', 'Support identity resolution initiatives across the Data Platform', 'Other data engineering duties as assigned', 'Data Platform Engineer to join our Enterprise Data Management team, which powers the storage, processing, integration and cataloging of our data', 'In this role, you will work across teams to build pipelines, services, and tools that enable both UA teammates and our integrated systems with the data, information, and knowledge to fulfill Under Armourâ€™s mission to make all athletes better', 'Work across teams to build pipelines, services, and tools that enable both UA teammates and our integrated systems with the data, information, and knowledge to fulfill Under Armourâ€™s mission to make all athletes better', 'Design and build data products and data flows for the continued expansion of the data platform mission; deployed code is performant, well-styled, validated, and documented', 'Implement data platform architectural designs/approaches', 'Instrument and monitor the systems and products of the data platform', 'Design, build, integrate, and maintain data ingestion pipelines from 3rd Party source systems and or data providers', 'Lead one or two medium or large projects at a time', 'Design and integrate data replication solutions from various UA enterprise or other back end services to the Data Platform', 'Design, build, integrate, and maintain data ingestion pipelines from offline sources, to include various 3rd Party services, to the Data Platform', 'Support data catalog initiatives and adoption across the Data Platform user community', 'Support data cleansing and data quality initiatives across the Data Platform for its foundational data', 'Support identity resolution initiatives across the Data Platform', 'Other data engineering duties as assigned']}], 'apply_options': [{'title': 'Snagajob', 'link': 'https://www.snagajob.com/jobs/1165223469?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Remote Jobs USA - Remotenow.mysmartprosnetwfh', 'link': 'https://remotenow.mysmartpros.com/job/1011055?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Towards AI Jobs', 'link': 'https://jobs.towardsai.net/job/brillio-sr-lead-data-engineer-r01539426-cksz?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Remote Jobs USA', 'link': 'https://whimsicalzap.com/job/973577?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Remote Jobs USA', 'link': 'https://miralshift.net/job/956546?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Swiftnook.net', 'link': 'https://swiftnook.net/job/739824?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Todayremotejobs', 'link': 'https://todayremotejobs.xyz/job/entry-level-account-manager-31-1011782?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTci4gUHJvZmVzc2lvbmFsLCBEYXRhIEVuZ2luZWVyaW5nIC0gRnVsbC10aW1lIiwiY29tcGFueV9uYW1lIjoiVW5kZXIgQXJtb3VyLCBJbmMuIiwiYWRkcmVzc19jaXR5IjoiQXRsYW50YSwgR0EiLCJodGlkb2NpZCI6IklLd3hNdUduSGZLNWI3b3ZBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Senior Manager, Data Engineering', 'company_name': 'Capital One', 'location': 'Wilmington, DE', 'via': 'JobzMall', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=vhqqUi7iBA2tuM4zAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXOsQrCQAyAYVz7CIKQWbQnggg6ahEEcXBwLGkJ18g1OS4Z-iK-r3X51--vvovq-CJhLfBAwUhlA1d0hEYiC1FhibCFu3ZghKUfQAVuqjHR8jy4ZzuFYJbqaI7Ofd3rGFSo0yl8tLN_WhuwUE7o1O4Pu6nOEterC2Z2TPAUAhZ4cxpny1XmgeYHAcZYSZgAAAA&shmds=v1_AdeF8KjaLeN5Wy0TcoFumO7mL4URpUKrBNwL3J64n5BnNPMw1g&shem=damc&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=vhqqUi7iBA2tuM4zAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965c29b762d608ab12cd1/images/816525ee76494dfc05f1d4d266b5b1aba440d8a57f02ec71af2a5c90e0f86aea.png', 'extensions': ['140Kâ€“170K a year', 'Full-time', 'No degree mentioned'], 'detected_extensions': {'salary': '140Kâ€“170K a year', 'schedule_type': 'Full-time', 'qualifications': 'No degree mentioned'}, 'description': 'Are you passionate about leveraging data to drive business decisions and strategies? Do you thrive in a fast-paced, innovative environment? If so, Capital One has an exciting opportunity for you to join our team as a Senior Manager of Data Engineering. As a leader in our data engineering team, you will play a critical role in driving the development and implementation of data solutions that enable our company to make data-driven decisions. We are seeking a highly skilled and experienced individual with a strong background in data engineering and management, excellent leadership capabilities, and a passion for driving results. If you are ready to take on a challenging and rewarding role in a dynamic organization, we invite you to apply for this position today.\n\nLead the development and implementation of data solutions that support business decisions and strategies.\nCollaborate with cross-functional teams to understand business needs and translate them into data requirements.\nDesign, develop, and maintain data pipelines and data warehousing systems.\nEnsure data quality and accuracy through regular monitoring and troubleshooting.\nStay current with industry trends and advancements in data engineering, and make recommendations for new tools and technologies to improve processes.\nProvide technical guidance and mentorship to junior members of the data engineering team.\nManage and prioritize multiple projects and deadlines, while maintaining a high level of quality and attention to detail.\nDevelop and maintain strong relationships with key stakeholders, including business leaders and data scientists.\nCollaborate with data scientists to enable advanced analytics and machine learning capabilities.\nDevelop and monitor key performance indicators to measure the success and impact of data solutions.\nRegularly communicate updates and progress to senior leadership.\nEnsure compliance with data security and privacy regulations.\nContinuously evaluate and improve data processes and procedures.\nParticipate in recruiting, hiring, and training of new team members.\nFoster a culture of innovation, collaboration, and continuous learning within the data engineering team.\n\nCapital One is an Equal Opportunity Employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. We do not discriminate based upon race, religion, color, national origin, sex, sexual orientation, gender identity, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.', 'job_highlights': [{'title': 'Qualifications', 'items': ['We are seeking a highly skilled and experienced individual with a strong background in data engineering and management, excellent leadership capabilities, and a passion for driving results']}, {'title': 'Responsibilities', 'items': ['Lead the development and implementation of data solutions that support business decisions and strategies', 'Collaborate with cross-functional teams to understand business needs and translate them into data requirements', 'Design, develop, and maintain data pipelines and data warehousing systems', 'Ensure data quality and accuracy through regular monitoring and troubleshooting', 'Stay current with industry trends and advancements in data engineering, and make recommendations for new tools and technologies to improve processes', 'Provide technical guidance and mentorship to junior members of the data engineering team', 'Manage and prioritize multiple projects and deadlines, while maintaining a high level of quality and attention to detail', 'Develop and maintain strong relationships with key stakeholders, including business leaders and data scientists', 'Collaborate with data scientists to enable advanced analytics and machine learning capabilities', 'Develop and monitor key performance indicators to measure the success and impact of data solutions', 'Regularly communicate updates and progress to senior leadership', 'Ensure compliance with data security and privacy regulations', 'Continuously evaluate and improve data processes and procedures', 'Participate in recruiting, hiring, and training of new team members']}], 'apply_options': [{'title': 'JobzMall', 'link': 'https://www.jobzmall.com/capital-one/job/senior-manager-data-engineering-s1839tgp39?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTZW5pb3IgTWFuYWdlciwgRGF0YSBFbmdpbmVlcmluZyIsImNvbXBhbnlfbmFtZSI6IkNhcGl0YWwgT25lIiwiYWRkcmVzc19jaXR5IjoiV2lsbWluZ3RvbiwgREUiLCJodGlkb2NpZCI6InZocXFVaTdpQkEydHVNNHpBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Lead Data Engineer', 'company_name': 'Keter Environmental Services', 'location': 'Stamford, CT', 'via': 'Indeed', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=NojnMAhcQnj1xCYkAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMsQrCQAwAUFz7CU5ZldoTwUVHLYK66V7Sa7yeXJNyCaVf4veqyxtf8VkUqzthB2c0hJpDZKIMG7hKC0qYfQ_CcBEJiZbH3mzUg3OqqQpqaNFXXgYnTK3M7i2t_mm0x0xjQqNmt9_O1chhXd7IfnXNU8zCA7FhggflKXpSiAwPw-EluSvh9PwC_G2H35kAAAA&shmds=v1_AdeF8Kjy0nIYSHlJxCX14XLocy7vN4V6M-l8nuGLJjqMezAK_g&shem=damc&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=NojnMAhcQnj1xCYkAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965c29b762d608ab12cd1/images/816525ee76494dfc204de8dbcc3e5d1a0985b3ef196eed4f8a86c391f3b63c5b.jpeg', 'extensions': ['Full-time', 'Dental insurance', 'Paid time off', 'Health insurance'], 'detected_extensions': {'schedule_type': 'Full-time', 'dental_coverage': True, 'paid_time_off': True, 'health_insurance': True}, 'description': 'About Us\n\nOver the past 25 years, Waste Harmonics Keter has been at the forefront of the waste and recycling industry, delivering innovative, data-driven solutions. We help companies right-size their waste operations and get out of the waste business with industry-leading expertise, state-of-the-art waste technologies, and industry-leading customer service.\n\nVisit Waste Harmonics Keter for more information.\n\nRole Purpose\n\nThe Data Engineer designs, builds, and optimizes scalable, reusable, and performance-oriented data infrastructure that supports enterprise-wide reporting and analytics needs. The role aligns with modern data platform architecture and engineering best practices to ensure long-term maintainability, flexibility, and data trust.\n\nKey Responsibilities\nâ€¢ Design, build, and maintain data pipelines to support reporting, analytics, and business intelligence.\nâ€¢ Develop and optimize scalable ETL/ELT processes using modern frameworks (e.g., dbt, Azure Data Factory, Fabric).\nâ€¢ Implement data models that support reporting and analytics needs (e.g., star schema, slowly changing dimensions).\nâ€¢ Ensure data quality, lineage, and observability for reliable business use.\nâ€¢ Collaborate with cross-functional teams to deliver integrated data solutions across the enterprise.\nâ€¢ Troubleshoot and optimize pipeline performance and SQL queries.\nâ€¢ Deliver documentation for technical workflows, transformations, and logic.Support governance by applying security, access control, and compliance standards in cloud environments.\n\nCore Competencies & Behaviors\nâ€¢ Technical Excellence: Strong understanding of data architecture, modelling, and transformation flows.\nâ€¢ Problem Solving: Able to troubleshoot complex performance issues and propose efficient solutions.\nâ€¢ Collaboration: Works effectively with analysts, engineers, and business teams to deliver end-to-end solutions.\nâ€¢ Continuous Improvement: Applies CI/CD, version control, and best practices to improve workflow efficiency.\nâ€¢ Detail Orientation: Ensures data accuracy, completeness, and consistency across systems.\nâ€¢ Adaptability: Thrives in a modern, cloud-based data environment and adapts to evolving technologies.\n\nExperience & Knowledge\nâ€¢ Experience building and maintaining cloud-based data pipelines (e.g., Azure, Snowflake, Fabric).\nâ€¢ Hands-on use of orchestration tools and ETL/ELT frameworks (dbt, ADF).\nâ€¢ Strong knowledge of data modelling principles and data architecture concepts.\nâ€¢ Experience with CI/CD pipelines, version control (e.g., Git), and modern data stack practices.\nâ€¢ Familiarity with monitoring and observability tools for pipelines.Understanding of security and access controls in cloud data platforms.\n\nQualifications\nâ€¢ Bachelorâ€™s degree in Computer Science, Data Engineering, Information Systems, or related field (or equivalent experience).\nâ€¢ Proficiency in SQL and modern data stack tools (dbt, Snowflake, Azure Data Factory, Fabric).Strong technical documentation and communication skills.\n\nWaste Harmonics Keter Comprehensive Benefits Package\nâ€¢ Competitive Compensation\nâ€¢ Annual Bonus Plan at Every Level\nâ€¢ Continuous Learning and Development Opportunities\nâ€¢ 401(k) Retirement Savings with Company Match; Immediate Vesting\nâ€¢ Medical & Dental Insurance\nâ€¢ Vision Insurance (Company Paid)\nâ€¢ Life Insurance (Company Paid)\nâ€¢ Short-term & Long-term Disability (Company paid)\nâ€¢ Employee Assistance Program\nâ€¢ Flexible Spending Accounts/Health Savings Accounts\nâ€¢ Paid Time Off (PTO), Including birthday off, community volunteer hours and a Friday off in the summer7 Paid Holidays\n\nAt Waste Harmonics Keter , we celebrate diversity and are committed to creating an inclusive environment for all employees. We welcome candidates from all backgrounds to apply.', 'job_highlights': [{'title': 'Qualifications', 'items': ['Technical Excellence: Strong understanding of data architecture, modelling, and transformation flows', 'Adaptability: Thrives in a modern, cloud-based data environment and adapts to evolving technologies', 'Experience building and maintaining cloud-based data pipelines (e.g., Azure, Snowflake, Fabric)', 'Hands-on use of orchestration tools and ETL/ELT frameworks (dbt, ADF)', 'Strong knowledge of data modelling principles and data architecture concepts', 'Experience with CI/CD pipelines, version control (e.g., Git), and modern data stack practices', 'Familiarity with monitoring and observability tools for pipelines', 'Bachelorâ€™s degree in Computer Science, Data Engineering, Information Systems, or related field (or equivalent experience)', 'Proficiency in SQL and modern data stack tools (dbt, Snowflake, Azure Data Factory, Fabric)', 'Strong technical documentation and communication skills']}, {'title': 'Benefits', 'items': ['Waste Harmonics Keter Comprehensive Benefits Package', 'Competitive Compensation', 'Annual Bonus Plan at Every Level', 'Continuous Learning and Development Opportunities', '401(k) Retirement Savings with Company Match; Immediate Vesting', 'Medical & Dental Insurance', 'Vision Insurance (Company Paid)', 'Life Insurance (Company Paid)', 'Short-term & Long-term Disability (Company paid)', 'Employee Assistance Program', 'Flexible Spending Accounts/Health Savings Accounts', 'Paid Time Off (PTO), Including birthday off, community volunteer hours and a Friday off in the summer7 Paid Holidays']}, {'title': 'Responsibilities', 'items': ['The Data Engineer designs, builds, and optimizes scalable, reusable, and performance-oriented data infrastructure that supports enterprise-wide reporting and analytics needs', 'The role aligns with modern data platform architecture and engineering best practices to ensure long-term maintainability, flexibility, and data trust', 'Design, build, and maintain data pipelines to support reporting, analytics, and business intelligence', 'Develop and optimize scalable ETL/ELT processes using modern frameworks (e.g., dbt, Azure Data Factory, Fabric)', 'Implement data models that support reporting and analytics needs (e.g., star schema, slowly changing dimensions)', 'Ensure data quality, lineage, and observability for reliable business use', 'Collaborate with cross-functional teams to deliver integrated data solutions across the enterprise', 'Troubleshoot and optimize pipeline performance and SQL queries', 'Deliver documentation for technical workflows, transformations, and logic', 'Support governance by applying security, access control, and compliance standards in cloud environments', 'Problem Solving: Able to troubleshoot complex performance issues and propose efficient solutions', 'Collaboration: Works effectively with analysts, engineers, and business teams to deliver end-to-end solutions', 'Continuous Improvement: Applies CI/CD, version control, and best practices to improve workflow efficiency', 'Detail Orientation: Ensures data accuracy, completeness, and consistency across systems', 'Understanding of security and access controls in cloud data platforms']}], 'apply_options': [{'title': 'Indeed', 'link': 'https://www.indeed.com/viewjob?jk=20c3cc5145eeb8ca&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Glassdoor', 'link': 'https://www.glassdoor.com/job-listing/lead-data-engineer-keter-environmental-services-JV_IC1148355_KO0,18_KE19,47.htm?jl=1009883942888&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'LinkedIn', 'link': 'https://www.linkedin.com/jobs/view/lead-data-engineer-at-waste-harmonics-keter-4303818968?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'SimplyHired', 'link': 'https://www.simplyhired.com/job/SF83spzu2nW2i_o9xLjCPCZ7kjX7OTnglRUkDbv-B1RKcEvIOo2ZaQ?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Teal', 'link': 'https://www.tealhq.com/job/lead-data-engineer_19e24fbc-86ba-40ed-a74d-6ee4a34b8a81?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Ladders', 'link': 'https://www.theladders.com/job/lead-data-engineer-keter-environmental-services-stamford-ct_83520913?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Adzuna', 'link': 'https://www.adzuna.com/details/5413779991?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'BeBee', 'link': 'https://us.bebee.com/job/b7f7d78808c699d96eb353b3ce51430d?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJMZWFkIERhdGEgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJLZXRlciBFbnZpcm9ubWVudGFsIFNlcnZpY2VzIiwiYWRkcmVzc19jaXR5IjoiU3RhbWZvcmQsIENUIiwiaHRpZG9jaWQiOiJOb2puTUFoY1FuajF4Q1lrQUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': 'Senior Specialist - Data Engineering', 'company_name': 'LTIMindtree', 'location': 'New York, NY', 'via': 'SimplyHired', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=3V4lsG80G4ftzpl9AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXIsQrCQAwAUFz7CbpkFu2J4GJXRRTtokuncj3DNVqT4xKwf-LvqssbXvGZFNUVmSTDNWEgP5AaLGHnzcOeIzFiJo6_OkkHij6HHoThIBIHnFa9WdKtc6pDGdW8USiDvJwwdjK6h3T6p9XeZ0yDN2zXm9VYJo7z2fl2vBDfLSMCMdT4hkbycwF18wWNxpSXmgAAAA&shmds=v1_AdeF8KicFqylhuusNG_0_DuOEcPWBLm8bkLlR19MPRc4z9s0Mg&shem=damc&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=3V4lsG80G4ftzpl9AAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965c29b762d608ab12cd1/images/816525ee76494dfc9e9ff846a6a757144140c7b73d027833978153af63e89897.jpeg', 'extensions': ['5 days ago', 'Full-time', 'No degree mentioned', 'Health insurance', 'Paid time off', 'Dental insurance'], 'detected_extensions': {'posted_at': '5 days ago', 'schedule_type': 'Full-time', 'qualifications': 'No degree mentioned', 'health_insurance': True, 'paid_time_off': True, 'dental_coverage': True}, 'description': 'Role description\n\nJob Summary\n\nSeeking a Senior Specialist with 7 to 11 years of experience in AWS Databricks to design develop and manage advanced data solutions using AWS S3 and Databricks platforms\n\nJob Description\n\nDesign and implement scalable and efficient data pipelines and ETL processes leveraging AWS S3 and Databricks Collaborate with crossfunctional teams to integrate data solutions that meet complex business requirements Optimize data storage and retrieval strategies on AWS S3 to enhance performance and reduce costs Develop and maintain Databricks notebooks and scripts to support data transformation analytics and reporting Monitor data workflows to ensure high data quality accuracy and reliability Stay current with new AWS and Databricks features tools and best practices to continuously improve data platform capabilities\n\nRoles and Responsibilities\n\nLead the design development and deployment of data processing solutions on AWS Databricks Manage and maintain AWS S3 storage buckets including implementing data lifecycle policies and security controls Collaborate closely with data engineers data scientists and business stakeholders to gather analyze and translate requirements into technical solutions Implement and enforce data governance compliance and security standards within the data platform Provide technical leadership mentorship and guidance to junior team members on AWS Databricks technologies and best practices Conduct performance tuning and cost optimization of data workloads to ensure efficient resource utilization Participate actively in code reviews documentation and knowledge sharing sessions to promote team excellence\n\nSkills\n\nMandatory Skills : RDS,Redshift,AWS EMR,AWS Glue,AWS S3,Databricks\n\nOther details\n\nBenefits/perks listed below may vary depending on the nature of your employment with LTIMindtree (â€œLTIMâ€):\n\nBenefits and Perks:\nâ€¢ Comprehensive Medical Plan Covering Medical, Dental, Vision\nâ€¢ Short Term and Long-Term Disability Coverage\nâ€¢ 401(k) Plan with Company match\nâ€¢ Life Insurance\nâ€¢ Vacation Time, Sick Leave, Paid Holidays\nâ€¢ Paid Paternity and Maternity Leave\n\nThe range displayed on each job posting reflects the minimum and maximum salary target for the position across all US locations. Within the range, individual pay is determined by work location and job level and additional factors including job-related skills, experience, and relevant education or training. Depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance-based bonus, sales incentive pay and other forms of bonus or variable compensation.\n\nDisclaimer: The compensation and benefits information provided herein is accurate as of the date of this posting.\n\nLTIMindtree is an equal opportunity employer that is committed to diversity in the workplace. Our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law.\n\nBenefits\n\nCompensation range: $100,000.00 to $120,000.00 per year\n\nAbout LTIMindtree\n\nLTIMindtree is a global technology consulting and digital solutions company that enables enterprises across industries to reimagine business models, accelerate innovation, and maximize growth by harnessing digital technologies. As a digital transformation partner to more than 700 clients, LTIMindtree brings extensive domain and technology expertise to help drive superior competitive differentiation, customer experiences, and business outcomes in a converging world. Powered by 83,000+ talented and entrepreneurial professionals across more than 40 countries, LTIMindtree â€” a Larsen & Toubro Group company â€” solves the most complex business challenges and delivers transformation at scale. For more information, please visit https://www.ltimindtree.com/.', 'apply_options': [{'title': 'SimplyHired', 'link': 'https://www.simplyhired.com/job/j3sIxh6obpj7QrgXEh96yjWWU-amZg_KX_j3_OUfG872CuaOhPFTIA?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'LinkedIn', 'link': 'https://www.linkedin.com/jobs/view/senior-specialist-data-engineering-at-ltimindtree-4339975053?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Indeed', 'link': 'https://www.indeed.com/viewjob?jk=a47f075798b5ffc0&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Glassdoor', 'link': 'https://www.glassdoor.com/job-listing/senior-specialist-data-engineering-ltimindtree-JV_IC1132348_KO0,34_KE35,46.htm?jl=1009936537526&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Ladders', 'link': 'https://www.theladders.com/job/senior-specialist-data-engineering-ltimindtree-new-york-ny_84389854?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'IHireAdmin', 'link': 'https://www.ihireadmin.com/jobs/view/500009332?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTZW5pb3IgU3BlY2lhbGlzdCAtIERhdGEgRW5naW5lZXJpbmciLCJjb21wYW55X25hbWUiOiJMVElNaW5kdHJlZSIsImFkZHJlc3NfY2l0eSI6Ik5ldyBZb3JrLCBOWSIsImh0aWRvY2lkIjoiM1Y0bHNHODBHNGZ0enBsOUFBQUFBQT09IiwiaGwiOiJlbiJ9'}, {'title': 'Insight Global', 'company_name': 'Insight Global', 'location': 'Atlanta, GA', 'via': 'Insight Global Jobs', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=F_L8tQ-AwRJT9wUVAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_13NsQrCMBAAUFz7CULhFhfRRAQXnToF_YlyCSGJnHehd0N3f7y4urz1Dd_dcHiytlINAklEgjO8JIJmXFIFYQgihfL-Uc263r1XJVfU0FpyST5eOEdZ_Vui_pi14pI7oeX5erusrnM5jn9HY5iMkA1PEKYNK0BxgIYAAAA&shmds=v1_AdeF8Kj2SuaioxlEd2k1A_5yEhjUAz-eYBWjfoesu5-4rcP_zg&shem=damc&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=F_L8tQ-AwRJT9wUVAAAAAA%3D%3D', 'extensions': ['Contractor', 'No degree mentioned'], 'detected_extensions': {'schedule_type': 'Contractor', 'qualifications': 'No degree mentioned'}, 'description': 'Insight Global is looking for an Intermediate Data Engineer to join a large marketing and analytics organization on a 6-month contract to start. As a member of the Data Hub team, you will be responsible for optimizing and expanding their data and data pipeline architecture in addition to optimizing data flows and collection for cross-functional teams. You will be responsible for recommending the best options for data transformation and product objectives using your knowledge of trends, best practices, and leading-edge techniques. You will expand and develop their cloud analytical platform that enables business users, data analysts, and data scientists to make data-driven decisions, build innovative data products, and roll out advanced analytics.', 'apply_options': [{'title': 'Insight Global Jobs', 'link': 'https://jobs.insightglobal.com/find_a_job/ontario/job-292660/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJJbnNpZ2h0IEdsb2JhbCIsImNvbXBhbnlfbmFtZSI6Ikluc2lnaHQgR2xvYmFsIiwiYWRkcmVzc19jaXR5IjoiQXRsYW50YSwgR0EiLCJodGlkb2NpZCI6IkZfTDh0US1Bd1JKVDl3VVZBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Data Engineer', 'company_name': 'Confidential', 'location': 'West Haven, CT', 'via': 'JobTarget', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=-BW5tXdT6tddx5xSAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCMBAAUFz7CQ5yg5NoIoKLHasozoJjucQzjcS70Dukuz8uvuE131mzPKIhnDhlJhphA1cJoIRjHEAYziKp0LwdzKoevFctLqmh5eiivL0wBZn8S4L-63XAkWpBo363306uclotOuFnfhBbxgKZ4U5qcMEP8Rq62w8cK3UmhgAAAA&shmds=v1_AdeF8Kj77nzyEzTOZFh2GKCz8HNx51DivG1Rokc84FiAXUZe4A&shem=damc&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=-BW5tXdT6tddx5xSAAAAAA%3D%3D', 'extensions': ['Full-time'], 'detected_extensions': {'schedule_type': 'Full-time'}, 'description': 'Introduction\n\nWe are seeking a highly skilled and motivated Data Engineer to join our dynamic team. As a Data Engineer, you will play a crucial role in building and maintaining robust data pipelines, ensuring the efficient processing and storage of data. Your expertise will help drive data-driven decision-making across the organization, enhancing our ability to deliver innovative solutions to our clients.\n\nJob Responsibilities\nâ€¢ Design, develop, and maintain scalable data pipelines to process large volumes of data efficiently.\nâ€¢ Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.\nâ€¢ Implement data integration processes, ensuring data is collected, transformed, and stored accurately and securely.\nâ€¢ Optimize and troubleshoot existing data systems and infrastructure to ensure high performance and reliability.\nâ€¢ Work with cutting-edge technologies to support data storage, processing, and analysis needs.\nâ€¢ Ensure data quality and integrity by implementing data validation and cleansing techniques.\nâ€¢ Stay up-to-date with industry trends and emerging technologies in data engineering and analytics.\nâ€¢ Document data processes and systems, providing clear and comprehensive technical documentation.', 'job_highlights': [{'title': 'Qualifications', 'items': ['We are seeking a highly skilled and motivated Data Engineer to join our dynamic team']}, {'title': 'Responsibilities', 'items': ['As a Data Engineer, you will play a crucial role in building and maintaining robust data pipelines, ensuring the efficient processing and storage of data', 'Your expertise will help drive data-driven decision-making across the organization, enhancing our ability to deliver innovative solutions to our clients', 'Design, develop, and maintain scalable data pipelines to process large volumes of data efficiently', 'Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions', 'Implement data integration processes, ensuring data is collected, transformed, and stored accurately and securely', 'Optimize and troubleshoot existing data systems and infrastructure to ensure high performance and reliability', 'Work with cutting-edge technologies to support data storage, processing, and analysis needs', 'Ensure data quality and integrity by implementing data validation and cleansing techniques', 'Stay up-to-date with industry trends and emerging technologies in data engineering and analytics', 'Document data processes and systems, providing clear and comprehensive technical documentation']}], 'apply_options': [{'title': 'JobTarget', 'link': 'https://www.jobtarget.com/jobs/jt-t7o1ddviy3/data-engineer-west-haven-connecticut?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoiQ29uZmlkZW50aWFsIiwiYWRkcmVzc19jaXR5IjoiV2VzdCBIYXZlbiwgQ1QiLCJodGlkb2NpZCI6Ii1CVzV0WGRUNnRkZHg1eFNBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Lead Data Engineer (Java, Scala, Spark)', 'company_name': 'Capital One', 'location': 'Norfolk, VA', 'via': 'CareerBuilder', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=GqCeF0v3ETCHwnqIAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXFsQrCMBAAUFz7CZ1uVKmNCC4KgqgIRXQQXMs1nklszIXkkP6Kfytd3it-k2J3IXzCEQXhFIwLRAmmDX6xgrtGPxYx9TNYQMMdZMKkLXCAM7PxVG6tSMwbpXL2tcmC4nSt-aM4UMeDenOXR9psMVH0KNSu1suhjsHMywNGJ-jhFghcgCunF_u-gsf-D8Ud7LecAAAA&shmds=v1_AdeF8KjiAQoPxZZ_-7wYTrHEFpqisAfF28P2w3bRd4189K4Rrg&shem=damc&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=GqCeF0v3ETCHwnqIAAAAAA%3D%3D', 'extensions': ['6 days ago', 'Full-time', 'Health insurance'], 'detected_extensions': {'posted_at': '6 days ago', 'schedule_type': 'Full-time', 'health_insurance': True}, 'description': "Lead Data Engineer (Java, Scala, Spark)\n\nDo you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative,[Link available when viewing the job]inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Lead Data Engineer, youâ€™ll have the opportunity to be on the forefront of driving a major transformation within Capital One.\n\nWhat Youâ€™ll Do:\nâ€¢ Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies\nâ€¢ Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems\nâ€¢ Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake\nâ€¢ Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community\nâ€¢ Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment\nâ€¢ Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance\n\nBasic Qualifications:\nâ€¢ Bachelorâ€™s Degree\nâ€¢ At least 4 years of experience in application development (Internship experience does not apply)\nâ€¢ At least 2 years of experience in big data technologies\nâ€¢ At least 1 year experience with cloud computing (AWS, Microsoft Azure, Google Cloud)\n\nPreferred Qualifications:\nâ€¢ 7+ years of experience in application development including Java or Scala\nâ€¢ 4+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)\nâ€¢ 4+ years experience with Distributed data/computing tools (Kafka, Spark, or MySQL)\nâ€¢ 4+ year experience working on real-time data and streaming applications\nâ€¢ 4+ years of experience with NoSQL implementation (Mongo, Cassandra)\nâ€¢ 4+ years of data warehousing experience (Redshift or Snowflake)\nâ€¢ 4+ years of experience with UNIX/Linux including basic commands and shell scripting\nâ€¢ 2+ years of experience with Agile engineering practices\n\nAt this time, Capital One will not sponsor a new applicant for employment authorization, or offer any immigration related support for this position (i.e. H1B, F-1 OPT, F-1 STEM OPT, F-1 CPT, J-1, TN, E-2, E-3, L-1 and O-1, or any EADs or other forms of work authorization that require immigration support from an employer).\n\nThe minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.\n\nMcLean, VA: $193,400 - $220,700 for Lead Data Engineer\n\nRichmond, VA: $175,800 - $200,700 for Lead Data Engineer\n\nCandidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidateâ€™s offer letter.\n\nThis role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n\nCapital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the [Link available when viewing the job]. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.\n\nThis role is expected to accept applications for a minimum of 5 business days.\n\nNo agencies please. Capital One is an equal opportunity employer (EOE, including disability/vet) committed to non-discrimination in compliance with applicable federal, state, and local laws. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York Cityâ€™s Fair Chance Act; Philadelphiaâ€™s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n\nIf you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at [Phone number shown when applying] or via email at [Link available when viewing the job]. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.\n\nFor technical support or questions about Capital One's recruiting process, please send an email to [Link available when viewing the job]\n\nCapital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.\n\nCapital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).", 'job_highlights': [{'title': 'Qualifications', 'items': ['Bachelorâ€™s Degree', 'At least 4 years of experience in application development (Internship experience does not apply)', 'At least 2 years of experience in big data technologies', 'At least 1 year experience with cloud computing (AWS, Microsoft Azure, Google Cloud)', 'At this time, Capital One will not sponsor a new applicant for employment authorization, or offer any immigration related support for this position (i.e. H1B, F-1 OPT, F-1 STEM OPT, F-1 CPT, J-1, TN, E-2, E-3, L-1 and O-1, or any EADs or other forms of work authorization that require immigration support from an employer)']}, {'title': 'Benefits', 'items': ['Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked', 'McLean, VA: $193,400 - $220,700 for Lead Data Engineer', 'Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidateâ€™s offer letter', 'This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI)', 'Incentives could be discretionary or non discretionary depending on the plan', 'Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being']}, {'title': 'Responsibilities', 'items': ['Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies', 'Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems', 'Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake', 'Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community', 'Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment', 'Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance']}], 'apply_options': [{'title': 'CareerBuilder', 'link': 'https://www.careerbuilder.com/job/J3T4VM6ZKSC0Q2QHFZV?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'JobServe', 'link': 'https://www.jobserve.com/us/en/extjob/LEAD-DATA-ENGINEER-in-Norfolk-Virginia-USA-FDF537EA25245FA510/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobs For Stevenage Fans', 'link': 'https://jobs.stevenagefc.com/jobs/lead-data-engineer-for-enterprise-platforms-norfolk-virginia/2451758918-2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'WhatJobs', 'link': 'https://www.whatjobs.com/jobs/lead-data-engineer-java-scala-spark?id=2256813007&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'FOX4KC Jobs', 'link': 'https://jobs.fox4kc.com/jobs/lead-data-engineer-for-enterprise-platforms-norfolk-virginia/2451758918-2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'FOX16 Jobs', 'link': 'https://jobs.fox16.com/jobs/lead-data-engineer-for-enterprise-platforms-norfolk-virginia/2451758918-2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJMZWFkIERhdGEgRW5naW5lZXIgKEphdmEsIFNjYWxhLCBTcGFyaykiLCJjb21wYW55X25hbWUiOiJDYXBpdGFsIE9uZSIsImFkZHJlc3NfY2l0eSI6Ik5vcmZvbGssIFZBIiwiaHRpZG9jaWQiOiJHcUNlRjB2M0VUQ0h3bnFJQUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': 'Data Engineer (Analytics)', 'company_name': 'Nava', 'location': 'Remote, OR', 'via': 'Jobrapido', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=QIA9VEbdk5bda5hgAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKsQrCMBAAUFw7OjrdqKJNEVx0EhTBQaE_UC7lSCLpXckdUmd_XF3e9KrPrGrOaAgXDomJCixPjPltqdcVbOEmHpSw9BGE4SoSMi2O0WzUg3OquQ5q-Nt1L4MTJi-Te4rXP51GLDRmNOp2-2aqRw7r-R1fCImhpUGMNvBov8aT1sWGAAAA&shmds=v1_AdeF8KjOYyq4_V7khqIH_Ox9s0h_jFX25hEZWlI3YuvLNjww6Q&shem=damc&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=QIA9VEbdk5bda5hgAAAAAA%3D%3D', 'extensions': ['Full-time', 'No degree mentioned', 'Health insurance', 'Paid time off', 'Dental insurance'], 'detected_extensions': {'schedule_type': 'Full-time', 'qualifications': 'No degree mentioned', 'health_insurance': True, 'paid_time_off': True, 'dental_coverage': True}, 'description': "About Nava\n\nNava is a consultancy and public benefit corporation working to make government services simple and effective. Since 2013, federal, state, and local agencies have trusted Nava to help solve highly scrutinized technology modernization challenges.\n\nAs a client services company, we guide agencies constrained by legacy systems to a future with sharp user experiences built on secure, reliable, fault-tolerant cloud infrastructure. We bill for our time, offering our expertise and problem-solving approach to help our government partners enhance their digital products and services.\n\nPeople are at the heart of our work, from members of the public who rely on benefit programs to government agency staff. Through human-centered design and modern engineering best practices, we help our government partners better understand user needs and deliver on their missions more effectively. This focus gives everyone at Nava the opportunity to do work that is meaningful, impactful, and deeply connected to public good.\n\nYou will work on cross-functional teams to build scalable systems for our government -- designing, implementing, and delivering services that millions of Americans depend on. You care deeply about working on technology that improves peopleâ€™s lives, but donâ€™t think technology is always the answer. You are passionate about building large-scale systems that are inclusive, well-designed, fast, scalable, and secure, and you'll help set high standards for our teams in terms of simplicity, empathy, and accessibility.\n\nThe Data Engineer role will be responsible for developing end to end systems for one of Nava's major government partners. The objective is to modernize an existing legacy enterprise platform to improve its public facing processes, and experiences which will enable users to have more access to the programs and services that they need, in real-time, in a more user friendly and uncomplicated way. This individual will be involved in everything, from strategy, and planning through to post production testing, and support.\n\nWhat you'll do\nâ€¢ Collect data access patterns and review current data models to optimize designs for customer use cases\nâ€¢ Develop scalable data ingestion and processing pipelinesImplement large-scale data ecosystems within cloud-based platforms that include data management and data governance of structured and unstructured data\nâ€¢ Leverage and enhance automation to speed development and improve reliability and performance\nâ€¢ Work with cross-functional project teams to gather business requirements and translate to detailed technical specifications\nâ€¢ Work with Government partners to assist and develop data engineering applications and pipelines that will enable data services and processing capabilities\nâ€¢ Design, develop, test, automate, and deploy data engineering solutions in a cloud platforms, such as AWS\nâ€¢ Participate in software design and code reviews\nâ€¢ Develop automated testing, monitoring and alerting, and CI/CD for production systems\nâ€¢ Maintain security and privacy standards in all aspects of the data pipeline\n\nRequired skills\nâ€¢ At least 5 years data engineering experience\nâ€¢ At least 2 years of experience in cloud data architecture (AWS preferred) and big data technologies\nâ€¢ Experience with AWS services including AWS Glue, AWS\u202c Athena, AWS Redshift, AWS RDS\nâ€¢ Experience with Databricks or Spark\nâ€¢ Experience with building ETL/ELT pipelines in Python\nâ€¢ Proficient with relational databases and advanced SQL queries\nâ€¢ Experience with data cleaning and data modeling while protecting sensitive data\nâ€¢ Proficient with building data integrations using both API and file-based protocols\nâ€¢ Tenacity to dive into problems and iterate in working code\nâ€¢ Equal parts systems thinker and advocate for users\nâ€¢ Highly resourceful, reliable, and detail-oriented\nâ€¢ Ability to think strategically around trade-offs and short-term vs. long-term benefits\nâ€¢ An adaptive, empathetic, collaborative, and positive mindset\nâ€¢ Excellent written and real-time communication, technical and otherwise\n\n$135,900 - $153,000 a year\n\nOther requirements\n\nAll roles at Nava require the following:\n\nMust be legally authorized to work in the United States.\n\nMust meet any other requirements for government contracts for which candidates are hired.\n\nWork authorization that doesnâ€™t require visa sponsorship, now or in the future.\n\nMay be subject to a government background check or security clearance, depending on the contract.\n\nPerks working with Nava\n\nHealth coverage â€“ Comprehensive medical, dental, and vision plans to support your overall health needs.\n\nTime off â€“ Vacation, holidays (including Juneteenth), and floating days to rest and recharge.\n\nCompany holidays- Enjoy 12 paid federal holidays each year on top of your regular PTO.\n\nAnnual bonus â€“ When Nava meets goals, eligible employees receive a performance-based annual bonus.\n\nParental leave â€“ Paid time off for new parents, plus weekly meals delivered to your home.\n\nWellness program â€“ Full platform offering physical, mental, & emotional health resources & support tools.\n\nVirtual care â€“ See doctors online with no copay through UnitedHealthcareâ€™s virtual visit program.\n\nSabbatical leave â€“ Earn extended paid leave after continuous service for personal growth or rest.\n\n401(k) match â€“ Nava matches 4% of your salary to support your retirement savings plan.\n\nFlexible work â€“ Remote-first environment with flexibility built around your schedule and responsibilities.\n\nHome office setup â€“ Company laptop & setup assistance provided via Staples for remote work needs.\n\nUtility support â€“ Monthly reimbursement to help offset eligible home office utility expenses.\n\nLearning opportunities â€“ Internal training programs and resources to help grow your professional skills.\n\nDevelopment budget â€“ Annual allowance for courses, tuition, certifications, & LinkedIn Learning access.\n\nReferral bonus â€“ Get rewarded when you refer great people who join the Nava team.\n\nCommuter benefits â€“ Pre-tax commuter programs to support in-office travel when applicable.\n\nInsurance coverage â€“ Nava provides disability, life, and accidental death insurance at no cost.\n\nSupportive culture â€“ A collaborative and remote-friendly team environment where people genuinely care.\n\nLocation\n\nWe have fully remote options if you reside in one of the following states:\n\nAlabama, Arizona, California, Colorado, DC, Florida, Georgia, Illinois, Louisiana, Maine, Maryland, Massachusetts, Michigan, Minnesota, Missouri, Nevada, North Carolina, New Jersey, New York, Oklahoma, Oregon, Pennsylvania, Rhode Island, South Carolina, Texas, Tennessee, Utah, Virginia, Washington, Wisconsin.\nâ€¢ If you are not living in one of the states listed above, unfortunately, you will not be considered for a position at this time. Additionally, if employed with Nava, you must be based in the U.S. and cannot work from outside the country.\n\nStay in touch\n\nSign up for our newsletter to find out about career opportunities, new partnerships, and news from the broader civic tech community.\n\nPlease contact the recruiting team at recruiting@navapbc.com if you would like to request reasonable accommodation during the application or interviewing process.\n\nWe participate in E-Verify. Upon hire, we will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. This role requires you to work from the contiguous United States.", 'job_highlights': [{'title': 'Qualifications', 'items': ['At least 5 years data engineering experience', 'Experience with AWS services including AWS Glue, AWS\u202c Athena, AWS Redshift, AWS RDS', 'Experience with Databricks or Spark', 'Experience with building ETL/ELT pipelines in Python', 'Proficient with relational databases and advanced SQL queries', 'Experience with data cleaning and data modeling while protecting sensitive data', 'Proficient with building data integrations using both API and file-based protocols', 'Tenacity to dive into problems and iterate in working code', 'Equal parts systems thinker and advocate for users', 'Highly resourceful, reliable, and detail-oriented', 'Ability to think strategically around trade-offs and short-term vs. long-term benefits', 'An adaptive, empathetic, collaborative, and positive mindset', 'Excellent written and real-time communication, technical and otherwise', 'Must be legally authorized to work in the United States', 'Must meet any other requirements for government contracts for which candidates are hired', 'Work authorization that doesnâ€™t require visa sponsorship, now or in the future', 'May be subject to a government background check or security clearance, depending on the contract']}, {'title': 'Benefits', 'items': ['$135,900 - $153,000 a year', 'Health coverage â€“ Comprehensive medical, dental, and vision plans to support your overall health needs', 'Time off â€“ Vacation, holidays (including Juneteenth), and floating days to rest and recharge', 'Company holidays- Enjoy 12 paid federal holidays each year on top of your regular PTO', 'Annual bonus â€“ When Nava meets goals, eligible employees receive a performance-based annual bonus', 'Parental leave â€“ Paid time off for new parents, plus weekly meals delivered to your home', 'Wellness program â€“ Full platform offering physical, mental, & emotional health resources & support tools', 'Sabbatical leave â€“ Earn extended paid leave after continuous service for personal growth or rest', '401(k) match â€“ Nava matches 4% of your salary to support your retirement savings plan', 'Flexible work â€“ Remote-first environment with flexibility built around your schedule and responsibilities', 'Home office setup â€“ Company laptop & setup assistance provided via Staples for remote work needs', 'Utility support â€“ Monthly reimbursement to help offset eligible home office utility expenses', 'Learning opportunities â€“ Internal training programs and resources to help grow your professional skills', 'Development budget â€“ Annual allowance for courses, tuition, certifications, & LinkedIn Learning access', 'Referral bonus â€“ Get rewarded when you refer great people who join the Nava team', 'Commuter benefits â€“ Pre-tax commuter programs to support in-office travel when applicable', 'Insurance coverage â€“ Nava provides disability, life, and accidental death insurance at no cost']}, {'title': 'Responsibilities', 'items': ["The Data Engineer role will be responsible for developing end to end systems for one of Nava's major government partners", 'The objective is to modernize an existing legacy enterprise platform to improve its public facing processes, and experiences which will enable users to have more access to the programs and services that they need, in real-time, in a more user friendly and uncomplicated way', 'This individual will be involved in everything, from strategy, and planning through to post production testing, and support', 'Collect data access patterns and review current data models to optimize designs for customer use cases', 'Develop scalable data ingestion and processing pipelines', 'Implement large-scale data ecosystems within cloud-based platforms that include data management and data governance of structured and unstructured data', 'Leverage and enhance automation to speed development and improve reliability and performance', 'Work with cross-functional project teams to gather business requirements and translate to detailed technical specifications', 'Work with Government partners to assist and develop data engineering applications and pipelines that will enable data services and processing capabilities', 'Design, develop, test, automate, and deploy data engineering solutions in a cloud platforms, such as AWS', 'Participate in software design and code reviews', 'Develop automated testing, monitoring and alerting, and CI/CD for production systems', 'Maintain security and privacy standards in all aspects of the data pipeline']}], 'apply_options': [{'title': 'Jobrapido', 'link': 'https://us.jobrapido.com/jobpreview/2323008924048949248?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'BeBee', 'link': 'https://us.bebee.com/job/645a9528b644dc42804d9531cd74e07d?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Empleos', 'link': 'https://empleos.io/jobs/view/119356?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobs', 'link': 'https://vindrake.christmas/job/mobile-architect-miroboard-inc-johannesburg-south-africa-61575.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Chris Maxcer', 'link': 'http://chrismaxcer.com/workspread/job/consultant-data-engineer-at-the-spur-group-remote-336c67769fccc0bfjktk-Y2hyaXNtYX?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobs Trabajo.org', 'link': 'https://us.trabajo.org/job-2850-73b40784e2ab4cb65e2120228c1b6c35?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChBbmFseXRpY3MpIiwiY29tcGFueV9uYW1lIjoiTmF2YSIsImFkZHJlc3NfY2l0eSI6IlJlbW90ZSwgT1IiLCJodGlkb2NpZCI6IlFJQTlWRWJkazViZGE1aGdBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Data Engineer - Shockwave Medical', 'company_name': 'Johnson & Johnson MedTech', 'location': 'Santa Clara, CA', 'via': 'Jobs And Careers', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=MRbMxRhJvem6O8sFAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_zWOsQrCQBBEsc0nWG1lITEngiBaSRQhYBV72ZzL3em5G7KH5of8T8_CZhimeG-Kz6TYHDAhHNkFJhpgAa0X-3jji-BMt2Ax5q2RDpRwsB6E4STiIk13PqVet8aoxsppwhRsZeVphKmT0dyl019c1eNAfcRE19V6OVY9u3nZiGfNsBn8W9ZdKBsCQ4ucT9URByyh3n8Bvv075qgAAAA&shmds=v1_AdeF8KhzvrnvgizJOUSM9byeXVUcaToinq56rBVwHP0T7aiKtg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=MRbMxRhJvem6O8sFAAAAAA%3D%3D', 'extensions': ['11 days ago', '77Kâ€“143K a year', 'Full-time', 'Health insurance', 'Dental insurance', 'Paid time off'], 'detected_extensions': {'posted_at': '11 days ago', 'salary': '77Kâ€“143K a year', 'schedule_type': 'Full-time', 'health_insurance': True, 'dental_coverage': True, 'paid_time_off': True}, 'description': 'At Johnson & Johnson,\u202fwe believe health is everything. Our strength in healthcare innovation empowers us to build a\u202fworld where complex diseases are prevented, treated, and cured,\u202fwhere treatments are smarter and less invasive, and\u202fsolutions are personal.\u202fThrough our expertise in Innovative Medicine and MedTech, we are uniquely positioned to innovate across the full spectrum of healthcare solutions today to deliver the breakthroughs of tomorrow, and profoundly impact health for humanity.\u202fLearn more at https://www.jnj.com\n\nJob Function\n\nData Analytics & Computational Sciences\n\nJob Sub Function\n\nData Engineering\n\nJob Category\n\nScientific/Technology\n\nAll Job Posting Locations:\n\nSanta Clara, California, United States of America\n\nJob Description\n\nJohnson & Johnson is hiring for a Data Engineer â€“ Shockwave Medical to join our team. The position is FULLY REMOTE and can sit anywhere in the US.\n\nFueled by innovation at the intersection of biology and technology, weâ€™re developing the next generation of smarter, less invasive, more personalized treatments. Ready to join a team thatâ€™s pioneering the development and commercialization of Intravascular Lithotripsy (IVL) to treat complex calcified cardiovascular disease. Our Shockwave Medical portfolio aims to establish a new standard of care for medical device treatment of atherosclerotic cardiovascular disease through its differentiated and proprietary local delivery of sonic pressure waves for the treatment of calcified plaque.\n\nPosition Overview\n\nAs a Data Engineer reporting to the Sr. Manager, Data Engineering, you will play a pivotal role in enabling our business groups to access and interpret meaningful data. You will need to bridge detailed knowledge of our data with an understanding of end usersâ€™ needs, crafting accessible, well-structured data for reporting, advanced analytics, and use in AI modeling.\n\nEssential Job Functions\nâ€¢ Develop and maintain SQL-based data pipelines to extract, transform, and load (ETL) data from enterprise data warehouses.\nâ€¢ Aid in semantic model and view development within Snowflake to enable advanced, self-service analytics aligned to the newest analytical practices.\nâ€¢ Collaborate with team members to understand data and provide novel technical solutions.\nâ€¢ Experiment with new data engineering techniques and tools, embracing the unknown to drive innovation and improvement.\nâ€¢ Monitor data systems and implement enhancements driving performance and efficiency.\nâ€¢ Coordinate with IT to ensure seamless flow of data across systems, maintaining high standards of data availability and security.\nâ€¢ This is a hybrid/remote role, depending on location and business needs.\nâ€¢ Travel up to 20% may be required for business needs.\n\nRequirements\nâ€¢ Bachelorâ€™s degree and 2 years of experience as a Data Engineer or similar technical role.\nâ€¢ Expertise in SQL and ETL tools for data transformation and warehouse management.\nâ€¢ Experience working with Snowflake.\nâ€¢ Experience working with version control systems (Bitbucket or GitHub)\nâ€¢ Understanding of Power BI or other business intelligence tools.\nâ€¢ Familiarity with data governance, security, and compliance best practices.\nâ€¢ Propensity to problem-solving and ability to optimize data applications.\nâ€¢ Understanding of Python, R, or other scripting languages.\n\nJohnson & Johnson is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, disability, protected veteran status or other characteristics protected by federal, state or local law. We actively seek qualified candidates who are protected veterans and individuals with disabilities as defined under VEVRAA and Section 503 of the Rehabilitation Act.\n\nJohnson & Johnson is committed to providing an interview process that is inclusive of our applicantsâ€™ needs. If you are an individual with a disability and would like to request an accommodation, please contact us via https://www.jnj.com/contact-us/careers or contact AskGS to be directed to your accommodation resource.\n\nThe anticipated base pay range for this position is :\n\nUS: $77,000 - $124,200 / Bay Area: $89,000 - $142,600\n\nAdditional Description For Pay Transparency\n\nSubject to the terms of their respective plans, employees and/or eligible dependents are eligible to participate in the following Company sponsored employee benefit programs: medical, dental, vision, life insurance, short- and long-term disability, business accident insurance, and group legal insurance. Subject to the terms of their respective plans, employees are eligible to participate in the Companyâ€™s consolidated retirement plan (pension) and savings plan (401(k)). This position is eligible to participate in the Companyâ€™s long-term incentive program. Subject to the terms of their respective policies and date of hire, Employees are eligible for the following time off benefits: Vacation â€“120 hours per calendar year Sick time - 40 hours per calendar year; for employees who reside in the State of Washington â€“56 hours per calendar year Holiday pay, including Floating Holidays â€“13 days per calendar year Work, Personal and Family Time - up to 40 hours per calendar year Parental Leave â€“ 480 hours within one year of the birth/adoption/foster care of a child Condolence Leave â€“ 30 days for an immediate family member: 5 days for an extended family member Caregiver Leave â€“ 10 days Volunteer Leave â€“ 4 days Military Spouse Time-Off â€“ 80 hours Additional information can be found through the link below. https://www.careers.jnj.com/employee-benefits', 'apply_options': [{'title': 'Jobs And Careers', 'link': 'https://www.career.com/job/johnson-johnson-medtech/data-engineer-shockwave-medical/j202509100209347108217?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIC0gU2hvY2t3YXZlIE1lZGljYWwiLCJjb21wYW55X25hbWUiOiJKb2huc29uIFx1MDAyNiBKb2huc29uIE1lZFRlY2giLCJhZGRyZXNzX2NpdHkiOiJTYW50YSBDbGFyYSwgQ0EiLCJodGlkb2NpZCI6Ik1SYk14UmhKdmVtNk84c0ZBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Senior Data Engineer (Python, Scala, AWS Cloud) (m/w/d)', 'company_name': 'Capital One', 'location': 'Salisbury, MD', 'via': 'SaluteMyJob', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=zyD8pTiPQs6I80VlAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXFvQrCMBAAYFz7COJwo0ptRHDRSVoRBH8gg2O51iONpHclSbF9Jl9SXb4v-UySqya24qHAiHBkY5nIw_w-xkY4BV2jwxQODw25k_65gHmr3ur3Cs5SQSD0dQPCcBIxjqb7JsYu7JQKwWUmRIy2zmpplTBVMqiXVOFPGRr01DmMVG626yHr2CxnOXY2ooMbE1gGjc6GqvdjCpfiC-qZ0NeuAAAA&shmds=v1_AdeF8KgbSI_rgrmINFsNNRdoF84n6u8Nj7RbkFQ6Qcncl-m6SA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=zyD8pTiPQs6I80VlAAAAAA%3D%3D', 'extensions': ['5 days ago', 'Full-time and Part-time', 'Health insurance'], 'detected_extensions': {'posted_at': '5 days ago', 'schedule_type': 'Full-time and Part-time', 'health_insurance': True}, 'description': "Senior Data Engineer (Python, Scala, AWS Cloud)\n\nDo you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative,inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you'll have the opportunity to be on the forefront of driving a major transformation within Capital One.\n\nWhat You'll Do:\nâ€¢ Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies\nâ€¢ Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems\nâ€¢ Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake\nâ€¢ Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community\nâ€¢ Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment\nâ€¢ Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance\n\nBasic Qualifications:\nâ€¢ Bachelor's Degree\nâ€¢ At least 3 years of experience in application development (Internship experience does not apply)\nâ€¢ At least 1 year of experience in big data technologies\n\nPreferred Qualifications:\nâ€¢ 5+ years of experience in application development including Python, SQL, Scala, or Java\nâ€¢ 2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)\nâ€¢ 3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)\nâ€¢ 2+ year experience working on real-time data and streaming applications\nâ€¢ 2+ years of experience with NoSQL implementation (Mongo, Cassandra)\nâ€¢ 2+ years of data warehousing experience (Redshift or Snowflake)\nâ€¢ 3+ years of experience with UNIX/Linux including basic commands and shell scripting\nâ€¢ 2+ years of experience with Agile engineering practices\n\nAt this time, Capital One will not sponsor a new applicant for employment authorization, or offer any immigration related support for this position (i.e. H1B, F-1 OPT, F-1 STEM OPT, F-1 CPT, J-1, TN, E-2, E-3, L-1 and O-1, or any EADs or other forms of work authorization that require immigration support from an employer).\n\nThe minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.McLean, VA: $158,600 - $181,000 for Senior Data Engineer\n\nRichmond, VA: $144,200 - $164,600 for Senior Data Engineer\n\nCandidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate's offer letter.This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n\nCapital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at theCapital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer (EOE, including disability/vet) committed to non-discrimination in compliance with applicable federal, state, and local laws. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City's Fair Chance Act; Philadelphia's Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n\nIf you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.\n\nFor technical support or questions about Capital One's recruiting process, please send an email to Careers@\n\nCapital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.\n\nCapital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).", 'job_highlights': [{'title': 'Qualifications', 'items': ["Bachelor's Degree", 'At least 3 years of experience in application development (Internship experience does not apply)', 'At least 1 year of experience in big data technologies', 'At this time, Capital One will not sponsor a new applicant for employment authorization, or offer any immigration related support for this position (i.e. H1B, F-1 OPT, F-1 STEM OPT, F-1 CPT, J-1, TN, E-2, E-3, L-1 and O-1, or any EADs or other forms of work authorization that require immigration support from an employer)']}, {'title': 'Benefits', 'items': ['Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.McLean, VA: $158,600 - $181,000 for Senior Data Engineer', 'Richmond, VA: $144,200 - $164,600 for Senior Data Engineer', "Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate's offer letter", 'This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI)', 'Incentives could be discretionary or non discretionary depending on the plan', 'Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being']}, {'title': 'Responsibilities', 'items': ['Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies', 'Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems', 'Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake', 'Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community', 'Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment', 'Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance']}], 'apply_options': [{'title': 'SaluteMyJob', 'link': 'https://salutemyjob.com/jobs/senior-data-engineer-python-scala-aws-cloud-m-w-d-salisbury-maryland/2457057266-2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobs For Stevenage Fans', 'link': 'https://jobs.stevenagefc.com/jobs/senior-data-engineer-python-scala-aws-cloud-m-w-d-salisbury-maryland/2457057266-2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'KGET Jobs', 'link': 'https://jobs.kget.com/jobs/senior-data-engineer-python-scala-aws-cloud-m-w-d-salisbury-maryland/2457057266-2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'KSNT Jobs', 'link': 'https://jobs.ksnt.com/jobs/senior-data-engineer-python-scala-aws-cloud-m-w-d-salisbury-maryland/2457057266-2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'IT JobServe', 'link': 'https://it.jobserve.com/job-in-Salisbury-Maryland-USA/SENIOR-DATA-ENGINEER-cadcd93b7bf5a122fe/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Aspire Media Group Jobs', 'link': 'https://jobs.aspiremediagroup.net/jobs/senior-data-engineer-python-scala-aws-cloud-m-w-d-salisbury-maryland/2457057266-2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'FOX8 Jobs', 'link': 'https://jobs.fox8.com/jobs/senior-data-engineer-python-scala-aws-cloud-m-w-d-salisbury-maryland/2457057266-2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'WDTN Jobs', 'link': 'https://jobs.wdtn.com/jobs/senior-data-engineer-python-scala-aws-cloud-m-w-d-salisbury-maryland/2457057266-2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBFbmdpbmVlciAoUHl0aG9uLCBTY2FsYSwgQVdTIENsb3VkKSAobS93L2QpIiwiY29tcGFueV9uYW1lIjoiQ2FwaXRhbCBPbmUiLCJhZGRyZXNzX2NpdHkiOiJTYWxpc2J1cnksIE1EIiwiaHRpZG9jaWQiOiJ6eUQ4cFRpUFFzNkk4MFZsQUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': 'Data Engineer (On-Site Columbia, SC)', 'company_name': 'Genesis Healthcare, Inc', 'location': 'Columbia, SC', 'via': 'Indeed', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=T02qFr_6ypct4zpuAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_1XNPQrCQBBAYWxzBKvp_CHJimBjyihRG4scIOwuw-7KZiZkRvA-XlQtbV71wSvei6I5WbVwppAIcYb1nao-KULL-Tm6ZEvo2w1UcGMHgnb2EZigYw4Zl01UneRojEiug6jV5GvPo2FCxy_zYCe_DBLtjFO2isP-sHvVE4XtqkNCSQIXtFmj_4oSruQh0d_9A19RBNWmAAAA&shmds=v1_AdeF8KjBYKU7uDtsrIsgMjCsDLgJMfcIRwYWNXbku65MZgeikA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=T02qFr_6ypct4zpuAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965c4fbbeb58ce41ae9b0/images/f7d61a995a39953d7cea5211609d76499ad0ae91ba8706b0e56ab8e073fd9f3a.jpeg', 'extensions': ['Full-time', 'Health insurance', 'Paid time off'], 'detected_extensions': {'schedule_type': 'Full-time', 'health_insurance': True, 'paid_time_off': True}, 'description': 'Work Where Balance Meets Purpose â€“ Competitive Pay & Excellent Benefits!\n\nJoin Genesis Healthcare and be the driving force behind smooth, patient-focused operations.\n\nNow Hiring: Data Engineer\n\nLocation: Genesis Healthcare, Inc (Columbia, SC- Onsite)\n\nSchedule: Monday â€“ Friday, Day Shift (8 hours)\n\nNo Weekends â€“ Enjoy Work-Life Balance!\n\nMake a Real Difference in Community Healthcare\n\nAre you looking for a meaningful role where you can truly make an impact? At Genesis Healthcare, Inc., weâ€™re not just another healthcare providerâ€”weâ€™re a nonprofit, community-focused FQHC dedicated to improving lives across the Pee Dee and Low Country regions of South Carolina.\n\nWhy Choose Genesis?\n\nGenerous Paid Time Off-Holidays, sick leave, CME hours\n\nLife Insurance & Employee Assistance Programâ€“ Prioritizing your well-being\n\n401(k) Match, Vision, and More\n\nReady to take the next step in your career? Apply today and be part of something bigger!\n\nPOSITION SUMMARY\n\nThe Data Engineer will work closely with the Business Intelligence Officer and other members of the Business Intelligence (BI) team to design, develop, and maintain robust data pipelines to ensure high quality and efficient data solutions that meet business needs. The Data Engineer will assist in creating queries, stored procedures, views, and perform other tasks in our SQL environment.\n\nPRIMARY ACCOUNTABILITIES\n\n1. Design, build, and maintain data pipelines for analytics, reporting, and machine learning.\n\n2. Develop, implement, and optimize data automation and ETL (Extract, Transform, Load) processes.\n\n3. Maintain and optimize databases, data warehouses, and other data storage solutions.\n\n4. Utilize Microsoft Azure to deploy data pipelines and processing jobs.\n\n5. Monitor and troubleshoot data pipeline issues to ensure reliable data flow.\n\n6. Assist in designing, developing, and maintaining AI models.\n\n7. Continuously evaluate and improve data engineering processes and tools for better performance and efficiency.\n\n8. Develop and implement queries, views, tables, stored procedures, and other SQL related tasks for reporting needs.\n\n9. Collaborate with business stakeholders to design, implement, and maintain BI models.\n\n10. Utilize and manage APIs for connecting to third party software for data processing, data management, and data extraction.\n\nOperational Excellence\n\n1. Ensure and uphold the confidentially requirements of all patient records, and manage all daily tasks and activities consistent with HIPAA, state and federal laws and regulations, as well as the clinicâ€™s policies and regulations regarding confidentiality and security.\n\n2. Assures that all payments issued are appropriate and documented as ordered and received\n\nRelationships\n\n3. Develop and ensure effective, positive relationships within and among the clinic staff, as well as with patients, vendors, contractors, and related resources.\n\nProfessionalism\n\n4. Ensure all actions, job performance, personal conduct and communications represent the organization in a highly professional manner at all times.\n\n5. Uphold and ensure compliance with and attention to all corporate policies and procedures, as well as the mission and values of the organization.\n\nPRIMARY TASKS & DUTIES\nâ€¢ Develop, implement, and maintain data pipelines for BI reporting across the business\nâ€¢ Utilize SSMS to develop queries, tables, views, and stored procedures\nâ€¢ Automate recurring data processes and improve data accessibility across the organization\nâ€¢ Creating, managing, and troubleshooting APIs for connecting to third party software for data processing and data extraction\n\nESSENTIAL FUNCTIONS/KEY COMPETENCIES\nâ€¢ 1-5 years (preferred) of experience with Microsoft Azure, SharePoint, and Power Platform\nâ€¢ 1-5 years (preferred) of experience with SQL Server Management Studio (SSMS), T-SQL query writing, and database development\nâ€¢ 1-5 years (preferred) of experience with Python and PowerShell\nâ€¢ 1-5 years (preferred) of experience with developing and utilizing APIs\nâ€¢ Advanced analytical and problem-solving skills\nâ€¢ Strong oral and written communication skills\nâ€¢ Knowledge of basic computer hardware and Windows OS\n\nPOSITION REQUIREMENTS\n\nEducation\nâ€¢ Batchelorâ€™s degree in Information Technology, Computer Science, or related area. Must be able to speak, read, write, and understand English.\n\nProfessional\nâ€¢ â€œSkilledâ€ business office experience.\n\nPhysical/Environmental\nâ€¢ Ability to interact with computer screen for up to six hours at a time (visual acuity required).\nâ€¢ Must have manual dexterity for use of keyboard. Ability to remain stationary for periods of up to four hours. Ability to communicate via phone, mail and in person to resolve disputes, solve problems, etc.\nâ€¢ Capacity to function in a sometimes stressful, multi-tasking environment', 'job_highlights': [{'title': 'Qualifications', 'items': ['Advanced analytical and problem-solving skills', 'Strong oral and written communication skills', 'Knowledge of basic computer hardware and Windows OS', 'Batchelorâ€™s degree in Information Technology, Computer Science, or related area', 'Must be able to speak, read, write, and understand English', 'â€œSkilledâ€ business office experience', 'Ability to interact with computer screen for up to six hours at a time (visual acuity required)', 'Must have manual dexterity for use of keyboard', 'Ability to remain stationary for periods of up to four hours', 'Ability to communicate via phone, mail and in person to resolve disputes, solve problems, etc', 'Capacity to function in a sometimes stressful, multi-tasking environment']}, {'title': 'Benefits', 'items': ['Work Where Balance Meets Purpose â€“ Competitive Pay & Excellent Benefits!', 'No Weekends â€“ Enjoy Work-Life Balance!', 'Generous Paid Time Off-Holidays, sick leave, CME hours', 'Life Insurance & Employee Assistance Programâ€“ Prioritizing your well-being', '401(k) Match, Vision, and More']}, {'title': 'Responsibilities', 'items': ['Schedule: Monday â€“ Friday, Day Shift (8 hours)', 'The Data Engineer will work closely with the Business Intelligence Officer and other members of the Business Intelligence (BI) team to design, develop, and maintain robust data pipelines to ensure high quality and efficient data solutions that meet business needs', 'The Data Engineer will assist in creating queries, stored procedures, views, and perform other tasks in our SQL environment', 'PRIMARY ACCOUNTABILITIES', 'Design, build, and maintain data pipelines for analytics, reporting, and machine learning', 'Develop, implement, and optimize data automation and ETL (Extract, Transform, Load) processes', 'Maintain and optimize databases, data warehouses, and other data storage solutions', 'Utilize Microsoft Azure to deploy data pipelines and processing jobs', 'Monitor and troubleshoot data pipeline issues to ensure reliable data flow', 'Assist in designing, developing, and maintaining AI models', 'Continuously evaluate and improve data engineering processes and tools for better performance and efficiency', 'Develop and implement queries, views, tables, stored procedures, and other SQL related tasks for reporting needs', 'Collaborate with business stakeholders to design, implement, and maintain BI models', 'Utilize and manage APIs for connecting to third party software for data processing, data management, and data extraction', 'Ensure and uphold the confidentially requirements of all patient records, and manage all daily tasks and activities consistent with HIPAA, state and federal laws and regulations, as well as the clinicâ€™s policies and regulations regarding confidentiality and security', 'Assures that all payments issued are appropriate and documented as ordered and received', 'Develop and ensure effective, positive relationships within and among the clinic staff, as well as with patients, vendors, contractors, and related resources', 'Ensure all actions, job performance, personal conduct and communications represent the organization in a highly professional manner at all times', 'Uphold and ensure compliance with and attention to all corporate policies and procedures, as well as the mission and values of the organization', 'PRIMARY TASKS & DUTIES', 'Develop, implement, and maintain data pipelines for BI reporting across the business', 'Utilize SSMS to develop queries, tables, views, and stored procedures', 'Automate recurring data processes and improve data accessibility across the organization', 'Creating, managing, and troubleshooting APIs for connecting to third party software for data processing and data extraction']}], 'apply_options': [{'title': 'Indeed', 'link': 'https://www.indeed.com/viewjob?jk=5b61a0e31fc414b4&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Glassdoor', 'link': 'https://www.glassdoor.com/job-listing/data-engineer-genesis-healthcare-inc-JV_IC1155132_KO0,13_KE14,36.htm?jl=1009843402930&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'SimplyHired', 'link': 'https://www.simplyhired.com/job/XuRMIcDVfXoc9IAn6FVFflGREX0F9LNxZPR1RTpEz-pLO2YCEp6d_w?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIChPbi1TaXRlIENvbHVtYmlhLCBTQykiLCJjb21wYW55X25hbWUiOiJHZW5lc2lzIEhlYWx0aGNhcmUsIEluYyIsImFkZHJlc3NfY2l0eSI6IkNvbHVtYmlhLCBTQyIsImh0aWRvY2lkIjoiVDAycUZyXzZ5cGN0NHpwdUFBQUFBQT09IiwiaGwiOiJlbiJ9'}, {'title': 'Freelance Data Scientist', 'company_name': 'Twine', 'location': 'Cincinnati, OH', 'via': 'Twine', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=ru8SYSABqIsX98KkAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXLsQrCQAwAUFz7A4JTZtGeCC46KiouDrqX3BGuKWdyNAH7B_62urztNZ9ZE84jUUFJBCd0hEdiEmdzWMNNIxjhmHpQgYtqLrQ49O7V9iGYlTabo3Nqk76CCkWdwqDR_nTW40i1oFO33W2mtkpezp9vFgIWOLIkFvntFdyvXwPF_WSKAAAA&shmds=v1_AdeF8KiN9zPoS2KmhQtZyvDBcsSogftcuafIr5gwguknTfi6Ww&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=ru8SYSABqIsX98KkAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965c4fbbeb58ce41ae9b0/images/f7d61a995a39953d9d36d090d41fefda8dcbfcd95a4b9f03d2dc9ebe0b3469a7.png', 'extensions': ['4 days ago', 'Contractor', 'Paid time off', 'Health insurance', 'Dental insurance'], 'detected_extensions': {'posted_at': '4 days ago', 'schedule_type': 'Contractor', 'paid_time_off': True, 'health_insurance': True, 'dental_coverage': True}, 'description': 'Data Scientist is needed in Cincinnati, United States.\n\nClient: 84.51Â°\n\nLocation: Cincinnati, OH\n\nContract: undefined\nJob Description\n\nThe Lead Data Engineer serves as both a technical leader and individual contributor within the data engineering team, embodying a player/coach role. This position is responsible for guiding and mentoring a team of data engineers while actively participating in data engineering projects to deliver results that support organizational objectives. As a technical lead, you will balance team leadership with hands-on contributions to drive innovation and excellence in data engineering. You will cultivate strategies and solutions to ingest, store, and distribute our big data. Our developers use Python (PySpark, FastAPI), Databricks, and Azure cloud services in 6-week long scrum cycles to develop the products, tools, and features.\nRequirements\nâ€¢ Bachelorâ€™s degree typically in Computer Science, Management Information Systems, Mathematics, Business Analytics, or another technically strong program.\nâ€¢ 5+ years of proven professional data development experience.\nâ€¢ Strong understanding of Agile Principles (Scrum).\nâ€¢ 5+ years of proven ability of developing Spark-based solutions in a cloud environment.\nâ€¢ Full understanding of ETL concepts and data warehousing concepts.\nâ€¢ 3+ years of developing experience with Python.\nâ€¢ Experience with Databricks, REST APIs, and Microsoft Azure cloud services.\nResponsibilities\nâ€¢ Lead design and perform development of Python (PySpark, FastAPI) / Databricks / Azure based solutions.\nâ€¢ Develop and execute unit and integration testing.\nâ€¢ Collaborate with senior resources to ensure consistent development practices.\nâ€¢ Provide mentoring to junior resources.\nâ€¢ Bring new perspectives to problems and be driven to improve yourself and the way things are done.\nâ€¢ Manage resourcing across key initiatives in support of domain roadmaps and initiatives.\nAdditional Information\n\nPay Transparency: The stated salary range represents the entire span applicable across all geographic markets from lowest to highest. Actual salary offers will be determined by multiple factors including but not limited to geographic location, relevant experience, knowledge, skills, other job-related qualifications, and alignment with market data and cost of labor. In addition to salary, this position is also eligible for variable compensation.\n\nBenefits Offered:\nâ€¢ Health: Medical with competitive plan designs, Dental, Vision.\nâ€¢ Wealth: 401(k) with Roth option and matching contribution, Health Savings Account with matching contribution, AD&D, and supplemental insurance options.\nâ€¢ Happiness: Hybrid work environment, paid time off with flexibility, including 5 weeks of vacation, 7 health and wellness days, and parental leave.\n\nPay Range: $121,000 - $201,250 USD\nPosted in 8 hours', 'job_highlights': [{'title': 'Qualifications', 'items': ['Data Scientist is needed in Cincinnati, United States', 'Bachelorâ€™s degree typically in Computer Science, Management Information Systems, Mathematics, Business Analytics, or another technically strong program', '5+ years of proven professional data development experience', 'Strong understanding of Agile Principles (Scrum)', '5+ years of proven ability of developing Spark-based solutions in a cloud environment', 'Full understanding of ETL concepts and data warehousing concepts', '3+ years of developing experience with Python', 'Experience with Databricks, REST APIs, and Microsoft Azure cloud services']}, {'title': 'Benefits', 'items': ['Actual salary offers will be determined by multiple factors including but not limited to geographic location, relevant experience, knowledge, skills, other job-related qualifications, and alignment with market data and cost of labor', 'In addition to salary, this position is also eligible for variable compensation', 'Health: Medical with competitive plan designs, Dental, Vision', 'Wealth: 401(k) with Roth option and matching contribution, Health Savings Account with matching contribution, AD&D, and supplemental insurance options', 'Happiness: Hybrid work environment, paid time off with flexibility, including 5 weeks of vacation, 7 health and wellness days, and parental leave', 'Pay Range: $121,000 - $201,250 USD']}, {'title': 'Responsibilities', 'items': ['The Lead Data Engineer serves as both a technical leader and individual contributor within the data engineering team, embodying a player/coach role', 'This position is responsible for guiding and mentoring a team of data engineers while actively participating in data engineering projects to deliver results that support organizational objectives', 'As a technical lead, you will balance team leadership with hands-on contributions to drive innovation and excellence in data engineering', 'You will cultivate strategies and solutions to ingest, store, and distribute our big data', 'Our developers use Python (PySpark, FastAPI), Databricks, and Azure cloud services in 6-week long scrum cycles to develop the products, tools, and features', 'Lead design and perform development of Python (PySpark, FastAPI) / Databricks / Azure based solutions', 'Develop and execute unit and integration testing', 'Collaborate with senior resources to ensure consistent development practices', 'Provide mentoring to junior resources', 'Bring new perspectives to problems and be driven to improve yourself and the way things are done', 'Manage resourcing across key initiatives in support of domain roadmaps and initiatives']}], 'apply_options': [{'title': 'Twine', 'link': 'https://www.twine.net/projects/b942g0-84510-lead-data-engineer-p4031-tagsnew-data-scientist-in-cincinnati-united-states-job?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJGcmVlbGFuY2UgRGF0YSBTY2llbnRpc3QiLCJjb21wYW55X25hbWUiOiJUd2luZSIsImFkZHJlc3NfY2l0eSI6IkNpbmNpbm5hdGksIE9IIiwiaHRpZG9jaWQiOiJydThTWVNBQnFJc1g5OEtrQUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': 'Lead Data Engineer', 'company_name': 'Alexander Technology Group', 'location': 'Dedham, MA', 'via': 'Data Placement', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=tTZJvDDddQQT2P3kAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMsQrCMBAAUFz7CU43qmgjgotOhUpBdHMv1_RIIuldyEWov-EXq8sbX_VZVOsb4QgtFoQLu8BEGXZwlQGUMFsPwtCJuEjLsy8l6ckY1Vg7LViCra1MRpgGmc1TBv3Tq8dMKWKh_nDcz3Vit1k1kWbk8dc_yHqWKO4NXZZXgsDQ0uhx2sK9-QIonmqIlQAAAA&shmds=v1_AdeF8KjowBy2C5H0giVLDY16AQK5DgbGb4o5XYFy5k_hy1e5xA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=tTZJvDDddQQT2P3kAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965c4fbbeb58ce41ae9b0/images/f7d61a995a39953d29505b923c3943f1273b1c82a20ad66f17b36bcf7045ffa8.jpeg', 'extensions': ['111,532â€“159,395 a year', 'Full-time', 'No degree mentioned'], 'detected_extensions': {'salary': '111,532â€“159,395 a year', 'schedule_type': 'Full-time', 'qualifications': 'No degree mentioned'}, 'description': 'JOB SUMMARY\n\nLead Data Engineer\n\nBoston MA/Remote (EST time zone)\n\nFull-time ( no sponsorship available)\n\nSaaS industry\n\nThe Alexander Technology Group is searching for a hands-on, technical leader to own the design, implementation, and management of a data architecture. As Lead Data Engineer, youâ€™ll operate as a hands-on lead â€”both architecting scalable solutions and directly building them as well as leading a small team. This role requires someone deeply hands-on with data engineering and able to lead a high-performing teamâ€”mentoring engineers, driving standards, and ensuring delivery excellence. Please reach out to Chris McMillan at cmcmillan@alexandertg.com with an updated resume.\n\nThe Role:\nâ€¢ Design and lead scalable, secure, high-performance data solutions within the Azure ecosystem.\nâ€¢ Define and enforce data engineering standards, best practices, and architectural patterns.\nâ€¢ Evaluate and integrate emerging data technologies.\nâ€¢ Lead, mentor, and develop a team of data engineers while actively contributing to projects.\nâ€¢ Serve as a subject matter expert for internal stakeholders and clients.\nâ€¢ Partner with data science, analytics, and business teams to align architecture with business objectives.\nâ€¢ Build and optimize data warehouses, data lakes, and ETL/ELT pipelines using Azure Synapse, Data Factory, and Microsoft Fabric.\nâ€¢ Design efficient data storage solutions (SQL, NoSQL, cloud warehouses).\nâ€¢ Develop and maintain data models and schemas.\nâ€¢ Optimize workflows for scalability and performance.\nâ€¢ Implement robust data governance, security, and compliance frameworks.\nâ€¢ Ensure data quality through validation, cleansing, and monitoring.\n\nExperience\nâ€¢ 8-10 years of data engineering experience, with 3+ years in a technical leadership.\nâ€¢ Data architecture experience is a must.\nâ€¢ Proven hands-on experience building pipelines, designing models, and coding in SQL, Python, and/or Spark.\nâ€¢ Demonstrated ability to lead, mentor, and manage a technical team.\nâ€¢ Expertise in Azure Synapse Analytics, Azure Data Factory, Azure Data Lake, and Microsoft Fabric or DataBricks or Snowflake.\nâ€¢ Strong background in data warehouse architecture, modeling, and performance tuning.\nâ€¢ Experience with cloud-native architecture, DevOps, and CI/CD pipelines in Azure.\nâ€¢ Excellent communication skills and the ability to influence strategy.\n\nPlease reach out to Chris McMillan at cmcmillan@alexandertg.com with an updated resume.\n\nATG456\nâ€¢ MONATG*\n\n#LI-CM1', 'job_highlights': [{'title': 'Qualifications', 'items': ['This role requires someone deeply hands-on with data engineering and able to lead a high-performing teamâ€”mentoring engineers, driving standards, and ensuring delivery excellence', '8-10 years of data engineering experience, with 3+ years in a technical leadership', 'Data architecture experience is a must', 'Proven hands-on experience building pipelines, designing models, and coding in SQL, Python, and/or Spark', 'Demonstrated ability to lead, mentor, and manage a technical team', 'Expertise in Azure Synapse Analytics, Azure Data Factory, Azure Data Lake, and Microsoft Fabric or DataBricks or Snowflake', 'Strong background in data warehouse architecture, modeling, and performance tuning', 'Experience with cloud-native architecture, DevOps, and CI/CD pipelines in Azure', 'Excellent communication skills and the ability to influence strategy', 'Please reach out to Chris McMillan at cmcmillan@alexandertg.com with an updated resume']}, {'title': 'Responsibilities', 'items': ['As Lead Data Engineer, youâ€™ll operate as a hands-on lead â€”both architecting scalable solutions and directly building them as well as leading a small team', 'Design and lead scalable, secure, high-performance data solutions within the Azure ecosystem', 'Define and enforce data engineering standards, best practices, and architectural patterns', 'Evaluate and integrate emerging data technologies', 'Lead, mentor, and develop a team of data engineers while actively contributing to projects', 'Serve as a subject matter expert for internal stakeholders and clients', 'Partner with data science, analytics, and business teams to align architecture with business objectives', 'Build and optimize data warehouses, data lakes, and ETL/ELT pipelines using Azure Synapse, Data Factory, and Microsoft Fabric', 'Design efficient data storage solutions (SQL, NoSQL, cloud warehouses)', 'Develop and maintain data models and schemas', 'Optimize workflows for scalability and performance', 'Implement robust data governance, security, and compliance frameworks', 'Ensure data quality through validation, cleansing, and monitoring']}], 'apply_options': [{'title': 'Data Placement', 'link': 'https://www.dataplacement.com/jobs/154644727-lead-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJMZWFkIERhdGEgRW5naW5lZXIiLCJjb21wYW55X25hbWUiOiJBbGV4YW5kZXIgVGVjaG5vbG9neSBHcm91cCIsImFkZHJlc3NfY2l0eSI6IkRlZGhhbSwgTUEiLCJodGlkb2NpZCI6InRUWkp2RERkZFFRVDJQM2tBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Software Engineering Manager - Snowpark Container Services', 'company_name': 'Snowflake', 'location': 'Bellevue, WA', 'via': 'Snowflake Careers', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=fEIxXNsW2N4p4lZ9AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWNMQrCQBAAsc0PtNpaNCeCjVYqIggWksIybMLmcubcPW7PJH_yk8ZmmhmY7DvLHoU0acBIcGHrmCg6tnBHRksR1lCwDAFjB2fhhFMQoaDYu5p0sjepQAlj3YIwXEWsp8WhTSno3hhVn1tNmFyd1_I2wlTJaF5S6R-lttM3eExUbnebMQ9sl_P_sPHYETiGE3lP_YdW8Dz-AFglsHSuAAAA&shmds=v1_AdeF8KgfdcasVqsmOW5Yv5ULIKepla7bh8tTaQHbjjUgU8iK7A&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=fEIxXNsW2N4p4lZ9AAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965c4fbbeb58ce41ae9b0/images/f7d61a995a39953d007fd67acc3597916c4d60e2ef71b403aca897d832b2a67d.png', 'extensions': ['1 day ago', 'Full-time', 'No degree mentioned', 'Dental insurance', 'Health insurance', 'Paid time off'], 'detected_extensions': {'posted_at': '1 day ago', 'schedule_type': 'Full-time', 'qualifications': 'No degree mentioned', 'dental_coverage': True, 'health_insurance': True, 'paid_time_off': True}, 'description': "Snowflake is about empowering enterprises to achieve their full potential â€” and people too. With a culture thatâ€™s all in on impact, innovation, and collaboration, Snowflake is the sweet spot for building big, moving fast, and taking technology â€” and careers â€” to the next level.\n\nSnowflake is on a mission to mobilize the world's data by changing the old-world paradigm of â€œbring data to computeâ€ to â€œbring compute to the dataâ€. Our innovative built-for-the-cloud architecture revolutionized the data warehousing industry with flexibility of big data platforms, and the elasticity of the cloud at a fraction of the cost of traditional solutions. Our products enable thousands of enterprises to unlock the value of their data with near-unlimited scale, concurrency, and performance.\n\nWe are looking for a talented, passionate Software Engineering Manager for our Snowpark Container Services product to build our elastic, high scale, high-performance, cloud native compute platform that makes bringing compute to data effortless and simple. Our mission is to make Snowflake a preferred platform to run all AI, ML, Data Science and Data Engineering workloads. We are building the next generation of compute container platform to open up the Snowflakeâ€™s doors to limitless opportunities of discovering structure out of unstructured data at scale. You will be part of a highly productive, fast moving, and growing team that is critical to realizing Snowflakeâ€™s Data Cloud Mission. This team innovates on behalf of our customers to build a fully managed compute platform that enables our customers to leverage their data to solve the business problems without needing to manage required compute infrastructure.\n\nAS A SOFTWARE ENGINEERING MANAGER FOR SNOWPARK CONTAINER SERVICES, YOU WILL:\nâ€¢ Provide the technical and strategic direction leadership to define and build a full eco-system of technologies and technical roadmap and vision.\nâ€¢ Plan, lead, and execute complex technical projects through your team that interact with a wide variety of teams within the company.\nâ€¢ Anticipate future trends and evolving customer needs accurately and drive strategies appropriately.\nâ€¢ Influence leadership on technical direction and product/team investment/pivot needs.\nâ€¢ Create tools/processes/solutions that improve teamâ€™s productivity and efficiency.\nâ€¢ Identify and lead improvements to product stability and reliability, on-call and service health, customer support, diagnosability and manageability of the product.\nâ€¢ Identify and fill skill and expertise gaps in the team.\nâ€¢ Mentor and guide engineers and grow a strong technical expertise in the team.\n\nOUR IDEAL SOFTWARE ENGINEERING MANAGER WILL HAVE:\nâ€¢ 10+ years industry experience designing and building large scale distributed systems platforms that accelerates product innovations.\nâ€¢ 4+ years of full-time Software Engineering Management experience.\nâ€¢ Strong passion for making developers highly productive and enabling them to be successful and grow in their career.\n\nEvery Snowflake employee is expected to follow the companyâ€™s confidentiality and security standards for handling sensitive data. Snowflake employees must abide by the companyâ€™s data security plan as an essential part of their duties. It is every employee's duty to keep customer information secure and confidential.\n\nSnowflake is growing fast, and weâ€™re scaling our team to help enable and accelerate our growth. We are looking for people who share our values, challenge ordinary thinking, and push the pace of innovation while building a future for themselves and Snowflake.\n\nHow do you want to make your impact?\n\nFor jobs located in the United States, please visit the job posting on the Snowflake Careers Site for salary and benefits information: careers.snowflake.com\n\nThe following represents the expected range of compensation for this role:\nâ€¢ The estimated base salary range for this role is $222,000 - $339,250.\nâ€¢ Additionally, this role is eligible to participate in Snowflakeâ€™s bonus and equity plan.\n\nThe successful candidateâ€™s starting salary will be determined based on permissible, non-discriminatory factors such as skills, experience, and geographic location. This role is also eligible for a competitive benefits package that includes: medical, dental, vision, life, and disability insurance; 401(k) retirement plan; flexible spending & health savings account; at least 12 paid holidays; paid time off; parental leave; employee assistance program; and other company benefits.\n\nTo comply with pay transparency requirements and other statutes, you can notify us if you believe that a job posting is not compliant by completing this form.", 'job_highlights': [{'title': 'Qualifications', 'items': ['10+ years industry experience designing and building large scale distributed systems platforms that accelerates product innovations', '4+ years of full-time Software Engineering Management experience', 'Strong passion for making developers highly productive and enabling them to be successful and grow in their career']}, {'title': 'Benefits', 'items': ['The estimated base salary range for this role is $222,000 - $339,250', 'Additionally, this role is eligible to participate in Snowflakeâ€™s bonus and equity plan', 'The successful candidateâ€™s starting salary will be determined based on permissible, non-discriminatory factors such as skills, experience, and geographic location', 'This role is also eligible for a competitive benefits package that includes: medical, dental, vision, life, and disability insurance; 401(k) retirement plan; flexible spending & health savings account; at least 12 paid holidays; paid time off; parental leave; employee assistance program; and other company benefits']}, {'title': 'Responsibilities', 'items': ['You will be part of a highly productive, fast moving, and growing team that is critical to realizing Snowflakeâ€™s Data Cloud Mission', 'AS A SOFTWARE ENGINEERING MANAGER FOR SNOWPARK CONTAINER SERVICES, YOU WILL:', 'Provide the technical and strategic direction leadership to define and build a full eco-system of technologies and technical roadmap and vision', 'Plan, lead, and execute complex technical projects through your team that interact with a wide variety of teams within the company', 'Anticipate future trends and evolving customer needs accurately and drive strategies appropriately', 'Influence leadership on technical direction and product/team investment/pivot needs', 'Create tools/processes/solutions that improve teamâ€™s productivity and efficiency', 'Identify and lead improvements to product stability and reliability, on-call and service health, customer support, diagnosability and manageability of the product', 'Identify and fill skill and expertise gaps in the team', 'Mentor and guide engineers and grow a strong technical expertise in the team']}], 'apply_options': [{'title': 'Snowflake Careers', 'link': 'https://careers.snowflake.com/us/en/job/SNCOUS18FF1F2C97064617960F77D44D82989FEXTERNALENUS1A58FCCC661A40588B9580C470E9B6F8/Software-Engineering-Manager-Snowpark-Container-Services?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Indeed', 'link': 'https://www.indeed.com/viewjob?jk=2a248108d21507f6&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Glassdoor', 'link': 'https://www.glassdoor.com/job-listing/software-engineering-manager-snowpark-container-services-snowflake-JV_IC1150442_KO0,56_KE57,66.htm?jl=1009736741968&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'LinkedIn', 'link': 'https://www.linkedin.com/jobs/view/software-engineering-manager-snowpark-container-services-at-snowflake-4286230840?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'SimplyHired', 'link': 'https://www.simplyhired.com/job/YglgMh_udGxg4SVNDLXJ-5iHua1VB2tv3VgyUycPRNZxfjmPK4N1VQ?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Teal', 'link': 'https://www.tealhq.com/job/software-engineering-manager-snowpark-container-services_d9b9c83f-8036-4288-9dfc-a61fb8b28b81?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Parallel', 'link': 'https://www.useparallel.com/app/candidate/job/68f3eed1e65fcd6ccad53c04?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'ZipRecruiter', 'link': 'https://www.ziprecruiter.com/c/Snowflake/Job/Software-Engineering-Manager-Snowpark-Container-Services/-in-Bellevue,WA?jid=1e70dbb192f5f463&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTb2Z0d2FyZSBFbmdpbmVlcmluZyBNYW5hZ2VyIC0gU25vd3BhcmsgQ29udGFpbmVyIFNlcnZpY2VzIiwiY29tcGFueV9uYW1lIjoiU25vd2ZsYWtlIiwiYWRkcmVzc19jaXR5IjoiQmVsbGV2dWUsIFdBIiwiaHRpZG9jaWQiOiJmRUl4WE5zVzJONHA0bFo5QUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': 'Data Engineer - Machine Learning', 'company_name': 'Insight Global', 'location': 'Atlanta, GA', 'via': 'Insight Global', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=dJNdxgX4eTpsAVe1AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWIQQrCMBAA8donCMKeRRsRRNBTQSmKvqFswpKkxN3Q3UM_4n-tl2Fmmu-qOd_QEO4cMxNNsIc3hrQ4vAgnzhyX9RQPumRIIAy9SCy0viazqhfnVEsb1dByaIN8nDB5md0oXv8YNOFEtaDRcDwd5rZy3G4erDkmg76IxwKZobOCbLiDvvsBVK5f65gAAAA&shmds=v1_AdeF8Kg2PnJIFpnulNUV1At4Oxualt4jPvWnZvYCTAmiTT-gQQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=dJNdxgX4eTpsAVe1AAAAAA%3D%3D', 'extensions': ['Full-time'], 'detected_extensions': {'schedule_type': 'Full-time'}, 'description': "Insight Global is looking for a Data Engineer to join one of our largest life insurance clients' Enterprise Data & Analytics engineering team. The ideal candidate will work closely with the Data Science team to enable cutting-edge AI and machine learning solutions that will contribute to enhancing customer well-being, fostering growth, maintaining competitive advantage, and customer satisfaction. The role requires a passion for data engineering, machine learning, and data-driven decision-making, and the ability to transform innovative ideas into tangible solutions that directly impact the business and customers. The Data Engineer will work in an innovative, fast-paced environment, collaborating with bright minds while enjoying a balance between strategic and hands-on work. The role offers opportunities for continuous learning and skillset expansion, mastering new tools and technologies that advance the companys goals. If you are a committed team player who thrives on creating value through innovative solutions and is eager to make a significant impact, this role could be a great fit for you.\nCollaborate with data scientists and analysts to understand data requirements and translate them into scalable, high-performing data pipeline solutions.\nSupport data discovery and preparation for model development, perform detailed analysis of raw data sources by applying business context, and collaborate with cross-functional teams to transform raw data into curated and certified data assets to be used for machine learning and business intelligence use cases.\nExtract text data from various sources like documents, logs, text notes stored in databases, and web pages using web scraping methods to support the development of natural language processing and language learning models.\nMonitor and troubleshoot data pipeline performance, identifying and resolving bottlenecks and issues.\nCollaborate with data science and data engineering teams to build scalable and reproducible machine learning pipelines for training and inference.\nImplement machine learning models into operations and processes via batch, streaming, and API methods.\nDevelop, test, and maintain robust tools, frameworks, and libraries that standardize and streamline the data and machine learning lifecycle.\nContribute to developing and maintaining end-to-end MLOps lifecycle to automate machine learning solutions development and delivery.\nImplement a robust monitoring framework for model performance.\nCollaborate with cross-functional teams of Data Science, Data Engineering, business units, and various IT teams.\nCreate and maintain effective documentation for projects and practices, ensuring transparency and effective team communication.\nStay up-to-date with the latest trends in modern data engineering, machine learning, and AI, ensuring that our company remains at the cutting edge of industry advancements.", 'job_highlights': [{'title': 'Responsibilities', 'items': ['The ideal candidate will work closely with the Data Science team to enable cutting-edge AI and machine learning solutions that will contribute to enhancing customer well-being, fostering growth, maintaining competitive advantage, and customer satisfaction', 'The role requires a passion for data engineering, machine learning, and data-driven decision-making, and the ability to transform innovative ideas into tangible solutions that directly impact the business and customers', 'The Data Engineer will work in an innovative, fast-paced environment, collaborating with bright minds while enjoying a balance between strategic and hands-on work', 'The role offers opportunities for continuous learning and skillset expansion, mastering new tools and technologies that advance the companys goals', 'If you are a committed team player who thrives on creating value through innovative solutions and is eager to make a significant impact, this role could be a great fit for you', 'Collaborate with data scientists and analysts to understand data requirements and translate them into scalable, high-performing data pipeline solutions', 'Support data discovery and preparation for model development, perform detailed analysis of raw data sources by applying business context, and collaborate with cross-functional teams to transform raw data into curated and certified data assets to be used for machine learning and business intelligence use cases', 'Extract text data from various sources like documents, logs, text notes stored in databases, and web pages using web scraping methods to support the development of natural language processing and language learning models', 'Monitor and troubleshoot data pipeline performance, identifying and resolving bottlenecks and issues', 'Collaborate with data science and data engineering teams to build scalable and reproducible machine learning pipelines for training and inference', 'Implement machine learning models into operations and processes via batch, streaming, and API methods', 'Develop, test, and maintain robust tools, frameworks, and libraries that standardize and streamline the data and machine learning lifecycle', 'Contribute to developing and maintaining end-to-end MLOps lifecycle to automate machine learning solutions development and delivery', 'Implement a robust monitoring framework for model performance', 'Collaborate with cross-functional teams of Data Science, Data Engineering, business units, and various IT teams', 'Create and maintain effective documentation for projects and practices, ensuring transparency and effective team communication', 'Stay up-to-date with the latest trends in modern data engineering, machine learning, and AI, ensuring that our company remains at the cutting edge of industry advancements']}], 'apply_options': [{'title': 'Insight Global', 'link': 'https://insightglobal.com/jobs/find_a_job/new-jersey/holmdel/data-engineer-machine-learning/job-336468/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobs By Workable', 'link': 'https://apply.workable.com/zyte/j/5BF866D3E2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Optim Hire', 'link': 'https://optimhire.com/fl/job/details/22668-data-engineer-nlpmachine-learning-python-sql-big-data-machine-learning-data-science-deep-learning-data-analytics?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'DailyRemote', 'link': 'https://dailyremote.com/remote-job/principle-machine-learning-system-engineer-data-engineering-3982931?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Remote Rocketship', 'link': 'https://www.remoterocketship.com/company/inovex-de/jobs/data-engineer-machine-learning-engineer-germany/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'MoAIJobs', 'link': 'https://www.moaijobs.com/job/machine-learning-engineer-fraud-data-plaid-6901?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Career.io', 'link': 'https://career.io/job/remote-data-engineer-machine-learning-phoenix-ctg-computer-task-group-ceec3cea89e16233f82bf9982645e960?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Career Vault', 'link': 'https://careervault.io/remote/addepar-580/data/sr-machine-learning-engineer-alternatives-data-management-remote-usa-5080316?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIC0gTWFjaGluZSBMZWFybmluZyIsImNvbXBhbnlfbmFtZSI6Ikluc2lnaHQgR2xvYmFsIiwiYWRkcmVzc19jaXR5IjoiQXRsYW50YSwgR0EiLCJodGlkb2NpZCI6ImRKTmR4Z1g0ZVRwc0FWZTFBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Data Engineer', 'company_name': 'TRESUME', 'location': 'Richmond, VA', 'via': 'Indeed', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=f5UgUBtVFU8gwl-YAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEvQrCMBAA4L0vIDjd4CSaiOCiU8EgCC71Zy1JPJJIehdyGTr48OI3fN23W51ts2AoJEKssIUrOxC01UdgggtzyLg8xdaKHLUWySpIsy155XnSTOh41h928m-UaCuWbBuO-8NuVoXCevEYzP15M5AIhuTjxPTewKv_ARfxPH5-AAAA&shmds=v1_AdeF8KgqHz7poNCAxV2JuWpOXZLgpTAGefjduHNdXeD_NK314w&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=f5UgUBtVFU8gwl-YAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965c4fbbeb58ce41ae9b0/images/f7d61a995a39953d5bc4e85efeea40a8f52942e21782b87d29a530c9381920ff.jpeg', 'extensions': ['19 days ago', '55Kâ€“60K a year', 'Full-time and Contractor', 'No degree mentioned'], 'detected_extensions': {'posted_at': '19 days ago', 'salary': '55Kâ€“60K a year', 'schedule_type': 'Full-time and Contractor', 'qualifications': 'No degree mentioned'}, 'description': 'Overview\nWe are seeking a skilled and motivated Data Engineer to join our dynamic team. The ideal candidate will possess a strong background in data architecture and engineering, with a passion for transforming raw data into actionable insights. As a Data Engineer, you will play a crucial role in designing, building, and maintaining scalable data pipelines and systems that support our data-driven decision-making processes.\n\nDuties\nâ€¢ Design, develop, and implement ETL processes to extract, transform, and load data from various sources into data warehouses or lakes.\nâ€¢ Collaborate with data scientists and analysts to understand their data needs and provide the necessary infrastructure.\nâ€¢ Utilize tools such as Apache Hive, Hadoop, and Azure Data Lake to manage large datasets effectively.\nâ€¢ Write efficient SQL queries for data retrieval and manipulation.\nâ€¢ Develop scripts in Python or Java for automating data processing tasks.\nâ€¢ Ensure the quality and integrity of data by implementing validation checks and monitoring systems.\nâ€¢ Explore linked data technologies to enhance the usability of datasets across platforms.\nâ€¢ Document processes, workflows, and system architectures for future reference.\n\nRequirements\nâ€¢ Proven experience as a Data Engineer or in a similar role with a strong understanding of database management systems.\nâ€¢ Proficiency in programming languages such as Python and Java.\nâ€¢ Experience with ETL tools and techniques; familiarity with Apache Hive is preferred.\nâ€¢ Knowledge of cloud platforms such as AWS or Azure Data Lake is highly desirable.\nâ€¢ Strong SQL skills for querying databases efficiently.\nâ€¢ Familiarity with linked data concepts is an advantage.\nâ€¢ Ability to work collaboratively in a team environment while managing multiple projects simultaneously.\nâ€¢ Excellent problem-solving skills and attention to detail.\n\nJoin us in leveraging the power of data to drive innovation and improve business outcomes!\n\nJob Types: Full-time, Contract\n\nPay: $55,000.00 - $60,000.00 per year\n\nWork Location: In person', 'job_highlights': [{'title': 'Qualifications', 'items': ['The ideal candidate will possess a strong background in data architecture and engineering, with a passion for transforming raw data into actionable insights', 'Proven experience as a Data Engineer or in a similar role with a strong understanding of database management systems', 'Proficiency in programming languages such as Python and Java', 'Strong SQL skills for querying databases efficiently', 'Familiarity with linked data concepts is an advantage', 'Ability to work collaboratively in a team environment while managing multiple projects simultaneously', 'Excellent problem-solving skills and attention to detail']}, {'title': 'Benefits', 'items': ['Pay: $55,000.00 - $60,000.00 per year']}, {'title': 'Responsibilities', 'items': ['As a Data Engineer, you will play a crucial role in designing, building, and maintaining scalable data pipelines and systems that support our data-driven decision-making processes', 'Design, develop, and implement ETL processes to extract, transform, and load data from various sources into data warehouses or lakes', 'Collaborate with data scientists and analysts to understand their data needs and provide the necessary infrastructure', 'Utilize tools such as Apache Hive, Hadoop, and Azure Data Lake to manage large datasets effectively', 'Write efficient SQL queries for data retrieval and manipulation', 'Develop scripts in Python or Java for automating data processing tasks', 'Ensure the quality and integrity of data by implementing validation checks and monitoring systems', 'Explore linked data technologies to enhance the usability of datasets across platforms', 'Document processes, workflows, and system architectures for future reference']}], 'apply_options': [{'title': 'Indeed', 'link': 'https://www.indeed.com/viewjob?jk=5a6adf7d6d4ebb61&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Ace Technologies', 'link': 'https://acetechnologies.com/jobs/onsite-jobs/data-engineer/946316?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'ZipRecruiter', 'link': 'https://www.ziprecruiter.com/c/Snowrelic-Inc/Job/Data-Engineer/-in-Richmond,VA?jid=df3c656d29f910db&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Monster', 'link': 'https://www.monster.com/job-openings/automation-engineer-richmond-va--e686ca88-fef6-4791-9ea8-f894d7e88471?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Dice', 'link': 'https://www.dice.com/job-detail/a413868f-3602-4124-9237-57ba89c9609f?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Glassdoor', 'link': 'https://www.glassdoor.com/job-listing/data-engineer-tresume-JV_IC1130198_KO0,13_KE14,21.htm?jl=1009919702450&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'LocalJobs.com', 'link': 'https://www.localjobs.com/job/richmond-va-data-engineer-3?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'SimplyHired', 'link': 'https://www.simplyhired.com/job/D4rjCTnQpDNE8kZF-QxmVbJ7ybgGgUhjvXDlbswZv5cF3jVAEkE5Cw?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyIiwiY29tcGFueV9uYW1lIjoiVFJFU1VNRSIsImFkZHJlc3NfY2l0eSI6IlJpY2htb25kLCBWQSIsImh0aWRvY2lkIjoiZjVVZ1VCdFZGVThnd2wtWUFBQUFBQT09IiwiaGwiOiJlbiJ9'}, {'title': 'Software Engineer (ETL Data Engineer) â€“ JOB ID â€“ ABT25080001', 'company_name': 'AMBRIGHT TECH LLC', 'location': 'United States', 'via': 'AmBright Tech', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=q9LkSo48hk3unjE2AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_z2NOwrCQBRFsc0SrB5WKpgZBUG0yo98iAgasQyT-EwicV7IDJjSPbgbl-NKjBY2l8MpzjVeA-N0oIu-ixbBk0UlEVsYe0kMrtDirybwfjwh2tkQuj-07GSx5CvO-RxmEFEGCkWbl0ASfKKixuGm1LpRa8aUqs1CaaGr3MzpxkhiRh27Uqa-k6qyf29qoTHtm53ZyGI6srb2PvSDBBLPCSCOHagkHGWl8QyHvoXqA_RQ-mu9AAAA&shmds=v1_AdeF8Ki0Y2Qoc4kg4hl6b9jDybMWFbTuCY1oAf3c8kQAYhOdRQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=q9LkSo48hk3unjE2AAAAAA%3D%3D', 'description': "Company: AmBright Tech LLC\n\nJob Title: Hadoop Developer\nClient: One of AmBright Tech LLC's Premier Financial Clients\nLocation: Plano, TX (Hybrid â€“ 3 Days Onsite per Week)\nDuration: 12 Months (with Possible Extension), Texas\n\nSoftware Engineer â€“ ETL Data Engineer â€“ JOB ID â€“ ABT25080001\n\nAbout the Role:\n\nAmBright Tech LLC is seeking a Hadoop Developer to support one of our key financial services clients. This mid to senior-level role focuses on production support and development within a high-performance data engineering environment. The ideal candidate will have strong experience in Big Data ecosystems, ETL workflows, and DevOps practices, especially in Credit Risk or financial data platforms.\n\nKey Responsibilities:\nâ€¢ Manage and support large-scale Big Data applications in a production support and DevOps environment.\nâ€¢ Perform data analysis and troubleshoot data quality/integrity issues across the Hadoop ecosystem.\nâ€¢ Develop and optimize Apache Spark jobs for data processing and transformation.\nâ€¢ Write efficient Hive, Impala, and SQL queries.\nâ€¢ Collaborate with cross-functional teams to support Credit Risk platforms and ensure smooth ETL operations.\nâ€¢ Handle root cause analysis for data-related incidents and performance bottlenecks.\nâ€¢ Work within Agile teams, participate in sprint planning and proactively identify risks and mitigation strategies.\nâ€¢ Implement and manage CI/CD pipelines using tools like Bitbucket, Gradle, Jenkins, Ansible, and Artifactory.\n\nRequired Qualifications:\nâ€¢ Bachelorâ€™s degree in Computer Science, Engineering, Information Systems, or related field.\nâ€¢ 7+ years of experience in ETL development, Big Data technologies, and Data Warehousing.\nâ€¢ Hands-on experience with:\nâ€¢ Hadoop (HDFS, Hive, Impala)\nâ€¢ Spark (optimization, troubleshooting, and development)\nâ€¢ Unix/Linux scripting\nâ€¢ SQL and query performance tuning\nâ€¢ Strong knowledge of DevOps tools and automation pipelines.\nâ€¢ Proven track record in Credit Risk or financial industry data platforms is a strong plus.\nâ€¢ Exceptional communication and collaboration skills.\n\nPreferred Skills:\nâ€¢ Previous experience working with financial clients, especially in Credit Risk domains.\nâ€¢ Understanding of regulatory requirements and risk modeling data pipelines.\n\nIf you're ready to bring your Big Data expertise into a critical production support environment and make a real impact on risk data operations, apply now and join AmBright Tech LLCâ€™s growing team of data professionals!", 'job_highlights': [{'title': 'Qualifications', 'items': ['The ideal candidate will have strong experience in Big Data ecosystems, ETL workflows, and DevOps practices, especially in Credit Risk or financial data platforms', 'Bachelorâ€™s degree in Computer Science, Engineering, Information Systems, or related field', '7+ years of experience in ETL development, Big Data technologies, and Data Warehousing', 'Hands-on experience with:', 'Hadoop (HDFS, Hive, Impala)', 'Spark (optimization, troubleshooting, and development)', 'Unix/Linux scripting', 'SQL and query performance tuning', 'Strong knowledge of DevOps tools and automation pipelines', 'Proven track record in Credit Risk or financial industry data platforms is a strong plus', 'Exceptional communication and collaboration skills', "If you're ready to bring your Big Data expertise into a critical production support environment and make a real impact on risk data operations, apply now and join AmBright Tech LLCâ€™s growing team of data professionals!"]}, {'title': 'Responsibilities', 'items': ['This mid to senior-level role focuses on production support and development within a high-performance data engineering environment', 'Manage and support large-scale Big Data applications in a production support and DevOps environment', 'Perform data analysis and troubleshoot data quality/integrity issues across the Hadoop ecosystem', 'Develop and optimize Apache Spark jobs for data processing and transformation', 'Write efficient Hive, Impala, and SQL queries', 'Collaborate with cross-functional teams to support Credit Risk platforms and ensure smooth ETL operations', 'Handle root cause analysis for data-related incidents and performance bottlenecks', 'Work within Agile teams, participate in sprint planning and proactively identify risks and mitigation strategies', 'Implement and manage CI/CD pipelines using tools like Bitbucket, Gradle, Jenkins, Ansible, and Artifactory']}], 'apply_options': [{'title': 'AmBright Tech', 'link': 'http://www.ambrighttech.com/jobs/software-engineer-etl-data-engineer-job-id-abt24021022/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTb2Z0d2FyZSBFbmdpbmVlciAoRVRMIERhdGEgRW5naW5lZXIpIOKAkyBKT0IgSUQg4oCTIEFCVDI1MDgwMDAxIiwiY29tcGFueV9uYW1lIjoiQU1CUklHSFQgVEVDSCBMTEMiLCJhZGRyZXNzX2NpdHkiOiJVbml0ZWQgU3RhdGVzIiwiaHRpZG9jaWQiOiJxOUxrU280OGhrM3VuakUyQUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': 'Senior Data Engineer, VP', 'company_name': 'Apple Bank', 'location': 'New York, NY, United States', 'via': 'ZipRecruiter', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=qMFVBwaqlMnojs2UAAAAAA%3D%3D&hl=en-EC&shndl=37&shmd=H4sIAAAAAAAA_xXMsQrCMBAGYFz7CE7_qtRGBBedFEVwKEJR6CRpPdLYehdyAfsqvq26fOOXfSaZqYi9RBxssjiy80wUc9wuWOAsDZRsbDsI4yTiBppuu5SCboxRHQqnySbfFq28jDA1MpqnNPrnrp2NFAab6L5aL8cisJvPdiEMhL3lHp5R0hu1xD5HWee4sk_0QPUrSb-0KygznAAAAA&shmds=v1_AdeF8KivcFbwDwj8rbpQO198bvPwPt8bHs1dhYbXU9i2G9-p_A&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=qMFVBwaqlMnojs2UAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965c52efef81170d6a44f/images/2dc7e0617baa2797b24779783c1f8a140c7fb56e96208030b237b01d21f4f262.jpeg', 'extensions': ['$170Kâ€“$240K a year', 'Full-time'], 'detected_extensions': {'salary': '$170Kâ€“$240K a year', 'schedule_type': 'Full-time'}, 'description': 'Hybrid/Manhattan, NY\nPay Range: $170,000 - $240,000\n\nThe Senior Data Engineer plays a key role in the design, development, and expansion of databases, data orchestrations, and business intelligence for Apple Bank. This position assists with the build, maintenance, and optimization of data pipelines into our Enterprise Data Warehouse (EDW) and creates extracts for vendors and partners. The Senior Engineer also develops stage tables, operational data store (ODS) databases and tables, data marts, and all ETL processes. The work product provided by the Engineer is required to be fit-for-purpose for each data source, repeatable, and automatable, which involves stored procedures, functions, data analyses, cleansing, validations, and reporting.\n\nThe role collaborates closely with IT infrastructure colleagues, database administrators, business stakeholders, data operations teammates, project managers, vendors, and analysts embedded in our businesses. The successful candidate must be able to ask incisive business questions, tackle complexity, and own delivery. Future opportunities may include sourcing third party and public data, additional source-system integrations, and hybrid cloud proof-of-concepts.\n\nESSENTIAL DUTIES & RESPONSIBILITIES\nâ€¢ Establish solution architecture, SQL Server databases, and schemas.\nâ€¢ Design and build corresponding SSIS ETLs, solving business needs and seizing opportunities.\nâ€¢ Perform data profiling to identify and understand projects thoroughly.\nâ€¢ Map data from source to repository.\nâ€¢ Build and automate data extracts for vendors and partners to specifications.\nâ€¢ Understand and coordinate the granting of permissions to service and proxy accounts to ensure inbound files, all data movement, and output to various destination folders is frictionless.\nâ€¢ Serve as an escalation point for BI analysts with obstacles on complex ad hocs and projects.\nâ€¢ Heavily contribute to broader enterprise data management decisions and initiatives.\nâ€¢ Perform peer code reviews, and other duties as requested.\n\nSKILLS, EDUCATION, & EXPERIENCE\nâ€¢ Bachelorâ€™s degree required.\nâ€¢ 10+ years of progressive experience.\nâ€¢ Proficiency in MS SQL Server, SSIS, Visual Studio, SSMS, and database administration.\nâ€¢ At least 6 years of experience as an ETL developer, specifically into MS SQL Server.\nâ€¢ 8+ years overall experience in ETL design, implementation, and maintenance, including for full and incremental loads, and slowly changing dimensions.\nâ€¢ Some experience with cloud database platforms.\nâ€¢ Experience developing with C# and (desirable) Python.\nâ€¢ Experience combining and de-duping data sources.\nâ€¢ Domain knowledge and experience in banking services.\nâ€¢ Experience with FIS Global platforms and their Business Intelligence Center (BIC) a plus.\n\nVisa sponsorship not available.\n\nWe are an equal opportunity employer and do not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, disability, military and/or veteran status, or any other Federal or State legally-protected classes.', 'job_highlights': [{'title': 'Qualifications', 'items': ['The successful candidate must be able to ask incisive business questions, tackle complexity, and own delivery', 'Bachelorâ€™s degree required', '10+ years of progressive experience', 'Proficiency in MS SQL Server, SSIS, Visual Studio, SSMS, and database administration', 'At least 6 years of experience as an ETL developer, specifically into MS SQL Server', '8+ years overall experience in ETL design, implementation, and maintenance, including for full and incremental loads, and slowly changing dimensions', 'Some experience with cloud database platforms', 'Experience combining and de-duping data sources', 'Domain knowledge and experience in banking services']}, {'title': 'Benefits', 'items': ['Pay Range: $170,000 - $240,000']}, {'title': 'Responsibilities', 'items': ['The Senior Data Engineer plays a key role in the design, development, and expansion of databases, data orchestrations, and business intelligence for Apple Bank', 'This position assists with the build, maintenance, and optimization of data pipelines into our Enterprise Data Warehouse (EDW) and creates extracts for vendors and partners', 'The Senior Engineer also develops stage tables, operational data store (ODS) databases and tables, data marts, and all ETL processes', 'The work product provided by the Engineer is required to be fit-for-purpose for each data source, repeatable, and automatable, which involves stored procedures, functions, data analyses, cleansing, validations, and reporting', 'The role collaborates closely with IT infrastructure colleagues, database administrators, business stakeholders, data operations teammates, project managers, vendors, and analysts embedded in our businesses', 'Future opportunities may include sourcing third party and public data, additional source-system integrations, and hybrid cloud proof-of-concepts', 'Establish solution architecture, SQL Server databases, and schemas', 'Design and build corresponding SSIS ETLs, solving business needs and seizing opportunities', 'Perform data profiling to identify and understand projects thoroughly', 'Map data from source to repository', 'Build and automate data extracts for vendors and partners to specifications', 'Understand and coordinate the granting of permissions to service and proxy accounts to ensure inbound files, all data movement, and output to various destination folders is frictionless', 'Serve as an escalation point for BI analysts with obstacles on complex ad hocs and projects', 'Heavily contribute to broader enterprise data management decisions and initiatives', 'Perform peer code reviews, and other duties as requested']}], 'apply_options': [{'title': 'ZipRecruiter', 'link': 'https://www.ziprecruiter.com/c/Apple-Bank/Job/Senior-Data-Engineer,-VP/-in-New-York,NY?jid=f9209ecdb2d24904&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTZW5pb3IgRGF0YSBFbmdpbmVlciwgVlAiLCJjb21wYW55X25hbWUiOiJBcHBsZSBCYW5rIiwiYWRkcmVzc19jaXR5IjoiTmV3IFlvcmssIE5ZLCBVbml0ZWQgU3RhdGVzIiwiaHRpZG9jaWQiOiJxTUZWQndhcWxNbm9qczJVQUFBQUFBPT0iLCJobCI6ImVuIn0='}, {'title': 'Data Engineering Leader', 'company_name': 'Deloitte', 'location': 'St. Louis, MO, United States', 'via': 'ZipRecruiter', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=IdKXQ-nBE9xtYX7DAAAAAA%3D%3D&hl=en-EC&shndl=37&shmd=H4sIAAAAAAAA_xXLsQrCQAyAYVz7CE4ZReqdCC66VgSpOIhzSdtwvXIm5RKhb-LrWpd_-OAvvqvCVWgIFw6RiXLkADVhTxl2cJMWlDB3AwjDVSQkWp8Hs0lP3qsmF9TQYuc6eXthamX2o7T6T6MDZpoSGjWH4352E4ftpqIk0YwgMjzNQS2fqCXcHyW8OBr1iy6H_gAalfHmmgAAAA&shmds=v1_AdeF8Kh_uUqzySWJP74m2dn9gRixgi_hc8fxCRWaRod1IRPskg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=IdKXQ-nBE9xtYX7DAAAAAA%3D%3D', 'extensions': ['16 days ago', 'Full-time'], 'detected_extensions': {'posted_at': '16 days ago', 'schedule_type': 'Full-time'}, 'description': "â€¢ As a Data Engineering Leader you will serve as both the visionary architect and driving force behind our data strategy-shaping the roadmap and leading its successful implementation. Your impact will span the entire organization, with a focus on managing core data platforms, enabling efficient data sharing, and maintaining a secure, high-performance data integration landscape. In this hands-on leadership role, you will set the standard for excellence, actively participating in high-impact projects and guiding engineering teams by example. You will mentor and inspire others, fostering a culture of innovation and continuous improvement. Success in this position requires a deep expertise in Data Management platforms and ETL/ELT technologies, combined with strong leadership skills and a genuine passion for advancing data-driven solutions. The ideal candidate will be a collaborative partner and trusted mentor, working closely with cross-functional teams to design, develop, and deploy sophisticated data software solutions that elevate our business.\n\nRecruiting for this role ends on November 17, 2025.\n\nKey Responsibilities:\nâ€¢ Strategic Vision and Alignment: Craft and articulate a vision for Data Management & Enteprise Integraton technologies as it specifically applies to the product engineering teams in alignment with the US Deloitte Technology Data strategy. Collaborate with diverse stakeholders, including product, engineering, experience, delivery, security, and infrastructure teams.\nâ€¢ Advocacy and Technology Roadmap: Advocate for, develop, and communicate the Data Management/Enteprise Integratons implementation approach to the product engineering teams. Ensure the organization is well-informed about objectives, KPIs, technology roadmaps, and progress.\nâ€¢ Craft Mastery and Objectives Realization: Define, measure, and drive the achievement of KPIs. Establish and evolve Data Management & Enteprise Integratons domain standards and best practices. Actively be hands-on with design, architecture, and code most of the time, contributing to team velocity, and be actively engaged with engineers across SSDLC. Review code, drive tech debt reduction, and experiment with new tech.\nâ€¢ Capability Evolution and Development: Mentor and develop engineers. Coach and develop skills in modern engineering practices, related to Data Management & Enteprise Integratons. Showcase learning and mastery by showcasing experiments internally, speaking at conferences, writing whitepapers or blogs, and leading R&D collaborations.\nâ€¢ Iterative Value Delivery: Embrace an iterative/incremental approach to product engineering. Apply a learning-forward approach to navigate complexity and uncertainty. Ensure alignment with customer and business goals through iterative steps and empirical evidence.\nâ€¢ Customer-Centric Problem Solving: Focus on addressing critical issues faced by customers and users. Align technical solutions with business objectives. Minimize unnecessary technical complexities and overengineering. Drive teams toward peak performance through continuous learning and improvement.\nâ€¢ Expert Proficiency and Continuous Improvement: Possess deep expertise in modern software engineering practices. Identify inefficiencies and opportunities for innovation. Enhance the product engineering operating model to be lean and adaptable. Guide and transform the organization to embrace lean principles and foster a culture of innovation.\nâ€¢ Tech/Quality Risk Management: Ensure appropriate technology use and adoption by engineers. Develop and implement explainable, scalable, reliable, and secure products. Inspire experimentation and quality of code. Identify potential technical risks and develop mitigation strategies.\nâ€¢ Influential Communication: Influence, persuade, and drive decision-making processes. Communicate effectively in both written and verbal forms. Craft clear, structured arguments and technical trade-offs supported by evidence.\nâ€¢ Organizational Engagement and Collaboration: Engage stakeholders at all levels of the organization. Build collaborative and constructive relationships. Co-create and drive momentum and value across multiple organizational levels.\n\nThe team: US Deloitte Technology Product Engineering has modernized software and product delivery, creating a scalable, cost-effective model that focuses on value/outcomes that leverages a progressive and responsive talent structure. As Deloitte's primary internal development team, Product Engineering delivers innovative digital solutions to businesses, service lines, and internal operations with proven bottom-line results and outcomes. It helps power Deloitte's success. It is the engine that drives Deloitte, serving many of the world's largest, most respected companies. We develop and deploy cutting-edge internal and go-to-market solutions that help Deloitte operate effectively and lead in the market. Our reputation is built on a tradition of delivering with excellence.\n\nThe successful candidate will possess:\nâ€¢ Excellent interpersonal and organizational skills, with the ability to handle diverse situations, complex projects, and changing priorities, behaving with passion, empathy, and care.\n\nRequired Qualifications:\nâ€¢ A bachelor's degree in computer science, software engineering, or a related discipline. Experience is the most relevant factor.\nâ€¢ Minimum 10 years of experience in data management, data governance, data structures, database systems, and data modeling.\nâ€¢ Minimum 5 years of experience in managing big data of various forms with data platforms such as SAP HANA, Databricks, Snowflake or other industry leading data platforms to generate insights and create intelligence.\nâ€¢ Minimum 3 years of experience with cloud hyperscalers like AWS, Azure, or GCP to build cloud-native applications.\nâ€¢ Minimim 2 years of experience leading and managing high-performing data engineering teams.\nâ€¢ Minimim 1 year of experience with AI/ML and GenAI.\nâ€¢ Prior experience implementing advanced data architectures that include zero-copy data sharing, data lakes and modern ELT strategies.\nâ€¢ Prior experience in modern software engineering practices, including deployment techniques such as Blue-Green and Canary to support A/B testing strategies.\nâ€¢ Prior software engineering experience with an understanding of Business Context Diagrams (BCD), sequence/activity/state/entity relationship/data flow diagrams, OOP/OOD, data structures, algorithms, and code instrumentations.\nâ€¢ Prior experience using methodologies and tools such as XP, Lean, SAFe, DevSecOps, SRE, ADO, GitHub, SonarQube, etc. to deliver high-quality products rapidly.\nâ€¢ Prior experience managing teams through complex transformation projects, especially focused on migrating from one data technology platform to another.\nâ€¢ Prior experience working with, managing and mentoring multi-national teams across global time zones and cultures.\nâ€¢ Prior experience creating a multi-year data strategy in support of organizational data and analytics needs.\n\nOther:\nâ€¢ Ability to travel 10%, on average, based on the work you do and products you build.\nâ€¢ Limited immigration sponsorship may be available.\n\nThe wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. At Deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current range is $130,900 - $268,700.\n\nYou may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.\n\nInformation for applicants with a need for accommodation: https://www2.deloitte.com/us/en/pages/careers/articles/join-deloitte-assistance-for-disabled-applicants.html\n\n#EA_ExpHire\nRITM9491149\nQualifications:\nâ€¢ As a Data Engineering Leader you will serve as both the visionary architect and driving force behind our data strategy-shaping the roadmap and leading its successful implementation. Your impact will span the entire organization, with a focus on managing core data platforms, enabling efficient data sharing, and maintaining a secure, high-performance data integration landscape. In this hands-on leadership role, you will set the standard for excellence, actively participating in high-impact projects and guiding engineering teams by example. You will mentor and inspire others, fostering a culture of innovation and continuous improvement. Success in this position requires a deep expertise in Data Management platforms and ETL/ELT technologies, combined with strong leadership skills and a genuine passion for advancing data-driven solutions. The ideal candidate will be a collaborative partner and trusted mentor, working closely with cross-functional teams to design, develop, and deploy sophisticated data software solutions that elevate our business.\n\nRecruiting for this role ends on November 17, 2025.\n\nKey Responsibilities:\nâ€¢ Strategic Vision and Alignment: Craft and articulate a vision for Data Management & Enteprise Integraton technologies as it specifically applies to the product engineering teams in alignment with the US Deloitte Technology Data strategy. Collaborate with diverse stakeholders, including product, engineering, experience, delivery, security, and infrastructure teams.\nâ€¢ Advocacy and Technology Roadmap: Advocate for, develop, and communicate the Data Management/Enteprise Integratons implementation approach to the product engineering teams. Ensure the organization is well-informed about objectives, KPIs, technology roadmaps, and progress.\nâ€¢ Craft Mastery and Objectives Realization: Define, measure, and drive the achievement of KPIs. Establish and evolve Data Management & Enteprise Integratons domain standards and best practices. Actively be hands-on with design, architecture, and code most of the time, contributing to team velocity, and be actively engaged with engineers across SSDLC. Review code, drive tech debt reduction, and experiment with new tech.\nâ€¢ Capability Evolution and Development: Mentor and develop engineers. Coach and develop skills in modern engineering practices, related to Data Management & Enteprise Integratons. Showcase learning and mastery by showcasing experiments internally, speaking at conferences, writing whitepapers or blogs, and leading R&D collaborations.\nâ€¢ Iterative Value Delivery: Embrace an iterative/incremental approach to product engineering. Apply a learning-forward approach to navigate complexity and uncertainty. Ensure alignment with customer and business goals through iterative steps and empirical evidence.\nâ€¢ Customer-Centric Problem Solving: Focus on addressing critical issues faced by customers and users. Align technical solutions with business objectives. Minimize unnecessary technical complexities and overengineering. Drive teams toward peak performance through continuous learning and improvement.\nâ€¢ Expert Proficiency and Continuous Improvement: Possess deep expertise in modern software engineering practices. Identify inefficiencies and opportunities for innovation. Enhance the product engineering operating model to be lean and adaptable. Guide and transform the organization to embrace lean principles and foster a culture of innovation.\nâ€¢ Tech/Quality Risk Management: Ensure appropriate technology use and adoption by engineers. Develop and implement explainable, scalable, reliable, and secure products. Inspire experimentation and quality of code. Identify potential technical risks and develop mitigation strategies.\nâ€¢ Influential Communication: Influence, persuade, and drive decision-making processes. Communicate effectively in both written and verbal forms. Craft clear, structured arguments and technical trade-offs supported by evidence.\nâ€¢ Organizational Engagement and Collaboration: Engage stakeholders at all levels of the organization. Build collaborative and constructive relationships. Co-create and drive momentum and value across multiple organizational levels.\n\nThe team: US Deloitte Technology Product Engineering has modernized software and product delivery, creating a scalable, cost-effective model that focuses on value/outcomes that leverages a progressive and responsive talent structure. As Deloitte's primary internal development team, Product Engineering delivers innovative digital solutions to businesses, service lines, and internal operations with proven bottom-line results and outcomes. It helps power Deloitte's success. It is the engine that drives Deloitte, serving many of the world's largest, most respected companies. We develop and deploy cutting-edge internal and go-to-market solutions that help Deloitte operate effectively and lead in the market. Our reputation is built on a tradition of delivering with excellence.\n\nThe successful candidate will possess:\nâ€¢ Excellent interpersonal and organizational skills, with the ability to handle diverse situations, complex projects, and changing priorities, behaving with passion, empathy, and care.\n\nRequired Qualifications:\nâ€¢ A bachelor's degree in computer science, software engineering, or a related discipline. Experience is the most relevant factor.\nâ€¢ Minimum 10 years of experience in data management, data governance, data structures, database systems, and data modeling.\nâ€¢ Minimum 5 years of experience in managing big data of various forms with data platforms such as SAP HANA, Databricks, Snowflake or other industry leading data platforms to generate insights and cr...", 'job_highlights': [{'title': 'Qualifications', 'items': ['Success in this position requires a deep expertise in Data Management platforms and ETL/ELT technologies, combined with strong leadership skills and a genuine passion for advancing data-driven solutions', 'Excellent interpersonal and organizational skills, with the ability to handle diverse situations, complex projects, and changing priorities, behaving with passion, empathy, and care', "A bachelor's degree in computer science, software engineering, or a related discipline", 'Experience is the most relevant factor', 'Minimum 10 years of experience in data management, data governance, data structures, database systems, and data modeling', 'Minimum 5 years of experience in managing big data of various forms with data platforms such as SAP HANA, Databricks, Snowflake or other industry leading data platforms to generate insights and create intelligence', 'Minimum 3 years of experience with cloud hyperscalers like AWS, Azure, or GCP to build cloud-native applications', 'Minimim 2 years of experience leading and managing high-performing data engineering teams', 'Minimim 1 year of experience with AI/ML and GenAI', 'Prior experience implementing advanced data architectures that include zero-copy data sharing, data lakes and modern ELT strategies', 'Prior experience in modern software engineering practices, including deployment techniques such as Blue-Green and Canary to support A/B testing strategies', 'Prior software engineering experience with an understanding of Business Context Diagrams (BCD), sequence/activity/state/entity relationship/data flow diagrams, OOP/OOD, data structures, algorithms, and code instrumentations', 'Prior experience using methodologies and tools such as XP, Lean, SAFe, DevSecOps, SRE, ADO, GitHub, SonarQube, etc', 'Prior experience managing teams through complex transformation projects, especially focused on migrating from one data technology platform to another', 'Prior experience working with, managing and mentoring multi-national teams across global time zones and cultures', 'Prior experience creating a multi-year data strategy in support of organizational data and analytics needs', 'Ability to travel 10%, on average, based on the work you do and products you build', 'Limited immigration sponsorship may be available', 'As a Data Engineering Leader you will serve as both the visionary architect and driving force behind our data strategy-shaping the roadmap and leading its successful implementation', 'Success in this position requires a deep expertise in Data Management platforms and ETL/ELT technologies, combined with strong leadership skills and a genuine passion for advancing data-driven solutions', 'The ideal candidate will be a collaborative partner and trusted mentor, working closely with cross-functional teams to design, develop, and deploy sophisticated data software solutions that elevate our business', 'Review code, drive tech debt reduction, and experiment with new tech', 'Coach and develop skills in modern engineering practices, related to Data Management & Enteprise Integratons', 'Expert Proficiency and Continuous Improvement: Possess deep expertise in modern software engineering practices', 'Communicate effectively in both written and verbal forms', 'Craft clear, structured arguments and technical trade-offs supported by evidence', 'Co-create and drive momentum and value across multiple organizational levels', 'Excellent interpersonal and organizational skills, with the ability to handle diverse situations, complex projects, and changing priorities, behaving with passion, empathy, and care', "A bachelor's degree in computer science, software engineering, or a related discipline", 'Experience is the most relevant factor', 'Minimum 10 years of experience in data management, data governance, data structures, database systems, and data modeling', 'Minimum 5 years of experience in managing big data of various forms with data platforms such as SAP HANA, Databricks, Snowflake or other industry leading data platforms to generate insights and cr..']}, {'title': 'Benefits', 'items': ['The wage range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs', 'A reasonable estimate of the current range is $130,900 - $268,700', 'You may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance']}, {'title': 'Responsibilities', 'items': ['Your impact will span the entire organization, with a focus on managing core data platforms, enabling efficient data sharing, and maintaining a secure, high-performance data integration landscape', 'In this hands-on leadership role, you will set the standard for excellence, actively participating in high-impact projects and guiding engineering teams by example', 'You will mentor and inspire others, fostering a culture of innovation and continuous improvement', 'Strategic Vision and Alignment: Craft and articulate a vision for Data Management & Enteprise Integraton technologies as it specifically applies to the product engineering teams in alignment with the US Deloitte Technology Data strategy', 'Collaborate with diverse stakeholders, including product, engineering, experience, delivery, security, and infrastructure teams', 'Advocacy and Technology Roadmap: Advocate for, develop, and communicate the Data Management/Enteprise Integratons implementation approach to the product engineering teams', 'Ensure the organization is well-informed about objectives, KPIs, technology roadmaps, and progress', 'Craft Mastery and Objectives Realization: Define, measure, and drive the achievement of KPIs', 'Establish and evolve Data Management & Enteprise Integratons domain standards and best practices', 'Actively be hands-on with design, architecture, and code most of the time, contributing to team velocity, and be actively engaged with engineers across SSDLC', 'Review code, drive tech debt reduction, and experiment with new tech', 'Capability Evolution and Development: Mentor and develop engineers', 'Coach and develop skills in modern engineering practices, related to Data Management & Enteprise Integratons', 'Showcase learning and mastery by showcasing experiments internally, speaking at conferences, writing whitepapers or blogs, and leading R&D collaborations', 'Iterative Value Delivery: Embrace an iterative/incremental approach to product engineering', 'Apply a learning-forward approach to navigate complexity and uncertainty', 'Ensure alignment with customer and business goals through iterative steps and empirical evidence', 'Customer-Centric Problem Solving: Focus on addressing critical issues faced by customers and users', 'Align technical solutions with business objectives', 'Minimize unnecessary technical complexities and overengineering', 'Drive teams toward peak performance through continuous learning and improvement', 'Expert Proficiency and Continuous Improvement: Possess deep expertise in modern software engineering practices', 'Identify inefficiencies and opportunities for innovation', 'Enhance the product engineering operating model to be lean and adaptable', 'Guide and transform the organization to embrace lean principles and foster a culture of innovation', 'Tech/Quality Risk Management: Ensure appropriate technology use and adoption by engineers', 'Develop and implement explainable, scalable, reliable, and secure products', 'Inspire experimentation and quality of code', 'Identify potential technical risks and develop mitigation strategies', 'Influential Communication: Influence, persuade, and drive decision-making processes', 'Communicate effectively in both written and verbal forms', 'Craft clear, structured arguments and technical trade-offs supported by evidence', 'Organizational Engagement and Collaboration: Engage stakeholders at all levels of the organization', 'Build collaborative and constructive relationships', 'Co-create and drive momentum and value across multiple organizational levels', 'to deliver high-quality products rapidly', 'Your impact will span the entire organization, with a focus on managing core data platforms, enabling efficient data sharing, and maintaining a secure, high-performance data integration landscape', 'In this hands-on leadership role, you will set the standard for excellence, actively participating in high-impact projects and guiding engineering teams by example', 'You will mentor and inspire others, fostering a culture of innovation and continuous improvement', 'Strategic Vision and Alignment: Craft and articulate a vision for Data Management & Enteprise Integraton technologies as it specifically applies to the product engineering teams in alignment with the US Deloitte Technology Data strategy', 'Collaborate with diverse stakeholders, including product, engineering, experience, delivery, security, and infrastructure teams', 'Advocacy and Technology Roadmap: Advocate for, develop, and communicate the Data Management/Enteprise Integratons implementation approach to the product engineering teams', 'Ensure the organization is well-informed about objectives, KPIs, technology roadmaps, and progress', 'Craft Mastery and Objectives Realization: Define, measure, and drive the achievement of KPIs', 'Establish and evolve Data Management & Enteprise Integratons domain standards and best practices', 'Actively be hands-on with design, architecture, and code most of the time, contributing to team velocity, and be actively engaged with engineers across SSDLC', 'Capability Evolution and Development: Mentor and develop engineers', 'Showcase learning and mastery by showcasing experiments internally, speaking at conferences, writing whitepapers or blogs, and leading R&D collaborations', 'Iterative Value Delivery: Embrace an iterative/incremental approach to product engineering', 'Apply a learning-forward approach to navigate complexity and uncertainty', 'Ensure alignment with customer and business goals through iterative steps and empirical evidence', 'Customer-Centric Problem Solving: Focus on addressing critical issues faced by customers and users', 'Align technical solutions with business objectives', 'Minimize unnecessary technical complexities and overengineering', 'Drive teams toward peak performance through continuous learning and improvement', 'Enhance the product engineering operating model to be lean and adaptable', 'Guide and transform the organization to embrace lean principles and foster a culture of innovation', 'Tech/Quality Risk Management: Ensure appropriate technology use and adoption by engineers', 'Develop and implement explainable, scalable, reliable, and secure products', 'Inspire experimentation and quality of code', 'Identify potential technical risks and develop mitigation strategies', 'Influential Communication: Influence, persuade, and drive decision-making processes', 'Organizational Engagement and Collaboration: Engage stakeholders at all levels of the organization', 'Build collaborative and constructive relationships']}], 'apply_options': [{'title': 'ZipRecruiter', 'link': 'https://www.ziprecruiter.com/c/Deloitte/Job/Data-Engineering-Leader/-in-Saint-Louis,MO?jid=9d7a214206807a89&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'LinkedIn', 'link': 'https://www.linkedin.com/jobs/view/data-engineering-leader-at-deloitte-4333432829?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Glassdoor', 'link': 'https://www.glassdoor.com/job-listing/data-engineering-leader-deloitte-JV_IC1131270_KO0,23_KE24,32.htm?jl=1009926714414&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyaW5nIExlYWRlciIsImNvbXBhbnlfbmFtZSI6IkRlbG9pdHRlIiwiYWRkcmVzc19jaXR5IjoiU3QuIExvdWlzLCBNTywgVW5pdGVkIFN0YXRlcyIsImh0aWRvY2lkIjoiSWRLWFEtbkJFOXh0WVg3REFBQUFBQT09IiwiaGwiOiJlbiJ9'}, {'title': 'ETL Developer, Data Warehouse, Denver Health Medical Plan', 'company_name': 'Denver Health', 'location': 'Colorado, United States', 'via': 'WhatJobs', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=S1IuwGCUQ5TH9_diAAAAAA%3D%3D&hl=en-EC&shndl=37&shmd=H4sIAAAAAAAA_1WNsQrCMBBAce0nON0oUhsRXHS0oohCQcWxXNOjicRcyMXSX_PvrKPLg7e8l30mWbW_naGknhwHijmUmBAeGMnwW2h08j1FOBK6ZOBCrdXooHLoYQEnbkAIozbAHg7MnaPp1qQUZKOUiCs6SZisLjS_FHtqeFBPbuSHWsy4CQ4T1av1ciiC7-az_5_1sGPHEVvO4e5tohauY5HkC21fBEC8AAAA&shmds=v1_AdeF8KjSR4iyj5iaa7Ku9dQpDk3kudNXWlW6SDDhbvc75ecuPg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=S1IuwGCUQ5TH9_diAAAAAA%3D%3D', 'extensions': ['11 days ago', 'Full-time'], 'detected_extensions': {'posted_at': '11 days ago', 'schedule_type': 'Full-time'}, 'description': "We are recruiting for a ETL Developer, Data Warehouse, Denver Health Medical Plan to join our team! We are here for life's journey.\n\nWhere is your life journey taking you?\n\nBeing the heartbeat of Denver means our heart reflects something bigger than ourselves, something that connects us all :\n\nHumanity in action, Triumph in hardship, Transformation in health.\nDepartment\n\nManaged Care Administration\nRemote Opportunity From Colorado, Only. Must Be a Colorado Resident. Job Summary\n\nUnder minimal supervision, the Data Warehouse / Business Intelligence Developer participates in the systems development life cycle for new development, enhancements and maintenance projects in a Data Warehouse / Advanced Analytics Environment, works closely with business leaders, end users and other technical resources to design and implement data and analytic solutions to enhance the business decision making process. The position will work with project teams to design, develop and deploy ETL software needed to prepare and deliver data in support a variety of business needs. The developer will leverage development, database, reporting, analytic, business intelligence and customer service skills across a diverse data landscape to meet a variety of business needs.\nEssential Functions Design, develop, test and support end to end Data Warehouse, Business Intelligence and Advanced Analytics solutions for analytics and management teams. (25%) Responsible for designing, building, and supporting the components of data warehouse, such as ETL processes, databases, analytic marts and analytics environments, and other critical data pipelines. (20%) Collaborates with business partners such as analysts, management, architects and other developers to clarify program objectives, determine scope, identify problems and recommend data solutions. (20%) Collaborates with the business analyst, user community, and other technical resources, to thoroughly document requirements and detailed technical designs for software solutions based on functional requirements from the different business users. (15%) Studies and understands the company's business processes and applications, including their effect on data and analytics, and applies the knowledge gained in designing solutions to meet analytics needs. (10%) Interact closely with members of both IT and the business. (10%) Education Bachelor's Degree Required Work Experience Three years of progressive experience working in large relational databases or in a large data warehouse environment required 1-3 years experience working with production operations teams required In lieu of a Bachelor's degree, must have 9 years development experience required. Knowledge, Skills and Abilities Excellent communication skills both written and verbal. Ability to adapt and excel in an evolving technical environment. Strong analytical and technical aptitude. Strong experience with SQL programming. Strong background in data warehousing and business intelligence. Experience with multiple programming languages, both design and development. Experience on agile development teams. Experience / Knowledge with commercial ETL tools, SSIS, Python, HDFS, Hive, Spark, R, Tableau Remote Opportunity From Colorado, Only. Must Be a Colorado Resident. Shift\n\nShift\nWork Type\n\nRegular\nSalary\n\n$95, - $143, / yr\nBenefits Outstanding benefits including up to 27 paid days off per year, immediate retirement plan employer contribution up to , and generous medical plans Free RTD EcoPass (public transportation) On-site employee fitness center and wellness classes Childcare discount programs & exclusive perks on large brands, travel, and more Tuition reimbursement & assistance Education & development opportunities including career pathways and coaching Professional clinical advancement program & shared governance Public Service Loan Forgiveness (PSLF) eligible employer+ free student loan coaching and assistance navigating the PSLF programÃ‚ National Health Service Corps (NHCS) and Colorado Health Service Corps (CHSC) eligible employer Our Values Respect Belonging Accountability Transparency\n\nDenver Health values the unique ideas, talents and contributions reflective of the needs of our community. For more about our commitment to diversity visit :\n\n#J-18808-Ljbffr", 'apply_options': [{'title': 'WhatJobs', 'link': 'https://www.whatjobs.com/jobs/data-engineer?id=2242579281&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJFVEwgRGV2ZWxvcGVyLCBEYXRhIFdhcmVob3VzZSwgRGVudmVyIEhlYWx0aCBNZWRpY2FsIFBsYW4iLCJjb21wYW55X25hbWUiOiJEZW52ZXIgSGVhbHRoIiwiYWRkcmVzc19jaXR5IjoiQ29sb3JhZG8sIFVuaXRlZCBTdGF0ZXMiLCJodGlkb2NpZCI6IlMxSXV3R0NVUTVUSDlfZGlBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Big data Data Engineer - Global Data Analytics', 'company_name': 'Atria Group LLC', 'location': 'Silver Spring, MD, United States', 'via': 'ZipRecruiter', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=B6TLN7PMfnj57XoHAAAAAA%3D%3D&hl=en-EC&shndl=37&shmd=H4sIAAAAAAAA_yWOuwrCQBBFsc0nWE0tMSuKjVbxQUC0CtYySYbNyjqz7KwSP82_M2JzisPlcLPPJDvunIUOE8LhhyNbx0QR5lB5adD_dcno38m1OvqTNKCEse1BGCoR62m67VMKujFG1RdWE47jopWHEaZGBnOXRn-4aY-RgsdEt-V6MRSB7WxVpugQqijPAOfzHhxD7fxrvFGH6NjmcDnkcGWXqIN6jJN-AcAC_Ha8AAAA&shmds=v1_AdeF8KjkfBhs0v3CaSLa1ri8e7HWlaPs7znle9-47knloyHDNw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=B6TLN7PMfnj57XoHAAAAAA%3D%3D', 'description': "Company Description\n\nWe specialize in Staffing, Consulting, Software Development, and Training along with IT services to small to medium size companies. AG's primary objective is to help companies maximize their IT resources and meet the ever-changing IT needs and challenges.\n\nIn addition, AG offers enterprise resource planning and enterprise application integration, supply-chain management, e-commerce solutions, and B2B public exchanges and B2B process integration solutions. Our company provides application analysis, design, development and programming, software engineering, systems development, testing, integration, and implementation, and management consulting services to various clients - including governmental agencies and private companies - throughout the United States and India.\n\nWe provide these services in multiple computing environments and use technologies such as client/server architecture, object-oriented programming languages and tools, distributed database management systems, state-of-the-art networking, and communications infrastructures. Our honest and realistic approach to recruiting dictates that AG does not entice or lure engineers from their employers. We represent only high caliber technical professionals who have committed to making a change required by career.\n\nJob Description\n\nMust Have Skills -\n\nOverall 10+ years' experience in Datawarehousing related technologies.\n\n3+ years architecting and managing AWS Big Data products and services such as EMR, RedShift, Data Pipeline and Kinesis\n\n3+ years of working experience with Hadoop-based technologies such as MapReduce, Hive/Pig/Impala and NoSQL Databases\n\n3+ years of extensive working knowledge in different programming or scripting languages like Java, Linux, C++, PHP, Ruby, Python and/or R.\n\nExperience working with unstructured, semi-structured and unstructured data sets including social, weblogs and real-time data feeds\n\nProficient in designing efficient and robust ETL/ELT workflows\n\nAble to tune Big Data solutions to improve performance and end-user experience\n\nBachelor's or Master's degree in computer science or software engineering\n\nKnowledge BI and Visualization tools such as MicroStrategy/Tableau is a plus\n\nExperience in the media industry is a plus\n\nMust have the legal right to work in the United States\n\nAdditional Information\n\nGOOD COMMUNICATION SKILLS\n\nDURATION: 6 Months\n\nINTERVIEW: PHONE", 'job_highlights': [{'title': 'Qualifications', 'items': ["Overall 10+ years' experience in Datawarehousing related technologies", '3+ years architecting and managing AWS Big Data products and services such as EMR, RedShift, Data Pipeline and Kinesis', '3+ years of working experience with Hadoop-based technologies such as MapReduce, Hive/Pig/Impala and NoSQL Databases', '3+ years of extensive working knowledge in different programming or scripting languages like Java, Linux, C++, PHP, Ruby, Python and/or R', 'Experience working with unstructured, semi-structured and unstructured data sets including social, weblogs and real-time data feeds', 'Proficient in designing efficient and robust ETL/ELT workflows', 'Able to tune Big Data solutions to improve performance and end-user experience', "Bachelor's or Master's degree in computer science or software engineering", 'Must have the legal right to work in the United States', 'DURATION: 6 Months']}], 'apply_options': [{'title': 'ZipRecruiter', 'link': 'https://www.ziprecruiter.com/c/Atria-Group-LLC/Job/Big-data-Data-Engineer-Global-Data-Analytics/-in-Silver-Spring,MD?jid=7b5bd2d5926ca00b&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJCaWcgZGF0YSBEYXRhIEVuZ2luZWVyIC0gR2xvYmFsIERhdGEgQW5hbHl0aWNzIiwiY29tcGFueV9uYW1lIjoiQXRyaWEgR3JvdXAgTExDIiwiYWRkcmVzc19jaXR5IjoiU2lsdmVyIFNwcmluZywgTUQsIFVuaXRlZCBTdGF0ZXMiLCJodGlkb2NpZCI6IkI2VExON1BNZm5qNTdYb0hBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Staff Engineer, Data Platform', 'company_name': 'BILL', 'location': 'San Jose, CA, United States', 'via': 'ZipRecruiter', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=eZBcbwl5lDplWVJkAAAAAA%3D%3D&hl=en-EC&shndl=37&shmd=H4sIAAAAAAAA_xXOPQrCQBBAYWxzBKsBO4lZEW208g9RUgjBOkziZLOSzISdKXIUj2tsXvnxku8s2RaGTQNX9oGJYgoXNIRnh9ZI7GEFD6lACWPdgjDcRHxH80NrNujeOdUu82pooc5q6Z0wVTK6j1T6T6ktRhomjcrNbj1mA_vl4nTPcwgMBfLEK6VwPqbw4mD0hunHSH9uST1NmwAAAA&shmds=v1_AdeF8KinYOw7KmXPyiEsDyO3ybQ5D5fehxVqdx7a0htW2zaCwg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=eZBcbwl5lDplWVJkAAAAAA%3D%3D', 'thumbnail': 'https://serpapi.com/searches/691965c52efef81170d6a44f/images/2dc7e0617baa2797983e797886e9cc733f1e424db6405cf33547ad938f57048c.jpeg', 'extensions': ['Full-time'], 'detected_extensions': {'schedule_type': 'Full-time'}, 'description': "Make your impact within a rapidly growing Fintech Company\n\nJoin the team developing the BILL Data Platform, shaping the data landscape and powering AI Agents and Application across the company! Looking for highly talented engineer excited about taking on interesting technical challenges in Data & AI and realizing its potential. Design, architect, develop, and operate the large-scale platform that is a key part of our business.\n\nResponsibilities:\nâ€¢ Play a key role in the development and operation of BILL's company-wide Data platform leveraged by agentic AI applications, flagship products, data science, and analytics\nâ€¢ Collaborate with architects, engineers, analysts, and product owners to understand requirements and translate them into technical design\nâ€¢ Drive simplicity, quality, efficiency, ease-of-use, and capabilities of the Data platform.\n\nWe'd love to chat if you have:\nâ€¢ Bachelor's degree in Computer Science, Engineering or related discipline. Master's or PhD a plus.\nâ€¢ 8+ years of professional experience with a bachelor's degree, or 6+ years of experience with a master's degree\nâ€¢ Extensive experience with large-scale data systems, stream processing, data lakes\nâ€¢ Extensive experience with data engineering practices and related languages, preferably Python and Java.\nâ€¢ Experience with AI systems and platforms, ML applications, and experimentation.\nâ€¢ Experience with batch and real time data frameworks such as Kafka, Flink, Spark, AWS Glue, AWS EMR, AWS Kinesis, Airflow, DBT, Trino, Neo4j, Iceberg.\nâ€¢ Experience with SQL and NoSQL data stores such as S3, Redshift, Snowflake, Dynamodb etc.\nâ€¢ Experience with Data Quality frameworks and Data Catalogs.\nâ€¢ Expert knowledge of Cloud Architecture and Networking principles.\nâ€¢ Experience with infrastructure-as-code using Terraform, AWS CDK, CloudFormation etc.\nâ€¢ Experience in building and developing CI/CD pipelines.\nâ€¢ Experience with monitoring and dashboarding solutions such as DataDog, Splunk etc.\n\nVisa Sponsorship: Please note that this position is not eligible for visa sponsorship. Applicants must have authorization to work in the United States without requiring visa sponsorship now or in the future.", 'job_highlights': [{'title': 'Qualifications', 'items': ["Bachelor's degree in Computer Science, Engineering or related discipline", "8+ years of professional experience with a bachelor's degree, or 6+ years of experience with a master's degree", 'Extensive experience with large-scale data systems, stream processing, data lakes', 'Extensive experience with data engineering practices and related languages, preferably Python and Java', 'Experience with AI systems and platforms, ML applications, and experimentation', 'Experience with batch and real time data frameworks such as Kafka, Flink, Spark, AWS Glue, AWS EMR, AWS Kinesis, Airflow, DBT, Trino, Neo4j, Iceberg', 'Experience with SQL and NoSQL data stores such as S3, Redshift, Snowflake, Dynamodb etc', 'Experience with Data Quality frameworks and Data Catalogs', 'Expert knowledge of Cloud Architecture and Networking principles', 'Experience with infrastructure-as-code using Terraform, AWS CDK, CloudFormation etc', 'Experience in building and developing CI/CD pipelines', 'Experience with monitoring and dashboarding solutions such as DataDog, Splunk etc', 'Applicants must have authorization to work in the United States without requiring visa sponsorship now or in the future']}, {'title': 'Responsibilities', 'items': ['Join the team developing the BILL Data Platform, shaping the data landscape and powering AI Agents and Application across the company!', "Play a key role in the development and operation of BILL's company-wide Data platform leveraged by agentic AI applications, flagship products, data science, and analytics", 'Collaborate with architects, engineers, analysts, and product owners to understand requirements and translate them into technical design', 'Drive simplicity, quality, efficiency, ease-of-use, and capabilities of the Data platform']}], 'apply_options': [{'title': 'ZipRecruiter', 'link': 'https://www.ziprecruiter.com/c/BILL/Job/Staff-Engineer,-Data-Platform/-in-San-Jose,CA?jid=6e8a402087c2becf&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'LinkedIn', 'link': 'https://www.linkedin.com/jobs/view/staff-engineer-data-platform-at-bill-4306314589?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'SimplyHired', 'link': 'https://www.simplyhired.com/job/kGLEfRLt8bUa5JssgqYofxOtJKtJlsZIQnNh0H_JCjZzwB-QdlJnmQ?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Employbl', 'link': 'https://www.employbl.com/jobs/staff-engineer-data-platform-billcom-1348649?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Accel Job Board', 'link': 'https://jobs.accel.com/companies/invoice2go-2/jobs/59568452-staff-engineer-data-platform?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Parallel', 'link': 'https://www.useparallel.com/app/candidate/job/68d9ce6385cca63f54d179f9?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobright', 'link': 'https://jobright.ai/jobs/info/68d99ec1061b716fa295ab2a?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Teal', 'link': 'https://www.tealhq.com/job/staff-engineer-data-platform_545a795f-6e61-4c17-87c0-38e3f4d81fa1?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTdGFmZiBFbmdpbmVlciwgRGF0YSBQbGF0Zm9ybSIsImNvbXBhbnlfbmFtZSI6IkJJTEwiLCJhZGRyZXNzX2NpdHkiOiJTYW4gSm9zZSwgQ0EsIFVuaXRlZCBTdGF0ZXMiLCJodGlkb2NpZCI6ImVaQmNid2w1bERwbFdWSmtBQUFBQUE9PSIsImhsIjoiZW4ifQ=='}, {'title': 'Data Engineering Lead', 'company_name': 'Verisk', 'location': 'Lehi, UT, United States', 'via': 'ZipRecruiter', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=lNjGhT91z1r68RtlAAAAAA%3D%3D&hl=en-EC&shndl=37&shmd=H4sIAAAAAAAA_xWNuw7CMAwAxdpPYDIrKg1CYoG1CAmx8Vgrp7WSQLGr2EO_gm8mDHfjXfVdVHWLhnDikJgoJw5wJRxgAxfxoIS5jyAMZ5Ew0vIYzSY9OKc6NkENLfVNLx8nTF5m9xKvf3UaMdM0olG322_nZuKwXj3LQN-QuDxiquFxL3AyGuBWUqQ_tk8HFJEAAAA&shmds=v1_AdeF8KgnmC8Shp1EhX8Yc3mnyJhtlHdazxRUZeZkWF9JGjeuPQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=lNjGhT91z1r68RtlAAAAAA%3D%3D', 'extensions': ['13 days ago', '$100Kâ€“$140K a year', 'Full-time'], 'detected_extensions': {'posted_at': '13 days ago', 'salary': '$100Kâ€“$140K a year', 'schedule_type': 'Full-time'}, 'description': "Job Description\n\nAs a Data Engineer, you will be responsible for transforming raw data from our applications into structured datasets for large-scale analysis and machine learning model training. You will work closely with our development, data science, and business intelligence teams to ensure data integrity, quality, and accessibility.\n\nResponsibilities\nâ€¢ Data Pipeline Development: Design, develop, and maintain scalable data pipelines to process raw data from various sources.\nâ€¢ Data Transformation: Clean, transform, and enrich data to create high-quality datasets suitable for analysis and machine learning.\nâ€¢ Collaboration: Work closely with product teams, software developers, data scientists, and analysts to understand data needs and deliver innovative solutions.\nâ€¢ Data Management: Ensure data accuracy, consistency, and reliability across all datasets.\nâ€¢ Optimization: Optimize data processes for performance and scalability.\nâ€¢ Documentation: Maintain comprehensive documentation of data pipelines, processes, and schemas.\n\nWorking Conditions:\nâ€¢ 40 hours per week, with occasional, but rare, overtime\nâ€¢ Remote / Hybrid / Flexible Work Options Available\nâ€¢ Frequent interaction with developers, test automation engineers, QA, management, and members of other Verisk subsidiaries\nâ€¢ Some days we just leave the office and have fun team building activities!\nâ€¢ Regular team lunches!\nâ€¢ State of the art facility with basketball, volleyball, and gym\nâ€¢ Ping pong, foosball, fruit bowls, snacks\nâ€¢ Fun and energetic teams\nâ€¢ Time for innovation, Hack-A-Thons, and learning\n\nQualifications\nâ€¢ Educational Background: bachelor's degree in computer science, Data Engineering, or a related field.\nâ€¢ Experience: 3+ years of experience as a Data Engineer or in a similar role.\nâ€¢ Technical Proficiency:\nâ€¢ Programming Languages: Proficiency in Python, SQL, and familiarity with languages such as C# or Java.\nâ€¢ Data Processing: Experience with ETL tools and frameworks (e.g., Apache Airflow, Luigi, DBT).\nâ€¢ Big Data Technologies: Hands-on experience with big data technologies such as Hadoop, Spark, and Kafka.\nâ€¢ Database Management: Strong knowledge of relational databases (e.g., PostgreSQL, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra).\nâ€¢ Cloud Platforms: Experience with cloud services (e.g., AWS, Google Cloud, Azure) and their data processing tools (e.g., AWS Glue, Google BigQuery).\nâ€¢ AI: Familiarity and enthusiasm for bleeding-edge analytical enablement using tools such as Large Language Models and Prompt Engineering.\nâ€¢ Data Warehousing: Knowledge of data warehousing concepts and solutions (e.g., Redshift, Snowflake).\nâ€¢ Version Control: Proficient with version control systems (e.g., Git).\n\nMachine Learning: Understanding of machine learning concepts and experience working with data for ML model training.\n\nAbout Us\n\nFor over 50 years, Verisk has been the leading data analytics and technology partner to the global insurance industry by delivering value to our clients through expertise and scale. We empower communities and businesses to make better decisions on risk, faster.\n\nAt Verisk, you'll have the chance to use your voice and build a rewarding career that's as unique as you are, with work flexibility and the support, coaching, and training you need to succeed.\n\nFor the eighth consecutive year, Verisk is proudly recognized as a Great Place to WorkÂ® for outstanding workplace culture in the US, fourth consecutive year in the UK, Spain, and India, and second consecutive year in Poland. We value learning, caring and results and make inclusivity and diversity a top priority. In addition to our Great Place to WorkÂ® Certification, we've been recognized by The Wall Street Journal as one of the Best-Managed Companies and by Forbes as a World's Best Employer and Best Employer for Women, testaments to the value we place on workplace culture.\n\nWe're 7,000 people strong. We relentlessly and ethically pursue innovation. And we are looking for people like you to help us translate big data into big ideas. Join us and create an exceptional experience for yourself and a better tomorrow for future generations.\n\nVerisk Businesses\n\nUnderwriting Solutions - provides underwriting and rating solutions for auto and property, general liability, and excess and surplus to assess and price risk with speed and precision\n\nClaims Solutions - supports end-to-end claims handling with analytic and automation tools that streamline workflow, improve claims management, and support better customer experiences\n\nProperty Estimating Solutions - offers property estimation software and tools for professionals in estimating all phases of building and repair to make day-to-day workflows the most efficient\n\nExtreme Event Solutions - provides risk modeling solutions to help individuals, businesses, and society become more resilient to extreme events.\n\nSpecialty Business Solutions - provides an integrated suite of software for full end-to-end management of insurance and reinsurance business, helping companies manage their businesses through efficiency, flexibility, and data governance\n\nMarketing Solutions - delivers data and insights to improve the reach, timing,relevance, and compliance of every consumer engagement\n\nLife Insurance Solutions - offers end-to-end, data insight-driven core capabilities for carriers, distribution, and direct customers across the entire policy lifecycle of life and annuities for both individual and group.\n\nVerisk Maplecroft - provides intelligence on sustainability, resilience, and ESG, helping people, business, and societies become stronger\n\nVerisk Analytics is an equal opportunity employer.\n\nVerisk invests in a benefits package for all employees that includes the following: Health Insurance, a Retirement Plan, Disability benefits, and a Paid Time Off program. We offer a competitive total rewards package that includes base salary determined based on role, experience, skill set, and location.\n\nAll members of the Verisk Analytics family of companies are equal opportunity employers. We consider all qualified applicants for employment without regard to race, religion, color, national origin, citizenship, sex, gender identity and/or expression, sexual orientation, veteran's status, age or disability. Verisk's minimum hiring age is 18 except in countries with a higher age limit subject to applicable law.\n\nhttps://www.verisk.com/company/careers/\n\nUnsolicited resumes sent to Verisk, including unsolicited resumes sent to a Verisk business mailing address, fax machine or email address, or directly to Verisk employees, will be considered Verisk property. Verisk will NOT pay a fee for any placement resulting from the receipt of an unsolicited resume.\n\nVerisk Employee Privacy Notice", 'job_highlights': [{'title': 'Qualifications', 'items': ["Educational Background: bachelor's degree in computer science, Data Engineering, or a related field", 'Experience: 3+ years of experience as a Data Engineer or in a similar role', 'Technical Proficiency:', 'Programming Languages: Proficiency in Python, SQL, and familiarity with languages such as C# or Java', 'Data Processing: Experience with ETL tools and frameworks (e.g., Apache Airflow, Luigi, DBT)', 'Big Data Technologies: Hands-on experience with big data technologies such as Hadoop, Spark, and Kafka', 'Database Management: Strong knowledge of relational databases (e.g., PostgreSQL, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra)', 'Cloud Platforms: Experience with cloud services (e.g., AWS, Google Cloud, Azure) and their data processing tools (e.g., AWS Glue, Google BigQuery)', 'AI: Familiarity and enthusiasm for bleeding-edge analytical enablement using tools such as Large Language Models and Prompt Engineering', 'Data Warehousing: Knowledge of data warehousing concepts and solutions (e.g., Redshift, Snowflake)', 'Version Control: Proficient with version control systems (e.g., Git)', 'Machine Learning: Understanding of machine learning concepts and experience working with data for ML model training']}, {'title': 'Benefits', 'items': ['Verisk invests in a benefits package for all employees that includes the following: Health Insurance, a Retirement Plan, Disability benefits, and a Paid Time Off program', 'We offer a competitive total rewards package that includes base salary determined based on role, experience, skill set, and location']}, {'title': 'Responsibilities', 'items': ['As a Data Engineer, you will be responsible for transforming raw data from our applications into structured datasets for large-scale analysis and machine learning model training', 'You will work closely with our development, data science, and business intelligence teams to ensure data integrity, quality, and accessibility', 'Data Pipeline Development: Design, develop, and maintain scalable data pipelines to process raw data from various sources', 'Data Transformation: Clean, transform, and enrich data to create high-quality datasets suitable for analysis and machine learning', 'Collaboration: Work closely with product teams, software developers, data scientists, and analysts to understand data needs and deliver innovative solutions', 'Data Management: Ensure data accuracy, consistency, and reliability across all datasets', 'Optimization: Optimize data processes for performance and scalability', 'Documentation: Maintain comprehensive documentation of data pipelines, processes, and schemas', '40 hours per week, with occasional, but rare, overtime', 'Remote / Hybrid / Flexible Work Options Available', 'Frequent interaction with developers, test automation engineers, QA, management, and members of other Verisk subsidiaries', 'Some days we just leave the office and have fun team building activities!', 'Regular team lunches!', 'State of the art facility with basketball, volleyball, and gym', 'Ping pong, foosball, fruit bowls, snacks', 'Fun and energetic teams', 'Time for innovation, Hack-A-Thons, and learning', 'Underwriting Solutions - provides underwriting and rating solutions for auto and property, general liability, and excess and surplus to assess and price risk with speed and precision', 'Claims Solutions - supports end-to-end claims handling with analytic and automation tools that streamline workflow, improve claims management, and support better customer experiences', 'Property Estimating Solutions - offers property estimation software and tools for professionals in estimating all phases of building and repair to make day-to-day workflows the most efficient', 'Marketing Solutions - delivers data and insights to improve the reach, timing,relevance, and compliance of every consumer engagement']}], 'apply_options': [{'title': 'ZipRecruiter', 'link': 'https://www.ziprecruiter.com/c/Verisk/Job/Data-Engineering-Lead/-in-Lehi,UT?jid=69045d5dec8a4508&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Indeed', 'link': 'https://www.indeed.com/viewjob?jk=4207fddb348f8c99&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Glassdoor', 'link': 'https://www.glassdoor.com/job-listing/data-engineering-lead-verisk-JV_IC1128238_KO0,21_KE22,28.htm?jl=1009927732771&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'LinkedIn', 'link': 'https://www.linkedin.com/jobs/view/data-engineering-lead-at-verisk-4334040394?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Career.io', 'link': 'https://career.io/job/data-engineering-lead-lehi-verisk-analytics-df4dceed55540ade6c13729c5981040f?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Levels.fyi', 'link': 'https://www.levels.fyi/jobs?jobId=104619397041005254&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Ladders', 'link': 'https://www.theladders.com/job/data-engineering-lead-veriskanalytics-lehi-ut_84261967?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'EQuest Jobs', 'link': 'https://jobs.equest.com/jobs/UT/xactware-solutions-inc/data-engineering-lead-427355162.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJEYXRhIEVuZ2luZWVyaW5nIExlYWQiLCJjb21wYW55X25hbWUiOiJWZXJpc2siLCJhZGRyZXNzX2NpdHkiOiJMZWhpLCBVVCwgVW5pdGVkIFN0YXRlcyIsImh0aWRvY2lkIjoibE5qR2hUOTF6MXI2OFJ0bEFBQUFBQT09IiwiaGwiOiJlbiJ9'}, {'title': 'Sr. Data Engineering Analyst - Columbus, OH', 'company_name': 'Abbott', 'location': 'Columbus, OH, United States', 'via': 'ZipRecruiter', 'share_link': 'https://www.google.com/search?ibp=htl;jobs&q=data+engineer+united+states&htidocid=fseTfFEudtq1-2zcAAAAAA%3D%3D&hl=en-EC&shndl=37&shmd=H4sIAAAAAAAA_1WNuwrCQBAAsc0nWG1jIzEngo1W8YFiYxGsw11cLieX3XC7gfhHfqaxtBmYZib7zLJDlQo4WbVwJh8IMQXyUJKNb1FYwZHj0LlBcrhfJ72xA0GbmhaY4MLsI873rWovO2NEYuFFrYamaLgzTOh4NC928kMtrU3YR6tYb7brsejJLxelc6wKgf5eOTwoKD6hmnIoX5X6c1-rAAAA&shmds=v1_AdeF8KiDrqNTp8Hc6Sae4maL4Hj0bUWmxcjIYsTL6krpqtoxgQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+united+states&htidocid=fseTfFEudtq1-2zcAAAAAA%3D%3D', 'extensions': ['Full-time'], 'detected_extensions': {'schedule_type': 'Full-time'}, 'description': "Abbott is a global healthcare leader that helps people live more fully at all stages of life. Our portfolio of life-changing technologies spans the spectrum of healthcare, with leading businesses and products in diagnostics, medical devices, nutritionals and branded generic medicines. Our 114,000 colleagues serve people in more than 160 countries.\n\nJOB DESCRIPTION:\n\nJob Title\n\nSr. Data Engineering Analyst - Columbus, OH\n\nAbout Abbott\n\nAbbott is a global healthcare leader, creating breakthrough science to improve people's health. We're always looking towards the future, anticipating changes in medical science and technology.\n\nOur nutrition business develops science-based nutrition products for people of all ages, from helping babies and children grow, to keeping adult bodies strong and active. Millions of people around the world count on our leading brands - including Similac, PediaSure, Pedialyte, Ensure, and Glucerna - to help get the nutrients they need to live their healthiest life.\n\nWorking at Abbott\n\nAt Abbott, you can do work that matters, grow, and learn, care for yourself and family, be your true self and live a full life. You'll also have access to:\nâ€¢ Career development with an international company where you can grow the career you dream of.\nâ€¢ Free medical coverage for employees* via the Health Investment Plan (HIP) PPO\nâ€¢ An excellent retirement savings plan with high employer contribution\nâ€¢ Tuition reimbursement, the Freedom 2 Save student debt program and FreeU education benefit - an affordable and convenient path to getting a bachelor's degree.\nâ€¢ A company recognized as a great place to work in dozens of countries around the world and named one of the most admired companies in the world by Fortune.\nâ€¢ A company that is recognized as one of the best big companies to work for as well as a best place to work for diversity, working mothers, female executives, and scientists.\n\nThe Opportunity\n\nThe Sr. Data Engineering Analyst position works out of our Columbus, OH location in the Nutrition Division.\n\nThe Sr. Data Engineering Analyst leads the design and management of analytics tools that support data-driven decision-making across Commercial Operations. This role transforms raw data into actionable insights, enhances reporting processes, and ensures data accuracy and accessibility. It also serves as a key liaison with IT and external partners to maintain seamless data flow. The focus is on delivering timely, reliable, and strategic information that aligns with business goals.\n\nWhat You'll Work On\n\nTechnical & Reporting Development\nâ€¢ Create and maintain advanced structured query language (SQL), Excel macros, and ETL workflows (e.g., Azure Data Factory) to support efficient data extraction and transformation.\nâ€¢ Build and manage Power BI data models and dashboards for operational and customer-facing reporting.\nâ€¢ Ensure timely, accurate data refreshes and reporting through tools like Cognos and Power BI.\n\nAnalytics & Insights\nâ€¢ Deliver actionable insights through ad hoc analysis, exception reporting, and performance scorecards.\nâ€¢ Streamline reporting processes to reduce manual effort and enable proactive, data-driven decision-making.\nâ€¢ Integrate data from multiple sources to create unified, business-centric dashboards.\n\nProject & Stakeholder Management\nâ€¢ Collaborate with cross-functional teams (Sales, Finance, IT, Supply Chain) to align analytics with business needs.\nâ€¢ Translate complex data into clear, impactful presentations for diverse audiences.\nâ€¢ Support strategic initiatives by developing metrics, dashboards, and tracking tools.\n\nCompliance & Data Integrity\nâ€¢ Validate data accuracy and completeness across all deliverables.\nâ€¢ Develop exception reporting to proactively identify and resolve system or process issues.\nâ€¢ Promote best practices in data governance and coach others on BI tool usage.\n\nAccountability & Impact\nâ€¢ Ensures integrity of indirect sales data used across key business functions.\nâ€¢ Drives long-term solutions for contract strategy and distributor performance.\nâ€¢ Leads data coaching and supports strategic decision-making across Commercial Operations.\n\nRequired Qualifications\n\nEducation and Experience\nâ€¢ Bachelor's degree in Business Analytics, Information Systems, Data Science, Computer Science, or related field.\nâ€¢ 4+ years in Business Intelligence; 3+ years in data analysis and performance management.\n\nTechnical Skills & Knowledge\nâ€¢ Advanced SQL and expert-level Excel (macros, large datasets).\nâ€¢ Proficient in Power BI, Cognos, and ETL tools (e.g., Azure Data Factory).\nâ€¢ Strong understanding of data warehousing, KPIs, and contract/claims processes.\nâ€¢ Skilled in translating complex data into clear, actionable insights for diverse audiences.\n\nBusiness & Communication Skills\nâ€¢ Proven ability to manage multiple priorities in a fast-paced environment.\nâ€¢ Effective communicator with both technical and non-technical stakeholders.\nâ€¢ Experienced in cross-functional collaboration (Finance, Sales, IT, Supply Chain).\nâ€¢ Coaches others on BI tools and data best practices.\n\nApply Now\nâ€¢ Participants who complete a short wellness assessment qualify for FREE coverage in our HIP PPO medical plan. Free coverage applies in the next calendar year.\n\nLearn more about our health and wellness benefits, which provide the security to help you and your family live full lives: www.abbottbenefits.com\n\nFollow your career aspirations to Abbott for diverse opportunities with a company that can help you build your future and live your best life. Abbott is an Equal Opportunity Employer, committed to employee diversity.\n\nConnect with us at www.abbott.com, on Facebook at www.facebook.com/Abbott and on Twitter @AbbottNews and @AbbottGlobal.\n\nThe base pay for this position is\n$75,300.00 - $150,700.00\n\nIn specific locations, the pay range may vary from the range posted.\n\nJOB FAMILY:Sales Support & Administration\n\nDIVISION:ANPD Nutrition Products\n\nLOCATION:United States > Columbus : RP02\n\nADDITIONAL LOCATIONS:\n\nWORK SHIFT:Standard\n\nTRAVEL:No\n\nMEDICAL SURVEILLANCE:No\n\nSIGNIFICANT WORK ACTIVITIES:Continuous sitting for prolonged periods (more than 2 consecutive hours in an 8 hour day), Keyboard use (greater or equal to 50% of the workday)\n\nAbbott is an Equal Opportunity Employer of Minorities/Women/Individuals with Disabilities/Protected Veterans.\n\nEEO is the Law link - English: http://webstorage.abbott.com/common/External/EEO_English.pdf\n\nEEO is the Law link - Espanol: http://webstorage.abbott.com/common/External/EEO_Spanish.pdf", 'job_highlights': [{'title': 'Qualifications', 'items': ["Bachelor's degree in Business Analytics, Information Systems, Data Science, Computer Science, or related field", '4+ years in Business Intelligence; 3+ years in data analysis and performance management', 'Technical Skills & Knowledge', 'Advanced SQL and expert-level Excel (macros, large datasets)', 'Proficient in Power BI, Cognos, and ETL tools (e.g., Azure Data Factory)', 'Strong understanding of data warehousing, KPIs, and contract/claims processes', 'Skilled in translating complex data into clear, actionable insights for diverse audiences', 'Business & Communication Skills', 'Proven ability to manage multiple priorities in a fast-paced environment', 'Effective communicator with both technical and non-technical stakeholders', 'Experienced in cross-functional collaboration (Finance, Sales, IT, Supply Chain)', 'Coaches others on BI tools and data best practices', 'SIGNIFICANT WORK ACTIVITIES:Continuous sitting for prolonged periods (more than 2 consecutive hours in an 8 hour day), Keyboard use (greater or equal to 50% of the workday)']}, {'title': 'Benefits', 'items': ['At Abbott, you can do work that matters, grow, and learn, care for yourself and family, be your true self and live a full life', 'Career development with an international company where you can grow the career you dream of', 'Free medical coverage for employees* via the Health Investment Plan (HIP) PPO', 'An excellent retirement savings plan with high employer contribution', "Tuition reimbursement, the Freedom 2 Save student debt program and FreeU education benefit - an affordable and convenient path to getting a bachelor's degree", 'Participants who complete a short wellness assessment qualify for FREE coverage in our HIP PPO medical plan', 'Free coverage applies in the next calendar year', '$75,300.00 - $150,700.00']}, {'title': 'Responsibilities', 'items': ['Data Engineering Analyst leads the design and management of analytics tools that support data-driven decision-making across Commercial Operations', 'This role transforms raw data into actionable insights, enhances reporting processes, and ensures data accuracy and accessibility', 'It also serves as a key liaison with IT and external partners to maintain seamless data flow', 'The focus is on delivering timely, reliable, and strategic information that aligns with business goals', 'Technical & Reporting Development', 'Create and maintain advanced structured query language (SQL), Excel macros, and ETL workflows (e.g., Azure Data Factory) to support efficient data extraction and transformation', 'Build and manage Power BI data models and dashboards for operational and customer-facing reporting', 'Ensure timely, accurate data refreshes and reporting through tools like Cognos and Power BI', 'Analytics & Insights', 'Deliver actionable insights through ad hoc analysis, exception reporting, and performance scorecards', 'Streamline reporting processes to reduce manual effort and enable proactive, data-driven decision-making', 'Integrate data from multiple sources to create unified, business-centric dashboards', 'Project & Stakeholder Management', 'Collaborate with cross-functional teams (Sales, Finance, IT, Supply Chain) to align analytics with business needs', 'Translate complex data into clear, impactful presentations for diverse audiences', 'Support strategic initiatives by developing metrics, dashboards, and tracking tools', 'Compliance & Data Integrity', 'Validate data accuracy and completeness across all deliverables', 'Develop exception reporting to proactively identify and resolve system or process issues', 'Promote best practices in data governance and coach others on BI tool usage', 'Accountability & Impact', 'Ensures integrity of indirect sales data used across key business functions', 'Drives long-term solutions for contract strategy and distributor performance', 'Leads data coaching and supports strategic decision-making across Commercial Operations']}], 'apply_options': [{'title': 'ZipRecruiter', 'link': 'https://www.ziprecruiter.com/c/Abbott-Laboratories/Job/Sr.-Data-Engineering-Analyst-Columbus,-OH/-in-Columbus,OH?jid=441cfe7a4452589e&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Levels.fyi', 'link': 'https://www.levels.fyi/jobs?jobId=84049898864812742&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Ladders', 'link': 'https://www.theladders.com/job/sr-data-engineering-analyst-columbus-oh-abbott-columbus-oh_83907070?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'JobGet', 'link': 'https://www.jobget.com/jobs/job/f9fa9511-f94d-4c54-ba1c-c3ba568afa58?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'SaluteMyJob', 'link': 'https://salutemyjob.com/jobs/sr.-data-engineering-analyst-columbus-oh-columbus-ohio/2436190485-2/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Spra.co.uk', 'link': 'https://spra.co.uk/wp-job/job/wealth-manager-at-compunnel-inc-columbus-oh-c0EzYUtRb0hxWEhYSkpTRWMzNEwvQTJEQ3c9PQ==?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'IT JobServe', 'link': 'https://it.jobserve.com/job-in-Columbus-Ohio-USA/SR-DATA-ENGINEERING-ANALYST-COLUMBUS-OH-96eeb333ef06225c91/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}, {'title': 'Jobrapido', 'link': 'https://us.jobrapido.com/jobpreview/514538200899780608?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic'}], 'job_id': 'eyJqb2JfdGl0bGUiOiJTci4gRGF0YSBFbmdpbmVlcmluZyBBbmFseXN0IC0gQ29sdW1idXMsIE9IIiwiY29tcGFueV9uYW1lIjoiQWJib3R0IiwiYWRkcmVzc19jaXR5IjoiQ29sdW1idXMsIE9ILCBVbml0ZWQgU3RhdGVzIiwiaHRpZG9jaWQiOiJmc2VUZkZFdWR0cTEtMnpjQUFBQUFBPT0iLCJobCI6ImVuIn0='}]
{'inserted_rows': 92, 'table': 'projects-portfolio-446806.jobs_scraping.serpapi_jobs'}
